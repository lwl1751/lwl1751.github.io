<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>how to build up a blog</title>
    <url>/2024/05/12/how-to-build-up-a-blog/</url>
    <content><![CDATA[<span id="more"></span>
<p>登录root用户</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p>切换路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd /Users/lwl/Blog</span><br></pre></td></tr></table></figure>
<p>博客文章发布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;title&quot;</span> <span class="comment"># 新建文章</span></span><br><span class="line"><span class="comment"># 编辑对应的markdown文件</span></span><br><span class="line">hexo g <span class="comment"># 渲染md文件为博客页面</span></span><br><span class="line">hexo s <span class="comment"># 执行后打开http://localhost:4000/预览</span></span><br><span class="line">hexo d <span class="comment"># 预览并编辑无误后再部署，也可以直接部署</span></span><br></pre></td></tr></table></figure>
<p>有时部署会失败，此时尝试下面命令清除缓存后再执行部署命令。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo clean </span><br></pre></td></tr></table></figure>
<p>更换端口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo s -p <span class="number">8888</span> <span class="comment"># 示例</span></span><br></pre></td></tr></table></figure>
<p>释放端口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lsof -i :端口号 <span class="comment"># 查找占用指定端口的进程ID（PID）</span></span><br><span class="line">kill -<span class="number">9</span> 进程ID</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>基于策略梯度的强化学习</title>
    <url>/2024/05/15/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>强化学习可分为两大类：</p>
<ul>
<li><p><strong>value-based method</strong>(DP,MC,TD)</p>
<p>通过价值函数求解最优策略，求解出来的策略是确定性的，虽然可以通过<span class="math inline">\(\epsilon\)</span>-贪心策略来获取一定的随机性。要求动作空间离散。</p></li>
<li><p><strong>policy-based method</strong></p>
<p>适用场景：随机策略；动作空间连续。</p>
<p>优点：具有更好的收敛性质。</p>
<p>缺点：通常会收敛到局部最优而非全局最优；评估一个策略通常不够高效并且具有较大的方差。</p></li>
</ul>
<span id="more"></span>
<h2 id="基本原理">1.基本原理</h2>
<p>由于策略实际上是一个概率分布，可以将策略参数化 <span class="math inline">\(\pi(a|s,\theta)\)</span> ，其中<span class="math inline">\(\theta\)</span>
是策略的参数。通过这种方式，可以将可见的已知状态泛化到未知的状态上。</p>
<h3 id="策略目标函数">1.1 策略目标函数</h3>
<p>在片段式的环境中，使用每个经历片段(episode)的平均总回报。在连续性的环境中，使用每一步的平均奖励。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/策略目标函数1.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/策略目标函数2.png" alt="图片2"></p>
<p>希望能够找到最大化<span class="math inline">\(J(\theta)\)</span>的<span class="math inline">\(\theta\)</span>，属于最优化问题，求解方法如下：</p>
<ul>
<li>不使用梯度的方法(Hill climbing, Simplex, 模拟退火, 遗传算法)</li>
<li>使用梯度的方法更高效(梯度下降, 共轭梯度, 拟牛顿法)</li>
</ul>
<h3 id="策略函数">1.2 策略函数</h3>
<ul>
<li>softmax策略，离散型动作空间</li>
<li>高斯函数策略，连续型动作空间</li>
<li>线性函数策略，连续型动作空间，表示给定状态下确定性的动作，<span class="math inline">\(a=\pi(s,\theta)\)</span></li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/score函数.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/softmax策略2.png" alt="图片2"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/softmax策略.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/高斯策略.png" alt="图片2"></p>
<h3 id="单步马尔可夫决策过程">1.3 单步马尔可夫决策过程</h3>
<p>从一个分布d(s)中采样得到一个状态s，从s开始，按照策略𝜋采取一个行为a，得到即时奖励<span class="math inline">\(r=R_{s,a}\)</span>。由于是单步过程，目标函数为
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/单步马尔可夫决策公式.png" alt="图片"></p>
<h2 id="策略梯度定理">2.策略梯度定理</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/策略推理1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/策略推理2.png" alt="图片"></p>
<p>由于状态转移函数的存在，虽然训练用的轨迹都是由同一个策略生成的，但其两两差异仍十分显著，并且显然轨迹越长差异越大，决策中每一个微小的差异累积起来都会导致最后结果的极大差异。也就是数据有着较大的方差，这会导致使用均值计算期望的效果变差，并使算法难以收敛。</p>
<p><strong>改进方法：使用时序因果关系；加入基线。</strong></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/时间连续性1.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/时间连续性2.png" alt="图片2"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/加入基线1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/加入基线2.png" alt="图片"></p>
<h2 id="蒙特卡洛策略梯度reinforce">3.蒙特卡洛策略梯度(REINFORCE)</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/REINFORCE-1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/REINFORCE2.png" alt="图片"></p>
<h2 id="actor-critic-算法">4. Actor-Critic 算法</h2>
<p><strong>actor --&gt; policy network</strong>，决定采取哪个动作</p>
<ul>
<li><span class="math inline">\(\pi(a|s;\theta)\)</span></li>
<li>input: state s</li>
<li>output: probability distribution over the actions</li>
<li>训练目标： 增加状态值函数 state-value</li>
</ul>
<p>c<strong>ritic --&gt; value
network</strong>，只负责评估动作的好坏</p>
<ul>
<li><span class="math inline">\(q(s,a;w)\)</span></li>
<li>input: state s and action a</li>
<li>output: approximate action-value(scalar)</li>
<li>训练目标： 使价值评估的更精准，接近于实际环境的return</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/A3C-1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/A3C-2.png" alt="图片"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p><a href="https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d" class="uri">https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
<p><a href="https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg" class="uri">https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg</a></p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d" class="uri">https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态知识图谱应用</title>
    <url>/2024/05/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>相比于单模态知识图谱，多模态知识图谱能够综合多种类型的数据，从而可以让智能体更深入的感知和理解真实的数据场景，因而多模态知识图谱在各个领域都有广泛的应用。如图像检索、模型推理与生成、模型预训练等。以电子商务为例，通过多模态产品图谱，可以对产品进行更细致的表示，再通过预训练，可以增强大型模型对电子商务领域的多模态知识理解，从而推动电子商务平台的发展。</p>
<span id="more"></span>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/多模态知识图谱应用1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>AliMe
MKG是阿里提出的一种面向直播的知识图谱，与传统的知识图谱不同，它的目标是向顾客种草某一产品，而非解决顾客的问题。因此，它需要构建逻辑思维链，引导用户需求。例如，在该左图1的知识图谱示例中，"熬夜"导致"皮肤暗沉"问题，这就需要"皮肤白皙"，而含有"甘草酸二钾"成分的"面膜"产品适合相应的用户。</p>
<p>在电商直播领域，这种知识图谱有两种应用：智能辅播和虚拟主播。智能辅播是在真人直播间构造了一个智能助理机器人，来协助主播去做商品介绍。比如说用户问的是口红，直播间内有多个口红，智能辅播就会将相关的信息展示出来给用户进行浏览，当用户点击确认，选择一个感兴趣的口红之后，辅播就会从知识图谱中抽取相应信息，以商品卡片信息的方式让用户和图片进行交互。除此之外，智能辅播还可以回答用户丰富的产品相关问题。比如用户问尺码的时候，辅播可以去推出文本介绍和对应的尺码图，用文本及图片来回答用户的咨询。</p>
<p>虚拟主播则是一个智能的直播间虚拟人，通过自动生成图文剧本介绍商品，生成具有吸引力和认知的知识型短视频，从而可以影响客户的购买决策。因此，多模态的知识图谱可以促进电商的发展。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/多模态知识图谱应用2.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>另一个是医疗诊断的案例，这是一个基于多模态知识图谱的医疗健康问答系统示例。它首先利用多种方法获取用户提交数据的关键信息，并确定用户查询的主题意图，建立用户的知识需求模型。在知识匹配阶段，我们计算用户需求与医疗健康知识的相关度，并消除可能的歧义，最终向用户提供匹配度高的医疗健康知识。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/多模态知识图谱应用3.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>参考资料</strong>:</p>
<p>Chen Z, Zhang Y, Fang Y, et al. Knowledge Graphs Meet Multi-Modal
Learning: A Comprehensive Survey[J]. arXiv preprint arXiv:2402.05391,
2024.</p>
<p>Xu G, Chen H, Li F L, et al. Alime mkg: A multi-modal knowledge graph
for live-streaming e-commerce[C]//Proceedings of the 30th ACM
International Conference on Information &amp; Knowledge Management.
2021: 4808-4812.</p>
<p><a href="https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A" class="uri">https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A</a></p>
<p><a href="https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g" class="uri">https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g</a></p>
<p>韩普,叶东宇,陈文祺,等.面向多模态医疗健康数据的知识组织模式研究[J].现代情报,2023,43(10):27-34+151.</p>
]]></content>
      <tags>
        <tag>多模态</tag>
        <tag>知识图谱</tag>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型知识分析、萃取与增强</title>
    <url>/2024/05/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E5%88%86%E6%9E%90%E3%80%81%E8%90%83%E5%8F%96%E4%B8%8E%E5%A2%9E%E5%BC%BA/</url>
    <content><![CDATA[<pre><code>大模型中蕴含着大量的知识，但是知识的类型、数量和质量并不可控。

知识分析实验表明，大模型自发学到了一些世界知识、常识知识，这些知识隐式地存储于模型参数中。

课程考试复习使用。</code></pre>
<span id="more"></span>
<h2 id="大模型的知识分析">1.大模型的知识分析</h2>
<h3 id="知识探测">1.1 知识探测</h3>
<ul>
<li><p><strong>知识探测</strong>：探测预训练语言模型掌握的知识。</p></li>
<li><p><strong>实现方式</strong>：将三元组或问答对形式的世界知识转化为自然语言填空的形式，从而判断语言模型掌握知识的准确性。</p></li>
<li><p><strong>预训练模型知识探测的良好性能主要来源于</strong>：</p>
<ul>
<li>提示语偏差，预测结果会受到提示词的影响，如 was born in [Mask] ,
模型会猜测下一个词应该为地名。</li>
<li>类别指导，类似于few shot learning，模型已经见过类似的问题。</li>
<li>答案泄漏，基于上下文的推理。</li>
</ul></li>
</ul>
<h3 id="知识定位">1.2 知识定位</h3>
<ul>
<li><strong>知识定位</strong>：分析预训练语言模型中的知识存储机制，可分为层粒度与神经元粒度。</li>
<li>大量事实知识存储在<strong>FNN模块</strong>中。</li>
</ul>
<h3 id="知识学习机理分析">1.3 知识学习机理分析</h3>
<p>分析影响预训练语言模型学习效果的因素。</p>
<ul>
<li><strong>长尾知识</strong>：信息出现次数非常少，甚至只出现了一次。LLM对长尾知识的掌握并不充分，回答问题的准确度就会降低，可以通过扩大模型规模(scaling
low)、检索增强来解决该问题。</li>
<li><strong>共现频率</strong>：LLM更加倾向于预测共现频率更高的答案。如，对于问题“加拿大的首都是？”，在预训练语料中，（加拿大，多伦多）共同出现的频率要大于（加拿大，渥太华）出现的频率，于是模型倾向于输出共现频率更高的“多伦多”，而不是正确答案“渥太华”。</li>
<li><strong>逆转诅咒</strong>，模型很难逆转思考，如 A is B 不能推出 B is
A.</li>
</ul>
<h2 id="大模型的知识萃取">2.大模型的知识萃取</h2>
<p>知识萃取是指利用特定方式诱导大模型，从中萃取出有用的显式符号化知识。</p>
<h2 id="大模型的知识增强">3.大模型的知识增强</h2>
<ul>
<li>幻觉可以分为事实性现象（生成的内容不忠于既定的事实知识）和忠实性幻觉（生成的内容前后冲突）。</li>
<li><strong>幻觉消除</strong>：清洗训练数据，解码方式改进，指令数据优化，外部知识增强。</li>
<li><strong>知识增强</strong>：RAG，Fine-tuning。</li>
<li>关于幻觉的知识，可以查看文献综述：《Survey on Factuality in Large
Language Models: Knowledge, Retrieval and Domain-Specificity》</li>
</ul>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识增强_1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="rag">3.1 RAG</h3>
<ul>
<li><strong>检索知识源</strong>
<ul>
<li>文档：检索粒度粗，知识覆盖度高，存在冗余信息。</li>
<li>知识图谱：提供丰富的结构化信息，受限于图谱的覆盖度。</li>
</ul></li>
<li><strong>检索方式</strong>
<ul>
<li>稀疏检索：简单词汇匹配，缺少上下文理解能力（BM25）</li>
<li>稠密检索：将问题和文档编码为稠密向量，计算点积作为相似度（DPR）</li>
<li>生成检索：直接使用大模型为问题生成相关文档，而不需要检索库</li>
</ul></li>
<li><strong>检索时间</strong>
<ul>
<li>推理前检索一次：效率高，但相关度低</li>
<li>推理过程中自适应地检索：平衡知识和效率，但是难以判断模型何时需要知识</li>
<li>推理过程中每隔N个token检索一次：效率低，信息量大</li>
</ul></li>
<li><strong>利用检索得到的知识进行推理</strong>
<ul>
<li>输入增强：使用简单，受限于上下文长度</li>
<li>中间增强：需要重新训练模型，支持处理更多文档</li>
<li>输出增强：对输出进行后修改，需要两次推理模型：第一次，模型直接输出答案。第二次，根据问题和答案，进行检索，对输出答案进行修改。</li>
</ul></li>
<li><strong>知识拉锯战</strong>：由于错误信息，观点不同，以及知识进化的本质，知识冲突问题广泛存在于检索增强语言模型中。</li>
<li><strong>知识冲突形式</strong>：
<ul>
<li>模型内部参数化知识和外部非参数化知识之间存在冲突。其中，使用外部知识回答的模型作为专家模型，依靠内部知识回答的模型作为业余模型。</li>
<li>非参数化知识中真实、虚假以及无关证据之间存在冲突。其中，通过指令微调，使用真实证据回答的模型作为专家模型，使用虚假证据回答的模型作为业余模型。</li>
</ul></li>
<li>Dunning-Kruger现象：人对于某些欠缺的能力反而会过度自信，对模型也同样适用。</li>
</ul>
<h2 id="大模型的工具增强">4.大模型的工具增强</h2>
<ul>
<li><strong>工具增强</strong>：让模型学会使用外部工具，以补充模型相关知识。</li>
<li>相关论文：
<ul>
<li>Timo Schick, Toolformer: Language Models Can Teach Themselves to Use
Tools, NeurIPS 2023。</li>
<li>Yujia Qin, ToolLLM: Facilitating Large Language Models to Master
16000+ Real-world APIs, ICLR2024。</li>
</ul></li>
</ul>
<h2 id="参考资料">5.参考资料</h2>
<p>中国科学院大学赵军老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习基础知识</title>
    <url>/2024/05/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p><strong>知识点：马尔科夫决策过程，动态规划。</strong></p>
<hr>
<span id="more"></span>
<h2 id="马尔科夫决策过程">马尔科夫决策过程</h2>
<p><strong><span style="color:purple;">马尔科夫过程</span></strong>:
一个具备马尔科夫性质的离散随机过程。</p>
<p><strong><span style="color:purple;">马尔科夫性</span></strong>:
下一时刻的状态只与当前状态有关。即<span class="math inline">\(P[S_{t+1}|S_{1},...,S_{t}]=P[S_{t+1}|S_{t}]\)</span></p>
<p><strong><span style="color:purple;">马尔科夫奖励函数</span></strong>:
把马尔科夫过程从 &lt;S, P&gt; 拓展到 &lt;S, P, R, γ&gt;。其中P
为状态转移矩阵，R 和 γ
分别表示奖励函数和奖励折扣因子。折扣因子越大，代表了智能体对长期性能指标考虑的程度越高（远视）；折扣因子越小，代表了智能体对长期性能指标考虑的程度越低（近视）。</p>
<p><strong><span style="color:purple;">回报</span></strong>:
回报是一个轨迹的累积奖励，<span class="math inline">\(G_{t} = R_{t+1} +
\gamma R_{t+2} = \sum_{k=0}^{\infty } \gamma ^{k}R_{t+k+1}\)</span></p>
<p><strong><span style="color:purple;">价值函数</span></strong>：状态s的期望回报，<span class="math inline">\(V(s) = E[G_{t}|S_{t}=s]\)</span>。</p>
<p><strong><span style="color:purple;">马尔科夫决策过程</span></strong>:
马尔可夫奖励过程的立即奖励只取决于状态(奖励值在节点上)，而马尔可夫决策过程的立即奖励与状态和动作都有关。即把马尔科夫过程从
&lt;S, P, R, γ&gt; 拓展到 &lt;S, A, P, R, γ&gt;。A是有限动作的集合。</p>
<p><strong><span style="color:purple;">动作价值函数</span></strong>:
依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。<span class="math inline">\(Q(s,a) = E[G_{t}|S_{t}=s,
A_{t}=a]\)</span>。易知<span class="math inline">\(V(s)=E_{a}[Q(s,a)]\)</span>。</p>
<p><strong><span style="color:purple;">策略</span></strong>:
状态到行为的映射。</p>
<blockquote>
<p>对于任何马尔科夫决策过程：</p>
<ul>
<li>总是存在一个最优策略<span class="math inline">\(\pi^*\)</span>，比任何其他策略更好或至少相等。</li>
<li>所有的最优策略有相同且最优的价值。</li>
<li>所有的最优策略具有相同且最优的动作价值。</li>
</ul>
</blockquote>
<p><strong><span style="color:purple;">贝尔曼方程</span></strong>：用于计算给定策略 π
时价值函数在策略指引下所采轨迹上的期望。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/截屏2024-05-16%2013.18.14.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/截屏2024-05-16%2013.18.34.png" alt="图片2"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/截屏2024-05-16%2013.27.29.png" alt="图片3"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/截屏2024-05-16%2013.27.43.png" alt="图片4"></p>
<p><strong><span style="color:purple;">最优价值函数</span></strong>:</p>
<p>即使是在相同的状态和动作集合上，不同的策略也将会带来不同的价值函数。定义最优价值函数为</p>
<p><span class="math display">\[v_*(s) = \max_{π} v_π(s), ∀s ∈
S\]</span></p>
<p>最优动作价值函数：</p>
<p><span class="math display">\[q_*(s,a) = \max_{\pi} q_π(s,a), ∀s ∈ S,
a ∈ A\]</span></p>
<p>则</p>
<p><span class="math display">\[v*(s) = \max_{a\sim \mathbf{A}} q_*(s,
a)\]</span></p>
<p><span class="math display">\[q_*(s, a) = E[R_t + γv_*(S_{t+1}) | S_t
= s, A_t = a]\]</span></p>
<p><strong><span style="color:purple;">逆矩阵法求解贝尔曼方程</span></strong>：</p>
<p><span class="math display">\[\mathbf{v}  = \mathbf{r} + γP\mathbf{v}
\]</span></p>
<p>其中 <strong>v</strong> 和 <strong>r</strong>
矢量，P是状态转移概率矩阵。求解如下：</p>
<p><span class="math display">\[\mathbf{v} = (I − γP
)^{-1}\mathbf{r}\]</span></p>
<p>复杂度为<span class="math inline">\(O(n^{3})\)</span>，考虑其他方法进行求解，如<span style="color:red;">动态规划、蒙特卡洛估计、时序差分法</span>等。</p>
<h2 id="动态规划">动态规划</h2>
<p>用动态规划算法在
能够获取MDP完整的环境信息（包括状态动作空间、转移矩阵、奖励等）的基础上
求解最优策略。</p>
<p><strong><span style="color:purple;">预测</span></strong>：给定一个MDP
&lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;和策略𝜋，输出基于当前策略𝜋的价值函数v。</p>
<p><strong><span style="color:purple;">控制</span></strong>：给定一个MDP
&lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;，输出最优价值函数𝑣∗以及最优策略𝜋∗</p>
<p><strong><span style="color:purple;">迭代策略评估</span></strong>：
预测问题，评估一个给定的策略<span class="math inline">\(\pi\)</span>。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/迭代策略评估.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/动态规划策略评估.png" alt="图片"></p>
<p><strong><span style="color:purple;">策略迭代</span></strong>：</p>
<ul>
<li>策略评估，在当前策略𝜋上迭代地计算𝑣值</li>
<li>策略更新，根据𝑣值贪婪地更新策略</li>
<li>如此反复多次，最终得到最优策略𝜋∗和最优状态价值函数𝑣∗ <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/动态规划策略迭代.png" alt="图片"></li>
</ul>
<p><strong><span style="color:purple;">价值迭代</span></strong>： <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/动态规划价值迭代.png" alt="图片"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学 林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索</title>
    <url>/2024/05/15/%E6%90%9C%E7%B4%A2/</url>
    <content><![CDATA[<pre><code>知识点：

经典搜索算法：基于路径的搜索的问题。如盲目搜索，启发式搜索。

局部搜索算法：最优化问题，没有初始状态，也没有终止状；并不需要到达这些解的路径。如爬山法，元启发算法。

元启发式算法是启发式算法的改进，它是随机算法与局部搜索算法相结合的产物。如禁忌搜索算法(Tabu Search)，模拟退火算法(Simulated annealing)，遗传算法(Geneticalgorithm)。

对抗搜索算法：也被称为博弈搜索，在一个竞争的环境中，智能体(agents)之间通过竞争实现相反的利益，一方最大化利益，另外一方最小化这个利益。如mini-max算法，Alpha-Beta算法、蒙特卡洛树。</code></pre>
<span id="more"></span>
<h2 id="搜索算法基础">1.搜索算法基础</h2>
<p>树搜索：</p>
<ul>
<li>结点：n.state, n.parent, n.action(父节点生成该节点时所采取的行动),
n.path-cost(从初始状态到达该结点的路径消耗g(n))</li>
<li>搜索策略：节点扩展到顺序</li>
<li>策略评价标注：完备性，时间复杂度，空间复杂度，最优性</li>
<li>复杂度表示：分子因子b，搜索树的中节点的最大分支树；深度d，目标结点所在的最浅深度；m，状态空间中，任何路径的最大长度</li>
</ul>
<p>图搜索：</p>
<ul>
<li>边缘(fringe)：待扩展的叶子结点，将状态空间分成已探索区域和未被探索区域。</li>
</ul>
<h2 id="盲目搜索">2.盲目搜索</h2>
<p>只能使用访问过的结点的信息。如<strong>宽度优先搜索</strong>，<strong>深度优先搜索</strong>，<strong>一致代价搜索</strong>（扩展路径消耗g(n)最小的结点），<strong>深度受限搜索</strong>（对深度优先搜索设置最大深度的界限l），<strong>迭代加深的深度优先搜索</strong>（不断增大深度限制，并且每次重新开始深度受限搜索）。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/盲目搜索.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h2 id="启发式搜索">3.启发式搜索</h2>
<p>优先扩展最优的结点。评价函数：f = g +
h。其中，g为一致代价，h为启发式函数，指的是结点n到目标结点的最小代价路径的代价估计值。</p>
<ul>
<li>贪婪算法：扩展离目标最近的结点，以期望很快找到解。f(n)=h(n)</li>
<li>A* 算法：避免扩展代价已经很高的路径。f(n) = g(n) + h(n)</li>
</ul>
<blockquote>
<p><strong>A* 算法:</strong></p>
<p><strong>可采纳启发式</strong>：永远不会高估代价，即<span class="math inline">\(h(n)\le h^*(n)\)</span>，且要求<span class="math inline">\(h(n)\ge 0\)</span>。</p>
<p>定理：如果h(n)是可采纳的，A*的树搜索版本是最优的。</p>
<p><strong>一致的启发式</strong>：对于每个结点n和通过任一行动a生成的后继结点n'，从结点n到达目标的估计代价不大于从n到n'的单步代价与从n到目标的估计代价之和，即<span class="math inline">\(h(n)\le c(n,a,n&#39;) +
h(n&#39;)\)</span>。如果h(n)是一致的，那么沿着任何路径的f(n)是非递减的。</p>
<p>定理：如果h(n)是一致的，那么A*的图搜索版本是最优的。</p>
<p>松弛问题：对原问题移除一些限制，一个松弛问题的最优解的代价是原问题的一个可采纳、一致的启发式。</p>
<p>松弛问题的最优解的代价不大于原问题最优解的代价。</p>
</blockquote>
<h2 id="局部搜索">4.局部搜索</h2>
<p>局部搜索：找到满足条件的状态，不关心路径，从单个当前节点(而不是多条路径)出发，通常只移动到它的邻近状态。</p>
<h3 id="爬山法">4.1 爬山法</h3>
<p>属于贪婪法，不断向值增加的方向移动，容易到达局部极大值。为了克服局部极大值，可以采用随机重启爬山法，完备的概率接近1。</p>
<h3 id="禁忌搜索算法">4.2 禁忌搜索算法</h3>
<p>从一个初始可行解出发，选择一系列的特定搜索方向（移动）作为试探，选择实现让特定的<strong>目标函数值变化最多</strong>的移动。</p>
<p>为了避免陷入局部最优解，TS搜索中采用了一种灵活的“记忆”技术，对已经进行的优化过程进行记录和选择，指导下一步的搜索方向，这就是Tabu表的建立。</p>
<p>标记已经解得的局部最优解或求解过程，并在进一步的迭代中避开这些局部最优解或求解过程。局部搜索的缺点在于，太过于对某一局部区域以及其邻域的搜索，导致一叶障目。为了找到全局最优解，<strong>禁忌搜索就是对于找到的一部分局部最优解，有意识地避开它，从而或得更多的搜索区域。</strong></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索2.png" alt="图片2"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索3.png" alt="图片3"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索4.png" alt="图片4"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索5.png" alt="图片2"></p>
<h3 id="模拟退火算法">4.3 模拟退火算法</h3>
<p>算法概述：</p>
<ul>
<li>若目标函数f在第i+1步移动后比第i步更优，即<span class="math inline">\(f(Y(i+1))\le
f(Y(i))\)</span>，则总是接受该移动。</li>
<li>若<span class="math inline">\(f(Y(i+1))&gt;f(Y(i))\)</span>，（即移动后的解比当前解要差），则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）。</li>
<li>Metroplis准则：温度越高，算法接受新解的概率就越高。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/模拟退火1.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/模拟退火2.png" alt="图片2"></p>
<h3 id="遗传算法">4.4 遗传算法</h3>
<p>基本思想：从初始种群出发，采用优胜劣汰、适者生存的自然法则选择个体，并通过杂交、变异来产生新一代种群，如此逐代进化，直到满足目标为止。</p>
<ul>
<li>种群：组候选解的集合，遗传算法正是通过种群的迭代进化，实现了最优解或者近似最优解。</li>
<li>个体：一个个体对应一个解，也就是构成种群的基本单元。</li>
<li>适应度函数:用来对种群中各个个体的环境适应性进行度量的函数，函数值是遗传算法实现优胜劣汰的主要依据。</li>
<li>遗传操作：作用于种群而产生新的种群的操作。如选择、交叉、变异。</li>
</ul>
<p>遗传编码：</p>
<ul>
<li>二进制编码</li>
<li>格雷编码，要求两个连续整数的编码之间只能有一个码位不同，其余码位都是完全相同的。</li>
<li>符号编码，个体染色体编码串中的基因值取自一个无数值含义、而只有代码含义的符号集</li>
</ul>
<p>适应度函数：</p>
<ul>
<li>原始适应度函数，直接将待求解问题的目标函数f(x)定义为遗传算法的适应度函数。它能够直接反映出待求解问题的最初求解目标但是有可能出现适应度值为负的情况。</li>
<li>标准适应度函数。在遗传算法中，一般要求适应度函数非负，并其适应度值越大越好。这就往往需要对原始适应函数进行某种变换，将其转换为标准的度量方式，以满足进化操作的要求，这样所得到的适应度函数被称为标准适应度函数<span class="math inline">\(f_{Normal}(x)\)</span></li>
</ul>
<p>选择：各个个体被选中的概率与其适应度大小成正比</p>
<ul>
<li>比例选择</li>
<li>锦标赛选择</li>
<li>轮盘赌选择</li>
</ul>
<p>交叉：</p>
<ul>
<li><p>单点交叉，先在两个父代个体的编码串中随机设定一个交叉点，然后对这两个父代个体交叉点前面或后面部分的基因进行交换，并生成子代中的两个新的个体。</p></li>
<li><p>两点交叉，先在两个父代个体的编码串中随机设定两个交叉点，然后再按这两个交叉点进行部分基因交换，生成子代中的两个新的个体。</p></li>
<li><p>多点交叉，从两个父代个体中选择多个交叉点，然后交换这些交叉点之间的基因片段，从而产生新的个体。</p></li>
<li><p>均匀交叉，父串中的每一位都是以相同的概率随机进行交叉的。</p></li>
<li><p>实值交叉，在实数编码情况下所采用的交叉操作，可分为部分离散交叉、整体交叉。</p>
<p>部分离散交叉: 先在两个父代个体的编码向量中随机选择一部分分量，
然后对这部分分量进行交换，生成子代中的两个新的个体。</p>
<p>整体交叉:
对两个父代个体的编码向量中的所有分量，都以1/2的概率进行交换，从而生成子代中的两个新的个体。</p></li>
<li><p>洗牌交叉，打乱之后再选择交叉点，再进行复原</p></li>
</ul>
<p>变异：</p>
<ul>
<li>二进制变异，随机地产生一个变异位，0-&gt;1，1-&gt;0</li>
<li>实值变异，用另外一个在规定范围内的随机实数去替换原变异位置上的基因值，产生一个新的个体。</li>
</ul>
<h2 id="对抗搜索">5.对抗搜索</h2>
<h3 id="alpha-beta算法">5.1 Alpha-Beta算法</h3>
<p>对mini-max算法的改进。剪枝本身不影响算法输出结果，但节点先后次序会影响剪枝效率。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Alpha-Beta%20剪枝1.png" alt="图片1"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Alpha-Beta%20剪枝2.png" alt="图片2"></p>
<h3 id="蒙特卡洛树">5.2 蒙特卡洛树</h3>
<p>参看蒙特卡洛树搜索部分。</p>
<h2 id="参考资料">6.参考资料</h2>
<p>中国科学院大学李国荣老师 高级人工智能课程课件</p>
<p>高级算法设计与分析 启发式算法 林海老师</p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>无模型强化学习</title>
    <url>/2024/05/15/%E6%97%A0%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<blockquote>
<p><strong>知识点</strong></p>
<p>1.无模型价值学习评估</p>
<ul>
<li>蒙特卡洛方法</li>
<li>时序差分学习</li>
<li>TD(𝝀)</li>
</ul>
<p>2.无模型策略优化控制</p>
<ul>
<li>蒙特卡洛策略迭代</li>
<li>时序差分策略迭代（SARSA）</li>
<li>Q值迭代 (Q-learning) <span id="more"></span></li>
</ul>
</blockquote>
<h2 id="无模型价值学习评估">1. 无模型价值学习评估</h2>
<h3 id="蒙特卡洛方法">1.1 蒙特卡洛方法</h3>
<p>蒙特卡洛方法是一种<strong>基于样本</strong>的方法，不需要知道环境的所有信息。只需基于过去的经验就可以学习。具体来说，给定一个策略
π，通过对 π 产生的回报取平均值来评估状态价值函数。这样就有两种估算方式:
<strong>首次蒙特卡罗</strong>(First-Visit Monte
Carlo)和<strong>每次蒙特卡罗</strong>(Every-Visit Monte
Carlo)。首次蒙特卡罗只考虑每一个回合中第一次到状态 s
的访问，而每次蒙特卡罗就是考虑每次到状态 s 的访问。</p>
<p>注意的是，和动态规划不同的是，蒙特卡罗不使用<strong>自举(Bootstrapping)</strong>，也就是说，它不用其他状态的估算来估算当前的状态值。
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/首次蒙特卡洛访问.png" alt="图片"></p>
<p>离线学习：智能体从预先收集好的数据中进行学习。</p>
<p>在线学习：智能体通过与环境实时交互来获取知识和经验。</p>
<h3 id="时序差分学习">1.2 时序差分学习</h3>
<p>时序差分学习方法同蒙特卡洛方法一样是不基于模型的，不需要马尔可夫决策过程的知识。但是时序差分学习方法可以直接从经历的不完整经历片段中学习，它通过<strong>自举(bootstrap)</strong>猜测经历片段的结果并不断更新猜测。即时序差分学习方法可以在每一次经历的过程中进行学习，而蒙特卡洛方法只能等到每次经历完全结束时才能进行学习。</p>
<p><span class="math display">\[𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝐺_t −
𝑉(𝑆_{𝑡}))\]</span></p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TD.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>对TD(0)，即one-step TD:</p>
<p><span class="math display">\[𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝑅_{𝑡+1} +
𝛾𝑉(𝑆_{t+1}) − 𝑉(𝑆_{𝑡}))\]</span></p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Sarsa价值评估.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>这个算法又被叫做<strong>SARSA</strong>，因为用到了 <span class="math inline">\((S_t, A_{𝑡}, R_{𝑡+1}, S_{𝑡+1},
A_{𝑡+1})\)</span>。</p>
<ul>
<li>蒙特卡洛方法没有偏倚，是对当前状态实际价值的无偏估计，但有着较高的变异性，且对初始值不敏感。</li>
<li>时序差分方法方差更低,
但有一定程度的偏差，对初始值较敏感，通常比蒙特卡洛方法更高效。</li>
</ul>
<h3 id="td𝝀">1.3 TD(𝝀)</h3>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/步数对TD的影响.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/反向TD_补充1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/反向TD_补充2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn-4.png" alt="图片"></p>
<h2 id="无模型策略优化控制">2.无模型策略优化控制</h2>
<h3 id="蒙特卡洛策略迭代">2.1 蒙特卡洛策略迭代</h3>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/MTCS策略迭代.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="时序差分策略迭代sarsa">2.2 时序差分策略迭代（SARSA）</h3>
<p><span class="math display">\[G_{t:t+n} = R_{t+1} + γR_{t+2} + \dot +
γ^{n−1}R_{t+n} + γ^nQ_{t+n−1}(S_{t+n}, A_{t+n})\]</span></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Sarsa价值评估.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/sarsa-n.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/sarsa-lambda.png" alt="图片"></p>
<h3 id="q值迭代-q-learning">2.3 Q值迭代 (Q-learning)</h3>
<p>Sarsa --&gt; on-policy</p>
<p>Q-learning --&gt; off-policy</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Q-learning策略迭代.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>强化学习入门——从原理到实践，叶强</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度强化学习</title>
    <url>/2024/05/15/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>强化学习从深度学习角度出发的挑战：</p>
<ul>
<li>强化学习的奖励信号是有延迟的，而深度学习的输入输出是直接联系的</li>
<li>强化学习的序贯决策序列有很高的相关性，而深度学习的假设数据是独立同分布</li>
<li>强化学习的数据分布是会随着学习发生变化的，而深度学习的假设是底层分布固定的</li>
</ul>
<span id="more"></span>
<h2 id="dqn算法">1. DQN算法</h2>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-4.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="dqn">1.1 DQN</h3>
<p>Deep Q-learning: DQN, <strong>Approximate <span class="math inline">\(Q^*(s,a)\)</span> by DQN,<span class="math inline">\(Q(s,a;w)\)</span></strong></p>
<p><strong>经历回放</strong>(experience replay): 在每个时间步t
中，DQN先将智能体获得的经验<span class="math inline">\((S_t, A_t, R_t,
S_{t+1})\)</span>存入回放缓存中，然后从该缓存中均匀采样小批样本用于
Q-Learning
更新。主要作用是<strong>解决数据的相关性和非静态分布问题</strong>。</p>
<p>DQN2015的改进：<strong>增加目标网络</strong>。目标网络通过使用旧参数生成
Q-Learning
目标，使目标值的产生不受最新参数的影响，从而大大减少发散和震荡的情况。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-3.png" alt="图片"></p>
<h3 id="double-dqn-ddqn">1.2 Double-DQN, DDQN</h3>
<p>Double DQN 是对 DQN
在减少过拟合方面的改进。这是由于DQN对动作值函数的max操作会引入一个正向的偏差，导致下一时刻的目标值存在过估计。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-5.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-6.png" alt="图片"></p>
<h3 id="prioritized-experience-replay-优先经验回放">1.3 Prioritized
Experience Replay, 优先经验回放</h3>
<p>采用优先级采样达到收敛所需的更新次数相比均匀采样要小很多，这也是进行优先经验池回放的原因。</p>
<p><strong>1.3.1 样本优先级</strong>:</p>
<p>样本优先级应满足两个条件：</p>
<ul>
<li>优先级在数值上应该和误差绝对值成单调递增关系，这是为了满足误差绝对值较大（即优先级较大）的样本获得更大的被抽样的机会；</li>
<li>优先级数值应大于0，这是为了保证每一个样本都有机会被抽样，即抽样概率大于0。</li>
</ul>
<p>优先级可以分为<strong>基于比例的样本优先级</strong>，<strong>基于排序的样本优先级</strong>。如下图所示：</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-7.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>1.3.2 随机优先级采样</strong>:</p>
<p>采样方法：</p>
<ul>
<li>贪婪优先级采样，完全按照优先级去采样</li>
<li>一致随机采样，均匀采样</li>
<li>随机优先级采样，随机采样</li>
</ul>
<p>基本原则：</p>
<ul>
<li>样本被采样的概率应该和样本优先级成正相关关系</li>
<li>每一个样本都应该有机会被采样，即被采样的概率大于0</li>
</ul>
<p>Sum-Tree随机优先级采样，属于基于比例的样本优先级：</p>
<p>重要性采样: 用一个分布来计算当前分布的期望。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-8.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="dueling-dqn">1.4 Dueling-DQN</h3>
<p>算法原理：将动作值的计算分解成状态值和优势函数，<span class="math inline">\(Q(s,a)=V(s)+A(s,a)\)</span>。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-9.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-10.png" alt="图片"></p>
<h2 id="策略梯度方法ddpg">2. 策略梯度方法DDPG</h2>
<h3 id="dpg-deterministic-policy-gradient-确定性策略梯度">2.1 DPG,
(Deterministic Policy Gradient) 确定性策略梯度</h3>
<p>确定性策略：每一步的动作都是确定的，即<span class="math inline">\(a=\mu_\theta(s)\)</span>。确定性策略梯度算法正是使用了确定性策略的策略梯度算法。</p>
<h3 id="ddpg-deep-deterministic-policy-gradient">2.2 DDPG, (Deep
Deterministic Policy Gradient)</h3>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DDPG-1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>几个trick：</p>
<ul>
<li><span class="math inline">\(\mu^{\prime}(s_t)=\mu(s_t|\theta_{t}^{\mu} +
N)\)</span>
添加了一个随迭代次数衰减的随机噪声，增加了动作空间的探索</li>
<li>目标网络缓慢更新保证了训练的稳定性</li>
<li>batch normalization 使得可以在不同的环境中获取的特征统一</li>
</ul>
<p>问题：值函数过估计；自举造成的偏差传播</p>
<h3 id="ppo">2.3 PPO</h3>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/PPO-1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="sac-soft-actoe-critic">2.4 SAC, (Soft Actoe Critic)</h3>
<p>熵：随机变量取各值时信息量的期望。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/SAC.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://github.com/QiangLong2017/Deep-Reiforcement-Learning" class="uri">https://github.com/QiangLong2017/Deep-Reiforcement-Learning</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱数据管理</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<pre><code>知识图谱的目标是构建一个能够刻画现实世界的知识库，为自动问答、信息检索等应用提供支撑。因此，对知识的持久化存储并提供对目标知识的高效检索/更新是合格的知识图谱（系统）必须具备的基本功能，也是知识图谱数据管理的主要研究内容。

课程复习使用。</code></pre>
<h2 id="一.符号化知识图谱数据管理">一.符号化知识图谱数据管理</h2>
<h3 id="知识图谱数据模型">1.1 知识图谱数据模型</h3>
<p>数据模型主要包含<strong>逻辑组织结构、操作、约束</strong>三部分，它决定了数据管理所采取的方法和策略，对于存储管理、查询处理、查询语言设计均至关重要。常见的数据模型有层次数据模型，网状数据模型，系数据模型。</p>
<ul>
<li>逻辑组织结构（数据结构）：描述数据的类型、内容、性质以及数据间的联系等。</li>
<li>数据操作：描述在相应的数据结构上的操作类型和操作方式。</li>
<li>数据约束：描述数据结构内数据间的语法、词义联系、他们之间的制约和依存关系，以及数据动态变化的规则，以保证数据的正确、有效和相容。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理2.png" alt="图片"></p>
<h3 id="知识图谱数据的存储">1.2 知识图谱数据的存储</h3>
<ul>
<li>基于表结构的存储：关系数据库。</li>
<li>基于图结构的存储：常见的图数据库存储系统，如Neo4j, OrientDB,
HyperGraphDB, InfiniteGraph, InfoGrid.</li>
</ul>
<h3 id="知识图谱数据的检索查询">1.3 知识图谱数据的检索（查询）</h3>
<p>知识图谱查询语言可分为
<strong>声明式语言</strong>（描述“是什么”而不是“怎么做”，它关注的是结果而不是实现结果的过程），<strong>过程式语言</strong>（描述“怎么做”，它定义了一系列的步骤或指令来实现目标，如图遍历、导航式游走）。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理4.png" alt="图片"></p>
<p>这些查询语言都有点类似于sql，可借助sql进行进一步理解。</p>
<h2 id="二.参数化知识图谱大模型数据管理">二.参数化知识图谱（大模型）数据管理</h2>
<h3 id="大模型知识存储">2.1 大模型知识存储</h3>
<p>一个流行的观点是：PLM的<strong>FFN</strong>（Feed Forward Nerual
Network）模块存储了大量知识。</p>
<h3 id="大模型的知识编辑检索与更新">2.2
大模型的知识编辑（检索与更新）</h3>
<ul>
<li>知识编辑的目标：定向更新模型中具有的知识，该过程应当尽可能地保证有效、能泛化、避免对无关知识产生不良影响</li>
<li>逻辑研究对象：三元组</li>
<li>评估方法：提示生成，看结果是否符合预期</li>
<li>评估指标：有效性，泛化性，专一性，流畅度</li>
<li>知识编辑的实现方法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理5.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理6.png" alt="图片"></p>
<h2 id="三.参考资料">三.参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱构建</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一-半结构化文本中的知识抽取">一.半结构化文本中的知识抽取</h2>
<p>目标：从百科普通条目半结构化网页中抽取实体属性名以及实体属性值。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA1.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA2.png" alt="图片"></p>
<h2 id="二-非结构化文本中的知识抽取">二.非结构化文本中的知识抽取</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA3.png" alt="图片"></p>
<h2 id="三-知识图谱众包构建">三.知识图谱众包构建</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA4.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA5.png" alt="图片"></p>
<h2 id="四-知识图谱质量控制">四.知识图谱质量控制</h2>
<p>知识图谱的质量控制指的是如何通过技术手段来确保知识图谱中知识的质量。</p>
<p><strong>评估思路</strong>：</p>
<ul>
<li>内检：利用知识图谱的内部知识进行综合推理，得到新的缺失知识，也可以发现相互矛盾的错误知识等。</li>
<li>外检：从外部知识源获得信息，然后结合知识图谱内部的知识进行比对，从而补全、修正或者更新知识图谱中的知识。</li>
</ul>
<p><strong>评估指标</strong>：准确性，一致性，完整性，时效性。</p>
<p><strong>评估方法</strong>：人工抽样检测法，一致性检测法，基于外部知识的对比评估法。</p>
<h2 id="五-参考资料">五.参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识建模与知识融合</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%BB%BA%E6%A8%A1%E4%B8%8E%E7%9F%A5%E8%AF%86%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<pre><code>课程复习使用，这章没有进行细致整理。

一. 知识建模

1.1 知识体系概述

1.2 典型知识体系

1.3 知识体系手工建模方法

1.4 知识体系自动建模方法

二. 知识融合

2.1 知识融合概述

2.2 知识体系融合方法

2.3 知识实例融合方法

三. 大模型中的知识融合

3.1 大模型对齐技术概述

3.2大模型对齐方法
</code></pre><h2 id="一-知识建模-amp-知识融合"><a href="#一-知识建模-amp-知识融合" class="headerlink" title="一. 知识建模 &amp; 知识融合"></a>一. 知识建模 &amp; 知识融合</h2><p>…</p>
<h2 id="二-大模型的知识融合"><a href="#二-大模型的知识融合" class="headerlink" title="二.大模型的知识融合"></a>二.大模型的知识融合</h2><p>SFT，RLHF，DPO。</p>
<h2 id="三-参考资料"><a href="#三-参考资料" class="headerlink" title="三.参考资料"></a>三.参考资料</h2><p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识推理</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/</url>
    <content><![CDATA[<pre><code>知识工程的生命周期：知识表示，知识建模，知识融合，知识管理，知识推理，知识应用。
</code></pre><h2 id="一-知识推理概述"><a href="#一-知识推理概述" class="headerlink" title="一.知识推理概述"></a>一.知识推理概述</h2><p>概念区分：<strong>推理 reasoning</strong>，指的是通过已知知识推断出未知知识的过程。而<strong>推断 inference</strong> 是推理的一个步骤，指的是神经网络中的前向计算。</p>
<p><strong>按新知识推出途径的分类</strong>：</p>
<ul>
<li>归纳推理（特殊-&gt;一般），归纳推理所推出的结论没有包含在前提内容中。这种由个别事物或现象推出一般性知识的过程，是增加新知识的过程。</li>
<li>演绎推理（一般-&gt;特殊），演绎推理只不过是将已有事实揭示出来，因此它不能增加新知识。</li>
<li>缺省推理，也称默认推理，在知识不完全的情况下作出的推理，通常的形式：如果没有足够的证据证明结论不成立，则认为结论是正确的。</li>
<li>溯因推理，也称反绎推理、反向推理。，是推理到最佳解释的过程。它是开始于事实的集合，并推导出其最佳解释的推理过程。</li>
</ul>
<p><strong>按前提与结论的联系性质的分类</strong>：</p>
<ul>
<li>必然性推理，前提与结论有必然性联系，即前提蕴含结论。传统逻辑中通过直言命题变形的直接推理（换质法、换位法推理等）、通过命题间对应关系所进行的直接推理、三段论推理、各种假言推理、选言推理以及完全归纳推理等等，都属于必然性推理。</li>
<li>或然性推理，前提与结论无蕴含关系。简单枚举归纳推理、类比推理、回溯推理等等都属于或然性推理。例如，P：房间里有物品 → C：房子会着火，前提P只是结论C的“疑似”必要条件。</li>
</ul>
<p><strong>按推理过程单调性的分类</strong>：</p>
<ul>
<li>单调推理，是指在推理过程中随着推理的向前推进以及新知识的加入，推出的结论呈单调增加的趋势并越来越接近最终目标，且在推理过程中不会出现反复的情况，即不会因新知识的加入而否定前面推出的结论，从而使推理又退回到前面的某一步。</li>
<li>非单调性推理，是指在推理过程中，由于新知识的加入，不仅没有加强已经推出的结论，反而要否定它，使其需要退回到之前步骤。</li>
</ul>
<p><strong>知识确定性的分类</strong>：</p>
<ul>
<li>确定性推理大多指确定性逻辑推理，它具有完备的推理过程和充分的表达能力，可以严格地按照专家预先定义好的规则准确地推导出最终结论。但是确定性推理很难应对真实世界中，尤其是存在于网络大规模知识图谱中的不确定甚至不正确的事实和知识。</li>
<li>不确定性推理：并不是严格地按照规则进行推理，而是根据以往的经验和分析，结合专家先验知识构建概率模型，并利用统计计数、最大化后验概率等统计学习的手段对推理假设进行验证或推测。</li>
</ul>
<p><strong>按实现技术</strong>：</p>
<ul>
<li>逻辑推理，过程包含了严格的约束和推理过程（研究较多）。</li>
<li>非逻辑推理，自然语言推理，推理过程相对模糊。</li>
<li>符号推理，符号推理的特点就是在知识图谱中的实体和关系符号上直接进行推理。确定性和不确定性逻辑推理都属于符号推理。</li>
<li>数值推理，使用数值计算，尤其是向量矩阵计算的方法，捕捉知识图谱上隐式的关联，模拟推理的进行。</li>
</ul>
<p><strong>应用</strong>：知识图谱补全，知识问答，搜索与推荐，行业应用。</p>
<h2 id="二-演绎推理：推理具体事实，一般-gt-特殊"><a href="#二-演绎推理：推理具体事实，一般-gt-特殊" class="headerlink" title="二.演绎推理：推理具体事实，一般-&gt;特殊"></a>二.演绎推理：推理具体事实，一般-&gt;特殊</h2><ul>
<li>经典逻辑推理：命题逻辑推理。</li>
<li>基于产生式规则的推理</li>
<li>基于概率逻辑学习的推理：马尔可夫逻辑网</li>
<li>自然语言演绎推理。</li>
</ul>
<h2 id="三-归纳推理：学习推理规则"><a href="#三-归纳推理：学习推理规则" class="headerlink" title="三.归纳推理：学习推理规则"></a>三.归纳推理：学习推理规则</h2><h2 id="四-基于深度学习的知识推理方法"><a href="#四-基于深度学习的知识推理方法" class="headerlink" title="四.基于深度学习的知识推理方法"></a>四.基于深度学习的知识推理方法</h2><ul>
<li>基于表示学习</li>
<li>基于强化学习</li>
</ul>
<h2 id="五-大模型的推理方法"><a href="#五-大模型的推理方法" class="headerlink" title="五.大模型的推理方法"></a>五.大模型的推理方法</h2><p>相比于传统方法的优势：由于LLM本身已经具备了出色的各类能力（e.g. 工具调用、知识生<br>成等），因此LLM推理相比于传统推理有更多的形式和种类；CoT工作指出可以简单通过ICL的方式激发LLM的推理能力，因此与传统方式的大量训练相比，LLM的推理成本低、效率高、泛化性强。</p>
<p>相比于传统方法的劣势：部分LLM为黑盒模型，只能调整输入输出，推理过程的不透明度更高；LLM参数量大，幻觉现象严重，导致推理的输出更不可控且不稳定。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识推理1.png" alt="图片"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识推理2.png" alt="图片"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识推理3.png" alt="图片"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识推理4.png" alt="图片"></p>
<h2 id="六-参考资料"><a href="#六-参考资料" class="headerlink" title="六.参考资料"></a>六.参考资料</h2><p>中国科学院大学老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识获取</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96/</url>
    <content><![CDATA[<pre><code>信息抽取：从自然语言文本中抽取指定类型的实体、关系、事件等事实信息，并形成结构化数据输出的文本处理技术。
</code></pre>
<h2 id="1-命名实体识别">1.命名实体识别</h2>
<h3 id="1-1-基于词典的方法">1.1 基于词典的方法</h3>
<p>典型方法包括正向匹配方法，反向匹配方法。原理：按照一定的策略将待分析的汉字串与一个充分大的词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。</p>
<h3 id="1-2-基于统计的方法">1.2 基于统计的方法</h3>
<ul>
<li>生成式方法，首先建立学习样本的生成模型，再利用模型对预测结果进行间接推理，如HMM。</li>
<li>判别式方法，基于由字构词的命名实体识别理念，将NER问题转化为判别式分类问题(序列标注问题)，如Maxent，SVM，CRF，CNN，RNN，LSTM+CRF。CRF做解码善于捕捉近距离的标签依赖，LSTM可以捕捉长距离的标签依赖。</li>
</ul>
<h3 id="1-3-基于大模型的方法">1.3 基于大模型的方法</h3>
<p>难点1: 任务形式差距。命名实体识别通常建模为序列标注任务，而大模型往往用于完成文本<br>
生成任务。难点2: 大模型存在较为严重的幻觉问题。</p>
<h2 id="2-关系知识抽取">2.关系知识抽取</h2>
<p>关系抽取：旨在自动识别由一对概念和联系这对概念的关系构成的相关三元组。如CEO(比尔盖茨，微软)。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%961.png" alt="图片"></p>
<h2 id="3-事件知识抽取">3.事件知识抽取</h2>
<p>事件是发生在某个特定的时间点或时间段、某个特定的地域范围内，由一个或者多个角色参与的一个或者多个动作组成的事情或者状态的改变。</p>
<p>事件关系：共指，时序，因果，子事件。</p>
<h2 id="4-参考资料">4.参考资料</h2>
<p>中国科学院大学赵军老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识计算</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h2 id="一.-命题逻辑">一. 命题逻辑</h2>
<h3 id="基本概念">1.1 基本概念</h3>
<p><span style="color: purple;">知识库</span>：KB,
利用形式化语言定义的句子的集合。</p>
<p><span style="color: purple;">逻辑</span>：用于表示信息的形式化语言。</p>
<p><span style="color: purple;">语法</span>：句子的形式化结构。</p>
<p><span style="color: purple;">语义</span>：语句在每个可能模型中的真值。</p>
<p><span style="color: purple;">模型</span>： m, m是语句 <span class="math inline">\(\alpha\)</span> 的一个模型，表示语句在模型m
中为真，也称m 满足 <span class="math inline">\(\alpha\)</span>。</p>
<p><span style="color: purple;">逻辑</span>：用于表示信息的形式语言。</p>
<p><span style="color: purple;">逻辑蕴涵</span>：某语句在逻辑上跟随另一个语句，如
<span class="math inline">\(\alpha \models \beta\)</span>，当且仅当在
<span class="math inline">\(\alpha\)</span> 为真的模型中，<span class="math inline">\(\beta\)</span> 也为真，即 <span class="math inline">\(M(\alpha) \subseteq
M(\beta)\)</span>。同样的，对于知识库 <span class="math inline">\(KB
\models \alpha\)</span>，则有 <span class="math inline">\(M(KB)
\subseteq M(\alpha)\)</span>。如果算法i 可以根据KB推导出 <span class="math inline">\(\alpha\)</span>，记为 <span class="math inline">\(KB\vdash _i \alpha\)</span>。</p>
<p><span style="color: purple;">可靠性</span>：只能生成被蕴含的语句。即永远不会返回一个错误的结果。</p>
<p><span style="color: purple;">完备性</span>：生成所有被蕴含的语句。即返回的结果总能覆盖所有正确的解。</p>
<p><span style="color: purple;">命题逻辑</span>：应用一套形式化规则对以符号表示的描述性陈述进行推理的系统。</p>
<p><span style="color: purple;">原子命题</span>：一个或真或假的描述性陈述语句，并且无法再分解为更简单的命题
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑1.png" alt="图片"></p>
<p>理解：<span class="math inline">\(S_1\)</span> 不成立时，相当于 <span class="math inline">\(S_1\)</span>
为空集，空集可以被其他所有集合包含，因此 <span class="math inline">\(S_1\)</span> 不成立时 <span class="math inline">\(S_1 \Rightarrow S_2\)</span> 为真。</p>
<p><span style="color: purple;">逻辑等价</span>：命题p 和命题q
在所有情况下都具有相同的真假结果。 <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑2.png" alt="图片"></p>
<p><span style="color: purple;">有效性</span>：一个句子在所有模型中都为真，则句子是有效的。</p>
<p><span style="color: purple;">可满足性</span>：一个句子在某些模型中为真。</p>
<h3 id="归结原理">1.2 归结原理</h3>
<p>在命题逻辑中，归结规则是可靠的、完备的。</p>
<p><strong>反证法</strong>：证明：<span class="math inline">\(KB\vdash  \alpha\)</span>，只需证明<span class="math inline">\(KB\wedge \neg \alpha\)</span>。</p>
<p><strong>子句集S 的归结闭包<span class="math inline">\(RC(S)\)</span></strong>：对S
中的子句或其派生子句反复使用归结规则而生成的所有子句的集合。</p>
<p><strong>限定子句</strong>：是受限形式的一种，它是指恰好只有一个正文字的析取式。</p>
<p><strong>Horn子句</strong>：包含至多一个正文字的析取式。用Horn子句判定蕴涵需要的时间与知识库的大小呈线性关系。如前向链接与反向链接。</p>
<p><strong>前向链接</strong>：如果蕴含式的所有前提已知，那么就把它的结论添加到已知事实集，真到查询q
被添加或者无法进一步推理。</p>
<p><strong>反向链接</strong>：从查询q 开始推理。如果查询q
为真，那么无须任何操作。否则寻找知识库中那些能以q
为结论的蕴含式。如果其中某个蕴含式的所有前提都能证明为真，那么q
为真。</p>
<h2 id="二.-谓词逻辑">二. 谓词逻辑</h2>
<p><strong>命题逻辑的局限性</strong>：
在命题逻辑中，每个陈述句是最基本的单位（即原子命题），无法对原子命题进行分解。因此，在命题逻辑中，不能表达<strong>局部与整体、一般与个别</strong>的关系，即使不同原子命题蕴含个体、群体和关系等内在丰富语义。</p>
<p><strong>谓词逻辑</strong>：将原子命题进一步细化，分解出<strong>个体、谓词和量词</strong>，来表达个体与总体的内在联系和数量关系。</p>
<p><strong>谓词</strong>：刻画个体属性或者描述个体之间关系存在性的元素,值域为{True,False}。</p>
<p><strong>量词</strong>：全称量词 <span class="math inline">\(\forall\)</span>，存在量词 <span class="math inline">\(\exists\)</span>。</p>
<p><strong>量词对偶</strong>：全称量词与存在量词可以相互转换，互相表示。</p>
<h2 id="三.-不确定性推理">三. 不确定性推理</h2>
<p><strong>效用</strong>：对不同决策结果的偏好。决策理论 = 概率理论 +
效用理论。</p>
<p><strong>不确定性推理</strong>：从不确定的初始证据出发，通过运用不确定性知识，推导出具有一定程度不确定性但合理的结论的思维过程。</p>
<p><strong>贝叶斯网</strong>：一个有向无环图模型，用简单的条件分布描述复杂的联合分布。其中，每个结点有一个条件概率分布<span class="math inline">\(P(X_i |
Parents(X_i))\)</span>，量化其父结点对该结点的影响。所有条件概率构成条件概率表。</p>
<p><strong>贝叶斯网独立条件</strong>：一个结点的概率只与其父结点有关，与其它祖先结点无关。</p>
<p><strong>马尔科夫毯</strong>：一个结点的父结点、子结点、子结点的父结点。</p>
<p><strong>贝叶斯球算法</strong>：</p>
<pre><code>假设在贝叶斯网络中有一个按一定规则运动的球。已知中间结点 (或结点集合)Z，如果球不能由结点X 出发到达结点Y (或者由Y 到X )，则称X 和Y 关于Z 独立。

通过： 父结点-&gt;当前结点 方向的球，访问当前结点的任意子结点。子结点-&gt;当前结点 方向的球，访问当前结点的任意父结点。即父-&gt;子，子-&gt;父 。

反弹：父结点-&gt;当前结点 方向的球，访问当前结点的任意父结点。子结点-&gt;当前结点 方向的球，访问当前结点的任意子结点。即父-&gt;父，子-&gt;子。

截止：当前结点阻止贝叶斯球继续运动。

贝叶斯球规则：未知结点：父-&gt;子，子-&gt;父/子 。已知结点：父-&gt;父，子-&gt;截止。</code></pre>
<p><strong>枚举推理（精确推理</strong>）：所有的变量集合X
,所有的证据变量集合E ,查询变量Q ,隐藏变量集合H = X - E- Q。则<span class="math inline">\(P(Q|E=e)=\alpha P(Q,E=e)=\alpha\sum_{H=h}
(Q,E=e,H=h)\)</span></p>
<p><strong>近似推理</strong>：抽取样本形成采样分布，通过该分布近似后验概率。</p>
<blockquote>
<p><strong>采样方法</strong>：</p>
<p>先验采样：按拓扑顺序进行采样，从<span class="math inline">\(P(X_i |
Parents(X_i))\)</span>采样<span class="math inline">\(X_i\)</span>。</p>
<p>拒绝采样：拒绝不符合证据E 的样本，即若<span class="math inline">\(X_i\)</span>与观察值不一样，放弃这个样本。</p>
<p>似然采样：只生成与证据一致的事件，即固定证据变量。但这会导致采样分布与实际分布不一致，因此正需要对样本进行加权，权重为该样本与证据的相似性。</p>
<p>吉布斯采样：固定证据变量，每次模拟一个隐藏变量的采样，从<span class="math inline">\(X_i ～ P(X_i |
马尔可夫毯(X_i))\)</span>，使频率收敛到真实概率。 <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/吉布斯采样1.png" alt="图片"></p>
</blockquote>
<h2 id="四.-参考资料">四. 参考资料</h2>
<p>中国科学院大学高级人工智能，李国荣老师，课程课件</p>
<p><a href="https://www.bilibili.com/video/BV1ZY4y1J78y/?spm_id_from=333.788&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d" class="uri">https://www.bilibili.com/video/BV1ZY4y1J78y/?spm_id_from=333.788&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>蒙特卡洛树搜索</title>
    <url>/2024/05/14/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2/</url>
    <content><![CDATA[
]]></content>
      <tags>
        <tag>强化学习</tag>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>连续状态系统基于模型的强化学习</title>
    <url>/2024/05/15/%E8%BF%9E%E7%BB%AD%E7%8A%B6%E6%80%81%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<blockquote>
<p>对于大规模的MDP问题，不希望使用查表（Table
Lookup）的方式，通过<strong>函数近似</strong>来估计实际的价值函数的方式，既节约了资源，又能达到泛化的效果。</p>
<ul>
<li><span class="math inline">\(\hat{v}(s,w) = v_\pi (s)\)</span></li>
<li><span class="math inline">\(\hat{q}(s,a,w) = q_\pi
(s,a)\)</span></li>
<li><span class="math inline">\(\hat{\pi}(a,s,w) = \pi
(a|s)\)</span></li>
</ul>
<p><strong>函数近似器</strong></p>
<ul>
<li>特征的线性组合</li>
<li>神经网络</li>
<li>决策树</li>
<li>最近邻方法</li>
<li>傅立叶/小波变换 <span id="more"></span></li>
</ul>
</blockquote>
<h2 id="价值函数近似-value-fuction-approximation-vfa">1. 价值函数近似,
Value Fuction Approximation, VFA</h2>
<p>近似函数逼近的类型：</p>
<ul>
<li>input: s, output: <span class="math inline">\(\hat{v}(s,w)\)</span></li>
<li>input: s, output: <span class="math inline">\(\hat{q}(s,a,w)\)</span></li>
<li>input: s, output: <span class="math inline">\(\hat{q}(s,a_1,w),\dots,\hat{q}(s,a_m,w)\)</span></li>
</ul>
<h3 id="线性函数近似">1.1 线性函数近似</h3>
<p>近似价值函数: <span class="math inline">\(\hat{v}(s,w)=x(s)^Tw\)</span></p>
<p>目标函数:
均方误差。由于实际的价值函数不可知，用样本近似期望损失。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/值函数近似2.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="神经网络值函数近似">1.2 神经网络值函数近似</h3>
<p>参看 深度强化学习 部分。</p>
<h3 id="基于模型的近似值迭代算法">1.3 基于模型的近似值迭代算法</h3>
<h3 id="模型无关的近似值迭代算法">1.4 模型无关的近似值迭代算法</h3>
<h2 id="近似策略迭代">2. 近似策略迭代</h2>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/值函数近似1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://www.bilibili.com/video/BV11V411f7bi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d" class="uri">https://www.bilibili.com/video/BV11V411f7bi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>群体智能</title>
    <url>/2024/05/31/%E7%BE%A4%E4%BD%93%E6%99%BA%E8%83%BD/</url>
    <content><![CDATA[<blockquote>
<p>知识点：</p>
<ul>
<li>演化计算：蚁群优化算法、粒子群优化算法</li>
<li>博弈论</li>
<li>课程复习使用</li>
</ul>
</blockquote>
<h2 id="一.-群体智能">一. 群体智能</h2>
<p><strong>群体智能</strong>：Social/Colletive Intelligence,
“无智能”或者仅具有相对简单智能的个体通过合作表现出更高智能行为的特性。其中的“无智能”/简单智能并不是绝对意义上的智能，而是相对于群体表现出来的相对智能。（众人拾材火焰高）</p>
<p><strong>集群智能</strong>：Swarm Intelligence,
众多无智能的个体，通过相互之间的简单合作，所表现出来的智能行为。
特点：</p>
<ul>
<li>分布式，无中心控制</li>
<li>随机性，非确定性</li>
<li>自适应，个体根据环境进行策略调整</li>
<li>正反馈，个体好的尝试会对个体产生正反馈</li>
<li>自发涌现，会在群体层面涌现出一种智能</li>
</ul>
<p><strong>博弈</strong>： Game Theory,
具备一定智能的理性个体，按照某种机制行动，群体层面表现出的智能。</p>
<p><strong>众包</strong>：Crowdsourcing,
设计合适的机制，激励个体参与，从而实现单个个体不具备的社会智能。</p>
<h2 id="二.-演化计算">二. 演化计算</h2>
<h3 id="蚁群优化算法">2.1 蚁群优化算法</h3>
<p>Ant Colony Optimization, AOC.
<strong>在图上寻找最优路径问题</strong>。</p>
<p><strong>形式化</strong>：蚂蚁(智能体)
依据一定的概率选择位置进行移动，途中会留下信息素，信息素会随时间挥发，且信息素浓度与该位置被选择的概率成正相关。</p>
<p><span style="color:purple;">用蚁群优化算法求解TSP问题</span>：</p>
<ul>
<li><strong>TSP问题描述</strong>：给定n个城市及每对城市之间的距离，求解访问每个城市一次、并回到起点的最短回路。</li>
<li><strong>符号表示</strong>：n个城市的有向图<span class="math inline">\(G = (V,E)\)</span>，其中 <span class="math inline">\(V={1,2,\dots,n}\)</span>,<span class="math inline">\(E={(i,j)|i,j\in V}\)</span>,<span class="math inline">\(d_{ij}\)</span>为节点之间的距离</li>
<li><strong>目标函数</strong>：<span class="math inline">\(min
f(w)=\sum_{l=1}^{n} d_{i_{l}i_{l+1}}\)</span>,其中，<span class="math inline">\(s=(i_{1},\dots,i_{n})\)</span></li>
<li><strong>根据信息素来选择下一个城市的概率计算为</strong>：</li>
</ul>
<p><span class="math display">\[
p_{i j}^{k}(t)=\left\{\begin{array}{ll}
\frac{\left(\tau_{i j}(t)\right)^{\alpha}\left(\eta_{i
j}(t)\right)^{\beta}}{\sum_{k \in \text { allowed }}\left(\tau_{i
k}(t)\right)^{\alpha}\left(\eta_{i k}(t)\right)^{\beta}} &amp; j \in
\text { allowed } \\
0, &amp; \text { otherwise }
\end{array}\right.
\]</span></p>
<p>其中，i为当前城市,j为下一城市，<span class="math inline">\(\tau_{i
j}(t)\)</span>为边<span class="math inline">\((i,j)\)</span>上的信息素浓度, <span class="math inline">\(\eta_{i j}(t)=1/d_{i
j}\)</span>是根据距离定义的启发式函数，<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\beta\)</span>反映了信息素与启发信息的相对重要性。</p>
<ul>
<li><strong>信息素更新</strong></li>
</ul>
<p><span class="math display">\[\begin{array}{l}\Delta \tau_{i
j}^{k}=f(x)=\left\{\begin{array}{cc}\frac{Q}{L_{k}}, &amp; (i, j) \in
w_{k} \\0, &amp; \text { otherwise }\end{array}\right. \\\tau_{i
j}(t+1)=\rho \cdot \tau_{i j}(t+1)+\Delta \tau_{i j} \\\Delta \tau_{i
j}=\sum_{k=1}^{m} \Delta \tau_{i j}^{k}\end{array}\]</span></p>
<p>其中：<span class="math inline">\(Q\)</span> 为常数，<span class="math inline">\(w_k\)</span> 表示第 <span class="math inline">\(k\)</span> 只蚂蚁在本轮迭代中走过的路径，<span class="math inline">\(L_k\)</span> 为路径长度，<span class="math inline">\(\rho\)</span> 为小于 1
的常数，反映信息素挥发速度。即路径越长，信息素越小。</p>
<ul>
<li><strong>算法流程</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 初始化 随机放置蚂蚁</span><br><span class="line"><span class="number">2.</span> 迭代过程</span><br><span class="line">    t = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> t &lt;= ItCount do (执行迭代)</span><br><span class="line">        <span class="keyword">for</span> k = <span class="number">1</span> to m do (对 m 只蚂蚁循环)</span><br><span class="line">            <span class="keyword">for</span> j = <span class="number">1</span> to n - <span class="number">1</span> do (对 n 个城市循环)</span><br><span class="line">                根据概率选择下一个城市；</span><br><span class="line">                将 j 置入禁忌表，蚂蚁转移到 j；</span><br><span class="line">            end <span class="keyword">for</span></span><br><span class="line">            计算每只蚂蚁的路径长度 \( L_k \);</span><br><span class="line">        end <span class="keyword">for</span></span><br><span class="line">        更新所有蚂蚁路径上的信息量；</span><br><span class="line">        t = t + <span class="number">1</span>;</span><br><span class="line">    end <span class="keyword">while</span></span><br><span class="line"><span class="number">3.</span> 输出结果</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>缺点</strong>：收敛速度慢（与m,n取值相关），容易陷入局部最优解(探索与开发的平衡)，不适于求解连续空间的优化问题。</li>
</ul>
<h3 id="粒子群优化算法">1.2 粒子群优化算法</h3>
<p>Particle Swarm Optimization, PSO.
求解<strong>连续解空间</strong>的优化问题，主要启发来源于对⻦群群体运动行为的研究。</p>
<ul>
<li><strong>形式化</strong>：每一只鸟(称为粒子，代表一个可行解解)
都有自己的状态信息：位置与速度，同时可以获得领域内其它鸟的信息，根据这些信息不断的改变自己的状态，去更好的适应环境，最终能找到最近最优解。</li>
<li><strong>算法流程</strong></li>
</ul>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/粒子群算法1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>其中，<span class="math inline">\(x_{n}^{(i)}\)</span>为粒子i 在第 n
轮的位置，<span class="math inline">\(v_{n}^{(i)}\)</span>为粒子i 在第 n
轮的速度，<span class="math inline">\(p_{best}^{(i)}\)</span>为粒子i
的历史最好位置,<span class="math inline">\(g_{best}^{(i)}\)</span>全局最好的历史位置。</p>
<p>速度更新公式包含三项，第一项为<strong>惯性项</strong>（保持原速度不变的倾向），第二项为<strong>记忆项</strong>（回到历史最好位置的倾向），第三项为<strong>社会项</strong>（走向粒子群全局最好位置的倾向）。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/粒子群算法2.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<ul>
<li><strong>算法终止条件</strong>：迭代次数，最佳位置连续为更新的次数，适应度函数的值达到预期要求。</li>
<li><strong>优化</strong>：对惯性项加上一个权重</li>
<li><strong>优点</strong>：收敛速度快，所需微粒群规模较小</li>
<li><strong>缺点</strong>：不保证收敛到全局最优解</li>
</ul>
<h2 id="三.-博弈">三. 博弈</h2>
<ul>
<li><p><strong>博弈类型</strong></p>
<p>静态博弈（同时选择策略） vs 动态博弈（按顺序选择策略）</p>
<p>竞争博弈（炒股） vs 合作博弈（结盟）</p>
<p>完全信息博弈（每个局中人对所有局中人的策略及效用充分了解） vs
不完全信息博弈</p></li>
<li><p><strong>效用函数</strong>，payoff，通常用<span class="math inline">\(U\)</span>来表示，是局势、时间（动态博弈中）的函数。每个局中人都有自己的效用函数。希望效用函数越大越好。</p></li>
<li><p><strong>最佳应对</strong>：对局中人1，若 <span class="math inline">\(U_1(s,t) \ge U_1(s&#39;,t)\)</span> ，其中 s'
是局中人除 s 外的其它策略，t 为局中人2的策略，<span class="math inline">\(U_{1}(s,t)\)</span>为局中人1
从这组决策中获得的收益，则称策略 𝑠 是局中人1对局中人2的策略 t
的最佳应对。</p></li>
<li><p><strong>最优策略</strong>：如果一个局中人的某个策略对其它局中人的任何策略都是最佳应对，那么这个策略就是该局中人的占优策略。</p></li>
<li><p><strong>纳什均衡</strong>：如果一个局势下，每个局中人的策略都是相对其他局中人当前策略的最佳应对，则称该局势是一个纳什均衡。也就是博弈进入了僵局。</p></li>
<li><p><strong>混合策略</strong>：每个局中人以某个概率分布在其策略集合中选择策略。</p></li>
<li><p><strong>混合策略纳什均衡</strong>：给定其他局中人的策略选择概率分布的情况下，
当前局中人选择任意一个(纯)策略获得的期望效用相等。</p></li>
<li><p><strong>纳什定理</strong>：任何有限博弈都至少存在一个纳什均衡。但寻找博弈的纳什均衡是困难的。</p></li>
<li><p><strong>帕累托最优</strong>：对于一组策略选择(局势)，若不存在其他策略选择
使<strong>所有参与者</strong>得到至少和目前一样高的回报，且至少一个参与者会得到严格较高的回报，则这组策略选择为帕累托最优。</p></li>
<li><p><strong>社会最优</strong>：使参与者的回报之和最大的策略选择(局势)。社会最优的结果一定也是帕累托最优的结果，但帕累托最优不一定是社会最优。</p></li>
<li><p><strong>机制设计</strong>：设计一个博弈，使其达到预期结果，如实现社会最优。</p></li>
<li><p><strong>maxmin策略</strong>，以我为主，最小化损失，抑制风险 <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/博弈1.png" alt="图片"></p></li>
<li><p><strong>minmax策略</strong>，抑制对手 <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/博弈2.png" alt="图片"></p></li>
<li><p><strong>匹配市场</strong></p>
<p>匹配定理：对于左右两部节点数相同的二部图，如果其不存在完全匹配（刚好一一对应），那么该二部图一定包含一个受限集。</p>
<p>匹配的效用：成功匹配的估价之和，称为匹配的效用。</p>
<p>最优匹配：效用最大的匹配。最优匹配对于个体而言不一定是最优的，甚至是最差的。</p>
<p>市场结清：每个卖方和买方都成交了。市场结清价格总是存在，且使得买卖双方总效用最优。
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/博弈3.png" alt="图片"></p></li>
<li><p><strong>中介市场</strong></p>
<p>买方和卖方通过中介交易。竞争不充分的地方，中介垄断价格。竞争充分的地方，中介的收益趋近于0。</p></li>
</ul>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/博弈4.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<ul>
<li><p><strong>议价权</strong></p>
<p>问题描述：给定一个网络，每个节点代表一个人，达成议价约定的两个人可以分配价值为1的东西。</p>
<p>不稳定边：对于结局中未参与配对的边，如果边的两个端点获得的收益之和小于1，则称这条边为不稳定边。</p>
<p>稳定结局：不存在不稳定边。</p>
<p>有备选项的议价：A、B两人议价，确定分配比例。A的备选项收益为x
，B的备选项为y 。要求 <span class="math inline">\(x+y\le
1\)</span>，否则A和B达不成交易。则定义剩余价值为<span class="math inline">\(s=1-x-y\)</span>。</p>
<p>纳什议价解：A的收益 <span class="math inline">\(x+\frac{s}{2}=\frac{1+x-y}{2}\)</span>, B的收益
<span class="math inline">\(y+\frac{s}{2}=\frac{1-x+y}{2}\)</span>。</p>
<p>均衡结局：结局中的任意一个参与配对的边都满足纳什议价解的条件。</p></li>
</ul>
<h2 id="四.-参考资料">四. 参考资料</h2>
<p>中国科学院大学 计院高级人工智能课程课件</p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>数值化知识表示</title>
    <url>/2024/06/01/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<pre><code>语言模型发展四范式：形式语言模型，统计语言模型，神经语言模型，预训练语言模型。

课程复习使用。
</code></pre>
<h2 id="一-语言的分布表示">一. 语言的分布表示</h2>
<ul>
<li>
<p><strong>Harris分布假说</strong>：上下文相似的词，其语义也相似。认为词的语义可以根据上下文统计获得，词之间的相似性可以通过向量距离衡量。</p>
</li>
<li>
<p><strong>word2vect</strong>: 词嵌入，上下文预测目标词</p>
</li>
<li>
<p><strong>CBOW</strong>：目标词预测上下文。</p>
</li>
</ul>
<h2 id="二-知识的分布表示">二. 知识的分布表示</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA1.png" alt="图片"></p>
<ul>
<li>
<p>打分函数：</p>
<p>位移距离模型。位移距离模型 (translational distance models)：基于位移假设，即头尾实体的表示存在位移关系，采用基于“头尾实体表示的位移”与“关系表示”的距离作为打分函数来衡量三元组成立的可能性。</p>
<p>语义匹配模型。无上述假设，直接利用头实体、关系和尾实体的数值表示进行计算，采用基于相似度的打分函数来衡量三元组成立的可能性。</p>
</li>
<li>
<p>模型训练：</p>
<p>封闭世界假设，但凡未在知识图谱中出现的事实都是错误的。</p>
<p>开放世界假设，知识图谱只包括正确的事实，那些不在其中出现的事实要么是错误的，要么是缺失的。</p>
</li>
</ul>
<h2 id="三-预训练语言模型">三. 预训练语言模型</h2>
<ul>
<li>Elmo: Embeddings from Language Models，首次使用大规模语料训练一个两层双向的RNN。</li>
<li>Bert: Bidirectional Encoder Representations from Transformers, transformer结构的encoder。</li>
<li>GPT: Generative Pre-Training,transformer结构的decoder。</li>
<li>In-context learning, CoT, few-shot learning, SFT, RLHF。由于对llm 部分比较了解，此处省略。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA2.png" alt="图片"></p>
<h2 id="四-讨论：预训练语言模型能否作为世界模型">四. 讨论：预训练语言模型能否作为世界模型</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA3.png" alt="图片"></p>
<p>相关论文：</p>
<p>Language Models Represent Space and Time, ICLR2024, MIT.</p>
<p>Reasoning with Language Model is Planning with World Model, EMNLP 2023, US San Diego.</p>
<p>Language Models Meet World Models, AAAI 2024, UC San Diego</p>
<h2 id="五-讨论：预训练语言模型能否作为知识库">五. 讨论：预训练语言模型能否作为知识库</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA4.png" alt="图片"></p>
<h2 id="六-参考文献">六. 参考文献</h2>
<p>中国科学院大学赵军老师 知识工程 课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>符号化知识表示</title>
    <url>/2024/06/01/%E7%AC%A6%E5%8F%B7%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<blockquote>
<p>四种知识建模方式：严格结构化符号表示，松散结构化/自由形式
符号表示，数值化表示，数值与符号融合表示。</p>
<p>表示方法的衡量：表达能力、推理能力、计算能力、可读性。</p>
<p>目前的语义网革命并不是在科学上有革命性的突破，而大部分是工程上的挑战，其中标准化、规模化、系统开发与集成、用户交互等都是语义网技术面临的挑战。</p>
<p>课程复习使用。</p>
</blockquote>
<h2 id="一.-经典知识表示理论">一. 经典知识表示理论</h2>
<h3 id="产生式规则">1.1 产生式规则</h3>
<ul>
<li><p><strong>产生式规则</strong>：用于表示事物之间的因果关系。</p></li>
<li><p><strong>确定性规则</strong>：P-&gt;Q。</p></li>
<li><p><strong>不确定性规则</strong>：P-&gt;Q(置信度)。当事实与前提条件不能精确匹配时，按照置信度的要求模糊匹配，并按特定算法将不确定性传递到结论。</p></li>
<li><p><strong>产生式系统</strong>：由数据库、规则库和推理机三部分组成。</p></li>
</ul>
<blockquote>
<p>数据库：用来存放问题的初始状态、已知事实、推理的中间结果和最终结论等。</p>
<p>规则库：用来存放与求解问题有关的所有规则。</p>
<p>推理机：用来控制整个系统的运行，决定问题求解的线路，包括匹配、冲突消解、路径解释等。</p>
<p>正向推理：类似于命题逻辑（查看<a href="https://lwl1751.github.io/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/">知识计算</a>）中的前向链接，从事实出发，通过规则获取结论。</p>
<p>反向推理：类似于命题逻辑中的反向链接，从目标出发，反向使用规则，求得已知事实</p>
</blockquote>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="语义网络">1.2 语义网络</h3>
<ul>
<li><p><strong>语义网络</strong>：一种将相关概念联系起来的 有向图表示的
知识系统。</p></li>
<li><p><strong>语义基元</strong>：语义网络中最基本的语义单元，用三元组形式表示，&lt;节点1，关系，节点2&gt;。</p></li>
<li><p><strong>语义网络系统</strong>：由知识库和推理机组成。</p></li>
<li><p><strong>优点</strong>：使用直观的图结构来描述知识，表达自然，而且方便于计算机的存储和检索，有较为成熟的应用。</p></li>
<li><p><strong>缺点</strong>：由于缺少形式化的语义定义，不同的语义网络之间难以互相操作，表示不完善。推理过程复杂。</p></li>
</ul>
<h3 id="框架">1.3 框架</h3>
<p>框架是一种描述所论对象属性的数据结构。</p>
<ul>
<li><strong>框架名</strong>：用来指代某一类或某一个对象。</li>
<li><strong>槽</strong>：用来表示对象的某个方面的属性，<strong>语义网络中的三元组也可看作槽结构</strong>。</li>
<li><strong>侧面</strong>：从不同侧面描述某个属性。</li>
<li><strong>值</strong>：槽/侧面的取值。</li>
<li><strong>类型</strong>：类框架，实例框架。</li>
<li><strong>层次结构</strong>：子类-subclass of-&gt;父类，示例-instance
of-&gt;类。</li>
<li><strong>推理</strong>：继承推理（下层框架继承上层框架的信息），匹配推理（安装条件进行推理）。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示3.png" alt="图片"></p>
<h3 id="脚本">1.4 脚本</h3>
<p>脚本与框架类似，由一组槽组成，用来表示特定领域内一些事件的发生序列。但脚本表示的知识有明确的时间或因果顺序，因此它描述的是一个<strong>过程</strong>而非静态知识。</p>
<p>脚本的结构化表示包括：进入条件，角色，道具，场景，结果。</p>
<h3 id="一阶谓词逻辑">1.5 一阶谓词逻辑</h3>
<p>由于语义网络、框架、脚本缺少形式化定义，不同网络之间难以相互操作，且推理过程复杂。因此引入了一阶谓词逻辑。参看<a href="https://lwl1751.github.io/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/">知识计算</a>。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示4.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="描述逻辑">1.6 描述逻辑</h3>
<p>通过<strong>概念类别</strong>来描述物理世界，没有一阶谓词逻辑中变量和谓词的概念，但具有形式化定义。描述逻辑有概念描述、属性、个体三个基础部分组成。</p>
<ul>
<li><strong>概念描述</strong>：表示一类事物。</li>
<li><strong>概念构造器</strong>：用两个概念描述构造出一个新的概念，有交集构造器、并集构造器、否定构造器。</li>
<li><strong>属性</strong>：作用于概念，必须搭配全称量词 <span class="math inline">\(\forall\)</span>，或存在量词 <span class="math inline">\(\exists\)</span>使用。</li>
<li><strong>个体</strong>：概念示例。</li>
<li><strong>知识库</strong>：包括术语（TBox，描述概念定义、公理），断言（ABox，描述个体知识）两部分。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示5.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示6.png" alt="图片"></p>
<h2 id="二.语义网的知识描述体系">二.语义网的知识描述体系</h2>
<ul>
<li><strong>本质</strong>：以Web数据的内容（即语义）为核心，用机器能够理解和处理的方式链接起来的海量分布式数据库。</li>
<li><strong>特征</strong>：Web上的事物拥有唯一的URI（通用资源标识符），事物之间存在显示链接，事物链接又具有不同的语义类型。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示7.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示8.png" alt="图片"></p>
<ul>
<li><strong>XML</strong>：由起始标签、元素内容和结尾标签构成，并且元素具有嵌套结构，同时没有约束嵌套的深度。</li>
<li><strong>RDF</strong>：Resource Description
Framework。由于XML只定义了文档结构和数据类型，没有定义数据的语义，机器仍然无法理解文档的内容。为了让应用程序理解数据的语义，定义了RDF。RDF利用Web标识符（URI）来标识事物，并通过指定的属性和相应的值描述资源的性质或资源之间的关系。</li>
</ul>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示9.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>注意：RDF并不是一种语言，只是一种书写规范。</p>
<ul>
<li><p><strong>RDFs</strong> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识表示10.png" alt="图片"></p></li>
<li><p><strong>OWL</strong>: Web Ontology
Language，相比于RDFS，添加了更多用于描述类和属性的建模原语，支持更加丰富的语义表达并支持推理。</p></li>
<li><p><strong>RIF</strong>: Rule Interchange
Format，一种不同的规则语言和推理引擎之间的交换格式。</p></li>
</ul>
<h2 id="三.-参考资料">三. 参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程 课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务处理</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>分布式系统中的交互处理</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E4%BA%A4%E4%BA%92%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>分布式系统中的数据处理</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>分布式系统中的故障处理</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>分布式系统的特征</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%89%B9%E5%BE%81/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>分布式系统设计的关注点</title>
    <url>/2024/06/02/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%85%B3%E6%B3%A8%E7%82%B9/</url>
    <content><![CDATA[
]]></content>
  </entry>
</search>
