<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>how to build up a blog</title>
    <url>/2024/05/12/how-to-build-up-a-blog/</url>
    <content><![CDATA[<span id="more"></span>
<p>登录root用户</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p>切换路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd /Users/lwl/Blog</span><br></pre></td></tr></table></figure>
<p>博客文章发布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;title&quot;</span> <span class="comment"># 新建文章</span></span><br><span class="line"><span class="comment"># 编辑对应的markdown文件</span></span><br><span class="line">hexo g <span class="comment"># 渲染md文件为博客页面</span></span><br><span class="line">hexo s <span class="comment"># 执行后打开http://localhost:4000/预览</span></span><br><span class="line">hexo d <span class="comment"># 预览并编辑无误后再部署，也可以直接部署</span></span><br></pre></td></tr></table></figure>
<p>有时部署会失败，此时尝试下面命令清除缓存后再执行部署命令。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo clean </span><br></pre></td></tr></table></figure>
<p>更换端口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo s -p <span class="number">8888</span> <span class="comment"># 示例</span></span><br></pre></td></tr></table></figure>
<p>释放端口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lsof -i :端口号 <span class="comment"># 查找占用指定端口的进程ID（PID）</span></span><br><span class="line">kill -<span class="number">9</span> 进程ID</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>基于策略梯度的强化学习</title>
    <url>/2024/05/15/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>强化学习可分为两大类：</p>
<ul>
<li>
<p><strong>value-based method</strong>(DP,MC,TD)</p>
<p>通过价值函数求解最优策略，求解出来的策略是确定性的，虽然可以通过$\epsilon$-贪心策略来获取一定的随机性。要求动作空间离散。</p>
</li>
<li>
<p><strong>policy-based method</strong></p>
<p>适用场景：随机策略；动作空间连续。</p>
<p>优点：具有更好的收敛性质。</p>
<p>缺点：通常会收敛到局部最优而非全局最优；评估一个策略通常不够高效并且具有较大的方差。</p>
</li>
</ul>
<span id="more"></span>
<h2 id="1-基本原理">1.基本原理</h2>
<p>由于策略实际上是一个概率分布，可以将策略参数化 $\pi(a|s,\theta)$ ，其中$\theta$ 是策略的参数。通过这种方式，可以将可见的已知状态泛化到未知的状态上。</p>
<h3 id="1-1-策略目标函数">1.1 策略目标函数</h3>
<p>在片段式的环境中，使用每个经历片段(episode)的平均总回报。在连续性的环境中，使用每一步的平均奖励。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%AD%96%E7%95%A5%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B01.png" alt="图1"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%AD%96%E7%95%A5%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B02.png" alt="图2"></p>
<p>希望能够找到最大化$J(\theta)$的$\theta$，属于最优化问题，求解方法如下：</p>
<ul>
<li>不使用梯度的方法(Hill climbing, Simplex, 模拟退火, 遗传算法)</li>
<li>使用梯度的方法更高效(梯度下降, 共轭梯度, 拟牛顿法)</li>
</ul>
<h3 id="1-2-策略函数">1.2 策略函数</h3>
<ul>
<li>softmax策略，离散型动作空间</li>
<li>高斯函数策略，连续型动作空间</li>
<li>线性函数策略，连续型动作空间，表示给定状态下确定性的动作，$a=\pi(s,\theta)$</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/score%E5%87%BD%E6%95%B0.png" alt="图3"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/softmax%E7%AD%96%E7%95%A52.png" alt="图4"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/softmax%E7%AD%96%E7%95%A5.png" alt="图5"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E9%AB%98%E6%96%AF%E7%AD%96%E7%95%A5.png" alt="图6"></p>
<h3 id="1-3-单步马尔可夫决策过程">1.3 单步马尔可夫决策过程</h3>
<p>从一个分布d(s)中采样得到一个状态s，从s开始，按照策略𝜋采取一个行为a，得到即时奖励$r=R_{s,a}$。由于是单步过程，目标函数为<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%95%E6%AD%A5%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E5%85%AC%E5%BC%8F.png" alt="图7"></p>
<h2 id="2-策略梯度定理">2.策略梯度定理</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%AD%96%E7%95%A5%E6%8E%A8%E7%90%861.png" alt="图8"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%AD%96%E7%95%A5%E6%8E%A8%E7%90%862.png" alt="图9"></p>
<p>由于状态转移函数的存在，虽然训练用的轨迹都是由同一个策略生成的，但其两两差异仍十分显著，并且显然轨迹越长差异越大，决策中每一个微小的差异累积起来都会导致最后结果的极大差异。也就是数据有着较大的方差，这会导致使用均值计算期望的效果变差，并使算法难以收敛。</p>
<p><strong>改进方法：使用时序因果关系；加入基线。</strong></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%97%B6%E9%97%B4%E8%BF%9E%E7%BB%AD%E6%80%A71.png" alt="图10"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%97%B6%E9%97%B4%E8%BF%9E%E7%BB%AD%E6%80%A72.png" alt="图11"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8A%A0%E5%85%A5%E5%9F%BA%E7%BA%BF1.png" alt="图12"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8A%A0%E5%85%A5%E5%9F%BA%E7%BA%BF2.png" alt="图13"></p>
<h2 id="3-蒙特卡洛策略梯度-REINFORCE">3.蒙特卡洛策略梯度(REINFORCE)</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/REINFORCE-1.png" alt="图14"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/REINFORCE2.png" alt="图15"></p>
<h2 id="4-Actor-Critic-算法">4. Actor-Critic 算法</h2>
<p><strong>actor --&gt; policy network</strong>，决定采取哪个动作</p>
<ul>
<li>$\pi(a|s;\theta)$</li>
<li>input: state s</li>
<li>output: probability distribution over the actions</li>
<li>训练目标： 增加状态值函数 state-value</li>
</ul>
<p>c<strong>ritic --&gt; value network</strong>，只负责评估动作的好坏</p>
<ul>
<li>$q(s,a;w)$</li>
<li>input: state s and action a</li>
<li>output: approximate action-value(scalar)</li>
<li>训练目标： 使价值评估的更精准，接近于实际环境的return</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/A3C-1.png" alt="图16"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/A3C-2.png" alt="图17"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p><a href="https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d">https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
<p><a href="https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg">https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg</a></p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d">https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态知识图谱应用</title>
    <url>/2024/05/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>相比于单模态知识图谱，多模态知识图谱能够综合多种类型的数据，从而可以让智能体更深入的感知和理解真实的数据场景，因而多模态知识图谱在各个领域都有广泛的应用。如图像检索、模型推理与生成、模型预训练等。以电子商务为例，通过多模态产品图谱，可以对产品进行更细致的表示，再通过预训练，可以增强大型模型对电子商务领域的多模态知识理解，从而推动电子商务平台的发展。</p>
<span id="more"></span>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%BA%94%E7%94%A81.png" alt="图1"></p>
<p>AliMe MKG是阿里提出的一种面向直播的知识图谱，与传统的知识图谱不同，它的目标是向顾客种草某一产品，而非解决顾客的问题。因此，它需要构建逻辑思维链，引导用户需求。例如，在该左图1的知识图谱示例中，“熬夜&quot;导致&quot;皮肤暗沉&quot;问题，这就需要&quot;皮肤白皙”，而含有&quot;甘草酸二钾&quot;成分的&quot;面膜&quot;产品适合相应的用户。</p>
<p>在电商直播领域，这种知识图谱有两种应用：智能辅播和虚拟主播。智能辅播是在真人直播间构造了一个智能助理机器人，来协助主播去做商品介绍。比如说用户问的是口红，直播间内有多个口红，智能辅播就会将相关的信息展示出来给用户进行浏览，当用户点击确认，选择一个感兴趣的口红之后，辅播就会从知识图谱中抽取相应信息，以商品卡片信息的方式让用户和图片进行交互。除此之外，智能辅播还可以回答用户丰富的产品相关问题。比如用户问尺码的时候，辅播可以去推出文本介绍和对应的尺码图，用文本及图片来回答用户的咨询。</p>
<p>虚拟主播则是一个智能的直播间虚拟人，通过自动生成图文剧本介绍商品，生成具有吸引力和认知的知识型短视频，从而可以影响客户的购买决策。因此，多模态的知识图谱可以促进电商的发展。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%BA%94%E7%94%A82.png" alt="图2"></p>
<p>另一个是医疗诊断的案例，这是一个基于多模态知识图谱的医疗健康问答系统示例。它首先利用多种方法获取用户提交数据的关键信息，并确定用户查询的主题意图，建立用户的知识需求模型。在知识匹配阶段，我们计算用户需求与医疗健康知识的相关度，并消除可能的歧义，最终向用户提供匹配度高的医疗健康知识。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%BA%94%E7%94%A83.png" alt="图3"></p>
<p><strong>参考资料</strong>:</p>
<p>Chen Z, Zhang Y, Fang Y, et al. Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey[J]. arXiv preprint arXiv:2402.05391, 2024.</p>
<p>Xu G, Chen H, Li F L, et al. Alime mkg: A multi-modal knowledge graph for live-streaming e-commerce[C]//Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. 2021: 4808-4812.</p>
<p><a href="https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A">https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A</a></p>
<p><a href="https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g">https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g</a></p>
<p>韩普,叶东宇,陈文祺,等.面向多模态医疗健康数据的知识组织模式研究[J].现代情报,2023,43(10):27-34+151.</p>
]]></content>
      <tags>
        <tag>多模态</tag>
        <tag>知识图谱</tag>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型知识分析、萃取与增强</title>
    <url>/2024/05/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E5%88%86%E6%9E%90%E3%80%81%E8%90%83%E5%8F%96%E4%B8%8E%E5%A2%9E%E5%BC%BA/</url>
    <content><![CDATA[<pre><code>大模型中蕴含着大量的知识，但是知识的类型、数量和质量并不可控。

知识分析实验表明，大模型自发学到了一些世界知识、常识知识，这些知识隐式地存储于模型参数中。

课程考试复习使用。</code></pre>
<span id="more"></span>
<h2 id="大模型的知识分析">1.大模型的知识分析</h2>
<h3 id="知识探测">1.1 知识探测</h3>
<ul>
<li><p><strong>知识探测</strong>：探测预训练语言模型掌握的知识。</p></li>
<li><p><strong>实现方式</strong>：将三元组或问答对形式的世界知识转化为自然语言填空的形式，从而判断语言模型掌握知识的准确性。</p></li>
<li><p><strong>预训练模型知识探测的良好性能主要来源于</strong>：</p>
<ul>
<li>提示语偏差，预测结果会受到提示词的影响，如 was born in [Mask] ,
模型会猜测下一个词应该为地名。</li>
<li>类别指导，类似于few shot learning，模型已经见过类似的问题。</li>
<li>答案泄漏，基于上下文的推理。</li>
</ul></li>
</ul>
<h3 id="知识定位">1.2 知识定位</h3>
<ul>
<li><strong>知识定位</strong>：分析预训练语言模型中的知识存储机制，可分为层粒度与神经元粒度。</li>
<li>大量事实知识存储在<strong>FNN模块</strong>中。</li>
</ul>
<h3 id="知识学习机理分析">1.3 知识学习机理分析</h3>
<p>分析影响预训练语言模型学习效果的因素。</p>
<ul>
<li><strong>长尾知识</strong>：信息出现次数非常少，甚至只出现了一次。LLM对长尾知识的掌握并不充分，回答问题的准确度就会降低，可以通过扩大模型规模(scaling
low)、检索增强来解决该问题。</li>
<li><strong>共现频率</strong>：LLM更加倾向于预测共现频率更高的答案。如，对于问题“加拿大的首都是？”，在预训练语料中，（加拿大，多伦多）共同出现的频率要大于（加拿大，渥太华）出现的频率，于是模型倾向于输出共现频率更高的“多伦多”，而不是正确答案“渥太华”。</li>
<li><strong>逆转诅咒</strong>，模型很难逆转思考，如 A is B 不能推出 B is
A.</li>
</ul>
<h2 id="大模型的知识萃取">2.大模型的知识萃取</h2>
<p>知识萃取是指利用特定方式诱导大模型，从中萃取出有用的显式符号化知识。</p>
<h2 id="大模型的知识增强">3.大模型的知识增强</h2>
<ul>
<li>幻觉可以分为事实性现象（生成的内容不忠于既定的事实知识）和忠实性幻觉（生成的内容前后冲突）。</li>
<li><strong>幻觉消除</strong>：清洗训练数据，解码方式改进，指令数据优化，外部知识增强。</li>
<li><strong>知识增强</strong>：RAG，Fine-tuning。</li>
<li>关于幻觉的知识，可以查看文献综述：《Survey on Factuality in Large
Language Models: Knowledge, Retrieval and Domain-Specificity》</li>
</ul>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识增强_1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="rag">3.1 RAG</h3>
<ul>
<li><strong>检索知识源</strong>
<ul>
<li>文档：检索粒度粗，知识覆盖度高，存在冗余信息。</li>
<li>知识图谱：提供丰富的结构化信息，受限于图谱的覆盖度。</li>
</ul></li>
<li><strong>检索方式</strong>
<ul>
<li>稀疏检索：简单词汇匹配，缺少上下文理解能力（BM25）</li>
<li>稠密检索：将问题和文档编码为稠密向量，计算点积作为相似度（DPR）</li>
<li>生成检索：直接使用大模型为问题生成相关文档，而不需要检索库</li>
</ul></li>
<li><strong>检索时间</strong>
<ul>
<li>推理前检索一次：效率高，但相关度低</li>
<li>推理过程中自适应地检索：平衡知识和效率，但是难以判断模型何时需要知识</li>
<li>推理过程中每隔N个token检索一次：效率低，信息量大</li>
</ul></li>
<li><strong>利用检索得到的知识进行推理</strong>
<ul>
<li>输入增强：使用简单，受限于上下文长度</li>
<li>中间增强：需要重新训练模型，支持处理更多文档</li>
<li>输出增强：对输出进行后修改，需要两次推理模型：第一次，模型直接输出答案。第二次，根据问题和答案，进行检索，对输出答案进行修改。</li>
</ul></li>
<li><strong>知识拉锯战</strong>：由于错误信息，观点不同，以及知识进化的本质，知识冲突问题广泛存在于检索增强语言模型中。</li>
<li><strong>知识冲突形式</strong>：
<ul>
<li>模型内部参数化知识和外部非参数化知识之间存在冲突。其中，使用外部知识回答的模型作为专家模型，依靠内部知识回答的模型作为业余模型。</li>
<li>非参数化知识中真实、虚假以及无关证据之间存在冲突。其中，通过指令微调，使用真实证据回答的模型作为专家模型，使用虚假证据回答的模型作为业余模型。</li>
</ul></li>
<li>Dunning-Kruger现象：人对于某些欠缺的能力反而会过度自信，对模型也同样适用。</li>
</ul>
<h2 id="大模型的工具增强">4.大模型的工具增强</h2>
<ul>
<li><strong>工具增强</strong>：让模型学会使用外部工具，以补充模型相关知识。</li>
<li>相关论文：
<ul>
<li>Timo Schick, Toolformer: Language Models Can Teach Themselves to Use
Tools, NeurIPS 2023。</li>
<li>Yujia Qin, ToolLLM: Facilitating Large Language Models to Master
16000+ Real-world APIs, ICLR2024。</li>
</ul></li>
</ul>
<h2 id="参考资料">5.参考资料</h2>
<p>中国科学院大学赵军老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习基础知识</title>
    <url>/2024/05/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p><strong>知识点：马尔科夫决策过程，动态规划。</strong></p>
<hr>
<span id="more"></span>
<h2 id="马尔科夫决策过程">马尔科夫决策过程</h2>
<p><strong><span style="color:purple;">马尔科夫过程</span></strong>: 一个具备马尔科夫性质的离散随机过程。</p>
<p><strong><span style="color:purple;">马尔科夫性</span></strong>: 下一时刻的状态只与当前状态有关。即$P[S_{t+1}|S_{1},…,S_{t}]=P[S_{t+1}|S_{t}]$</p>
<p><strong><span style="color:purple;">马尔科夫奖励函数</span></strong>: 把马尔科夫过程从 &lt;S, P&gt; 拓展到 &lt;S, P, R, γ&gt;。其中P 为状态转移矩阵，R 和 γ 分别表示奖励函数和奖励折扣因子。折扣因子越大，代表了智能体对长期性能指标考虑的程度越高（远视）；折扣因子越小，代表了智能体对长期性能指标考虑的程度越低（近视）。</p>
<p><strong><span style="color:purple;">回报</span></strong>: 回报是一个轨迹的累积奖励，$G_{t} = R_{t+1} + \gamma R_{t+2} = \sum_{k=0}^{\infty } \gamma ^{k}R_{t+k+1}$</p>
<p><strong><span style="color:purple;">价值函数</span></strong>：状态s的期望回报，$V(s) = E[G_{t}|S_{t}=s]$。</p>
<p><strong><span style="color:purple;">马尔科夫决策过程</span></strong>: 马尔可夫奖励过程的立即奖励只取决于状态(奖励值在节点上)，而马尔可夫决策过程的立即奖励与状态和动作都有关。即把马尔科夫过程从 &lt;S, P, R, γ&gt; 拓展到 &lt;S, A, P, R, γ&gt;。A是有限动作的集合。</p>
<p><strong><span style="color:purple;">动作价值函数</span></strong>: 依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。$Q(s,a) = E[G_{t}|S_{t}=s, A_{t}=a]$。易知$V(s)=E_{a}[Q(s,a)]$。</p>
<p><strong><span style="color:purple;">策略</span></strong>: 状态到行为的映射。</p>
<blockquote>
<p>对于任何马尔科夫决策过程：</p>
<ul>
<li>总是存在一个最优策略$\pi^*$，比任何其他策略更好或至少相等。</li>
<li>所有的最优策略有相同且最优的价值。</li>
<li>所有的最优策略具有相同且最优的动作价值。</li>
</ul>
</blockquote>
<p><strong><span style="color:purple;">贝尔曼方程</span></strong>：用于计算给定策略 π 时价值函数在策略指引下所采轨迹上的期望。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%88%AA%E5%B1%8F2024-05-16%2013.18.14.png" alt="图1"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%88%AA%E5%B1%8F2024-05-16%2013.18.34.png" alt="图2"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%88%AA%E5%B1%8F2024-05-16%2013.27.29.png" alt="图3"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%88%AA%E5%B1%8F2024-05-16%2013.27.43.png" alt="图4"></p>
<p><strong><span style="color:purple;">最优价值函数</span></strong>:</p>
<p>即使是在相同的状态和动作集合上，不同的策略也将会带来不同的价值函数。定义最优价值函数为</p>
<p>$$v_*(s) = \max_{π} v_π(s), ∀s ∈ S$$</p>
<p>最优动作价值函数：</p>
<p>$$q_*(s,a) = \max_{\pi} q_π(s,a), ∀s ∈ S, a ∈ A$$</p>
<p>则</p>
<p>$$v*(s) = \max_{a\sim \mathbf{A}} q_*(s, a)$$</p>
<p>$$q_<em>(s, a) = E[R_t + γv_</em>(S_{t+1}) | S_t = s, A_t = a]$$</p>
<p><strong><span style="color:purple;">逆矩阵法求解贝尔曼方程</span></strong>：</p>
<p>$$\mathbf{v}  = \mathbf{r} + γP\mathbf{v} $$</p>
<p>其中 <strong>v</strong> 和 <strong>r</strong> 矢量，P是状态转移概率矩阵。求解如下：</p>
<p>$$\mathbf{v} = (I − γP )^{-1}\mathbf{r}$$</p>
<p>复杂度为$O(n^{3})$，考虑其他方法进行求解，如<span style="color:red;">动态规划、蒙特卡洛估计、时序差分法</span>等。</p>
<h2 id="动态规划">动态规划</h2>
<p>用动态规划算法在 能够获取MDP完整的环境信息（包括状态动作空间、转移矩阵、奖励等）的基础上 求解最优策略。</p>
<p><strong><span style="color:purple;">预测</span></strong>：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;和策略𝜋，输出基于当前策略𝜋的价值函数v。</p>
<p><strong><span style="color:purple;">控制</span></strong>：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;，输出最优价值函数𝑣∗以及最优策略𝜋∗</p>
<p><strong><span style="color:purple;">迭代策略评估</span></strong>： 预测问题，评估一个给定的策略$\pi$。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E8%BF%AD%E4%BB%A3%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0.png" alt="图5"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0.png" alt="图6"></p>
<p><strong><span style="color:purple;">策略迭代</span></strong>：</p>
<ul>
<li>策略评估，在当前策略𝜋上迭代地计算𝑣值</li>
<li>策略更新，根据𝑣值贪婪地更新策略</li>
<li>如此反复多次，最终得到最优策略𝜋∗和最优状态价值函数𝑣∗<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3.png" alt="图7"></li>
</ul>
<p><strong><span style="color:purple;">价值迭代</span></strong>：<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3.png" alt="图8"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学 林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索</title>
    <url>/2024/05/15/%E6%90%9C%E7%B4%A2/</url>
    <content><![CDATA[<pre><code>知识点：

经典搜索算法：基于路径的搜索的问题。如盲目搜索，启发式搜索。

局部搜索算法：最优化问题，没有初始状态，也没有终止状；并不需要到达这些解的路径。如爬山法，元启发算法。

元启发式算法是启发式算法的改进，它是随机算法与局部搜索算法相结合的产物。如禁忌搜索算法(Tabu Search)，模拟退火算法(Simulated annealing)，遗传算法(Geneticalgorithm)。

对抗搜索算法：也被称为博弈搜索，在一个竞争的环境中，智能体(agents)之间通过竞争实现相反的利益，一方最大化利益，另外一方最小化这个利益。如mini-max算法，Alpha-Beta算法、蒙特卡洛树。
</code></pre><h2 id="1-搜索算法基础"><a href="#1-搜索算法基础" class="headerlink" title="1.搜索算法基础"></a>1.搜索算法基础</h2><p>树搜索：</p>
<ul>
<li>结点：n.state, n.parent, n.action(父节点生成该节点时所采取的行动), n.path-cost(从初始状态到达该结点的路径消耗g(n))</li>
<li>搜索策略：节点扩展到顺序</li>
<li>策略评价标注：完备性（是否能找到存在的解），时间复杂度，空间复杂度，最优性</li>
<li>复杂度表示：分支因子b，搜索树的中节点的最大分支树；深度d，目标结点所在的最浅深度；m，状态空间中，任何路径的最大长度</li>
</ul>
<p>图搜索：</p>
<ul>
<li>边缘(fringe)：待扩展的叶子结点，将状态空间分成已探索区域和未被探索区域。</li>
</ul>
<h2 id="2-盲目搜索"><a href="#2-盲目搜索" class="headerlink" title="2.盲目搜索"></a>2.盲目搜索</h2><p>只能使用访问过的结点的信息。如<strong>宽度优先搜索</strong>，<strong>深度优先搜索</strong>，<strong>一致代价搜索</strong>（扩展路径消耗g(n)最小的结点），<strong>深度受限搜索</strong>（对深度优先搜索设置最大深度的界限），<strong>迭代加深的深度优先搜索</strong>（不断增大深度限制，并且每次重新开始深度受限搜索）。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/盲目搜索.png" alt="图片"></p>
<h2 id="3-启发式搜索"><a href="#3-启发式搜索" class="headerlink" title="3.启发式搜索"></a>3.启发式搜索</h2><p>优先扩展最优的结点。评价函数：f = g + h。其中，g为一致代价，h为启发式函数，指的是结点n到目标结点的最小代价路径的代价估计值。</p>
<ul>
<li>贪婪算法：扩展离目标最近的结点，以期望很快找到解。f(n)=h(n)</li>
<li>A* 算法：避免扩展代价已经很高的路径。f(n) = g(n) + h(n)</li>
</ul>
<blockquote>
<p><strong>A* 算法:</strong></p>
<p><strong>可采纳启发式</strong>：永远不会高估代价，即$h(n)\le h^<em>(n)$，且要求$h(n)\ge 0$。其中，$h^</em>(n)$为实际的代价。</p>
<p>定理：<strong>如果h(n)是可采纳的，A*的树搜索版本是最优的。</strong></p>
<p><strong>一致的启发式</strong>：对于每个结点n和通过任一行动a生成的后继结点n’，从结点n到达目标的估计代价不大于从n到n’的单步代价与从n到目标的估计代价之和，即$h(n)\le c(n,a,n’) + h(n’)$。如果h(n)是一致的，那么沿着任何路径的f(n)是非递减的。</p>
<p>定理：<strong>如果h(n)是一致的，那么A*的图搜索版本是最优的。</strong></p>
<p>松弛问题：对原问题移除一些限制，一个松弛问题的最优解的代价是原问题的一个可采纳、一致的启发式。</p>
<p>松弛问题的最优解的代价不大于原问题最优解的代价。</p>
</blockquote>
<h2 id="4-局部搜索"><a href="#4-局部搜索" class="headerlink" title="4.局部搜索"></a>4.局部搜索</h2><p>局部搜索：找到满足条件的状态，不关心路径，从单个当前节点(而不是多条路径)出发，通常只移动到它的邻近状态。</p>
<h3 id="4-1-爬山法"><a href="#4-1-爬山法" class="headerlink" title="4.1 爬山法"></a>4.1 爬山法</h3><p>属于贪婪法，不断向值增加的方向移动，容易到达局部极大值。为了克服局部极大值，可以采用随机重启爬山法，完备的概率接近1。</p>
<h3 id="4-2-禁忌搜索算法"><a href="#4-2-禁忌搜索算法" class="headerlink" title="4.2 禁忌搜索算法"></a>4.2 禁忌搜索算法</h3><p>从一个初始可行解出发，选择一系列的特定搜索方向（移动）作为试探，选择实现让特定的<strong>目标函数值变化最多</strong>的移动。</p>
<p>为了避免陷入局部最优解，TS搜索中采用了一种灵活的“记忆”技术，对已经进行的优化过程进行记录和选择，指导下一步的搜索方向，这就是Tabu表的建立。</p>
<p>标记已经解得的局部最优解或求解过程，并在进一步的迭代中避开这些局部最优解或求解过程。局部搜索的缺点在于，太过于对某一局部区域以及其邻域的搜索，导致一叶障目。为了找到全局最优解，<strong>禁忌搜索就是对于找到的一部分局部最优解，有意识地避开它，从而或得更多的搜索区域。</strong></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索2.png" alt="图片2"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索3.png" alt="图片3"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索4.png" alt="图片4"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/禁忌搜索5.png" alt="图片2"></p>
<h3 id="4-3-模拟退火算法"><a href="#4-3-模拟退火算法" class="headerlink" title="4.3 模拟退火算法"></a>4.3 模拟退火算法</h3><p>算法概述：</p>
<ul>
<li>若目标函数f在第i+1步移动后比第i步更优，即$f(Y(i+1))\le f(Y(i))$，则总是接受该移动。</li>
<li>若$f(Y(i+1))&gt;f(Y(i))$，（即移动后的解比当前解要差），则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）。</li>
<li>Metroplis准则：温度越高，算法接受新解的概率就越高。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/模拟退火1.png" alt="图片1"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/模拟退火2.png" alt="图片2"></p>
<h3 id="4-4-遗传算法"><a href="#4-4-遗传算法" class="headerlink" title="4.4 遗传算法"></a>4.4 遗传算法</h3><p>基本思想：从初始种群出发，采用优胜劣汰、适者生存的自然法则选择个体，并通过杂交、变异来产生新一代种群，如此逐代进化，直到满足目标为止。</p>
<ul>
<li>种群：组候选解的集合，遗传算法正是通过种群的迭代进化，实现了最优解或者近似最优解。</li>
<li>个体：一个个体对应一个解，也就是构成种群的基本单元。</li>
<li>适应度函数:用来对种群中各个个体的环境适应性进行度量的函数，函数值是遗传算法实现优胜劣汰的主要依据。</li>
<li>遗传操作：作用于种群而产生新的种群的操作。如选择、交叉、变异。</li>
</ul>
<p>遗传编码：</p>
<ul>
<li>二进制编码</li>
<li>格雷编码，要求两个连续整数的编码之间只能有一个码位不同，其余码位都是完全相同的。</li>
<li>符号编码，个体染色体编码串中的基因值取自一个无数值含义、而只有代码含义的符号集</li>
</ul>
<p>适应度函数：</p>
<ul>
<li>原始适应度函数，直接将待求解问题的目标函数f(x)定义为遗传算法的适应度函数。它能够直接反映出待求解问题的最初求解目标但是有可能出现适应度值为负的情况。</li>
<li>标准适应度函数。在遗传算法中，一般要求适应度函数非负，并其适应度值越大越好。这就往往需要对原始适应函数进行某种变换，将其转换为标准的度量方式，以满足进化操作的要求，这样所得到的适应度函数被称为标准适应度函数$f_{Normal}(x)$</li>
</ul>
<p>选择：各个个体被选中的概率与其适应度大小成正比</p>
<ul>
<li>比例选择</li>
<li>锦标赛选择，随机选择一组个体进行竞赛，然后从中选择最优秀的个体来进行繁殖。</li>
<li>轮盘赌选择，根据每个个体的选择概率$P(x_i)$将一个圆盘分成N 个扇区，其中第i 个扇区的中心角为:$2 \pi \frac{f\left(x_{i}\right)}{\sum_{j=1}^{N} f_{i}\left(x_{j}\right)}=2 \pi p\left(x_{i}\right)$，并再设立一个固定指针。当进行选择时，可以假想转动圆盘，若圆盘静止 时指针指向第i个扇区，则选择个体i。</li>
</ul>
<p>交叉：</p>
<ul>
<li>单点交叉，先在两个父代个体的编码串中随机设定一个交叉点，然后对这两个父代个体交叉点前面或后面部分的基因进行交换，并生成子代中的两个新的个体。</li>
<li>两点交叉，先在两个父代个体的编码串中随机设定两个交叉点，然后再按这两个交叉点进行部分基因交换，生成子代中的两个新的个体。</li>
<li>多点交叉，从两个父代个体中选择多个交叉点，然后交换这些交叉点之间的基因片段，从而产生新的个体。</li>
<li>均匀交叉，父串中的每一位都是以相同的概率随机进行交叉的。</li>
<li><p>实值交叉，在实数编码情况下所采用的交叉操作，可分为部分离散交叉、整体交叉。</p>
<p>  部分离散交叉: 先在两个父代个体的编码向量中随机选择一部分分量， 然后对这部分分量进行交换，生成子代中的两个新的个体。</p>
<p>  整体交叉: 对两个父代个体的编码向量中的所有分量，都以1/2的概率进行交换，从而生成子代中的两个新的个体。</p>
</li>
<li>洗牌交叉，打乱之后再选择交叉点，再进行复原</li>
</ul>
<p>变异：</p>
<ul>
<li>二进制变异，随机地产生一个变异位，0-&gt;1，1-&gt;0</li>
<li>实值变异，用另外一个在规定范围内的随机实数去替换原变异位置上的基因值，产生一个新的个体。</li>
</ul>
<h2 id="5-对抗搜索"><a href="#5-对抗搜索" class="headerlink" title="5.对抗搜索"></a>5.对抗搜索</h2><h3 id="5-1-Alpha-Beta算法"><a href="#5-1-Alpha-Beta算法" class="headerlink" title="5.1 Alpha-Beta算法"></a>5.1 Alpha-Beta算法</h3><p>对mini-max算法的改进。剪枝本身不影响算法输出结果，但节点先后次序会影响剪枝效率。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Alpha-Beta%20剪枝1.png" alt="图片1"><br><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Alpha-Beta%20剪枝2.png" alt="图片2"></p>
<h3 id="5-2-蒙特卡洛树"><a href="#5-2-蒙特卡洛树" class="headerlink" title="5.2 蒙特卡洛树"></a>5.2 蒙特卡洛树</h3><p>参看蒙特卡洛树搜索部分。</p>
<h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6.参考资料"></a>6.参考资料</h2><p>中国科学院大学李国荣老师 高级人工智能课程课件</p>
<p>高级算法设计与分析 启发式算法 林海老师</p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>无模型强化学习</title>
    <url>/2024/05/15/%E6%97%A0%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<blockquote>
<p><strong>知识点</strong></p>
<p>1.无模型价值学习评估</p>
<ul>
<li>蒙特卡洛方法</li>
<li>时序差分学习</li>
<li>TD(𝝀)</li>
</ul>
<p>2.无模型策略优化控制</p>
<ul>
<li>蒙特卡洛策略迭代</li>
<li>时序差分策略迭代（SARSA）</li>
<li>Q值迭代 (Q-learning) <span id="more"></span></li>
</ul>
</blockquote>
<h2 id="无模型价值学习评估">1. 无模型价值学习评估</h2>
<h3 id="蒙特卡洛方法">1.1 蒙特卡洛方法</h3>
<p>蒙特卡洛方法是一种<strong>基于样本</strong>的方法，不需要知道环境的所有信息。只需基于过去的经验就可以学习。具体来说，给定一个策略
π，通过对 π 产生的回报取平均值来评估状态价值函数。这样就有两种估算方式:
<strong>首次蒙特卡罗</strong>(First-Visit Monte
Carlo)和<strong>每次蒙特卡罗</strong>(Every-Visit Monte
Carlo)。首次蒙特卡罗只考虑每一个回合中第一次到状态 s
的访问，而每次蒙特卡罗就是考虑每次到状态 s 的访问。</p>
<p>注意的是，和动态规划不同的是，蒙特卡罗不使用<strong>自举(Bootstrapping)</strong>，也就是说，它不用其他状态的估算来估算当前的状态值。
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/首次蒙特卡洛访问.png" alt="图片"></p>
<p>离线学习：智能体从预先收集好的数据中进行学习。</p>
<p>在线学习：智能体通过与环境实时交互来获取知识和经验。</p>
<h3 id="时序差分学习">1.2 时序差分学习</h3>
<p>时序差分学习方法同蒙特卡洛方法一样是不基于模型的，不需要马尔可夫决策过程的知识。但是时序差分学习方法可以直接从经历的不完整经历片段中学习，它通过<strong>自举(bootstrap)</strong>猜测经历片段的结果并不断更新猜测。即时序差分学习方法可以在每一次经历的过程中进行学习，而蒙特卡洛方法只能等到每次经历完全结束时才能进行学习。</p>
<p><span class="math display">\[𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝐺_t −
𝑉(𝑆_{𝑡}))\]</span></p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TD.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>对TD(0)，即one-step TD:</p>
<p><span class="math display">\[𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝑅_{𝑡+1} +
𝛾𝑉(𝑆_{t+1}) − 𝑉(𝑆_{𝑡}))\]</span></p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Sarsa价值评估.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>这个算法又被叫做<strong>SARSA</strong>，因为用到了 <span class="math inline">\((S_t, A_{𝑡}, R_{𝑡+1}, S_{𝑡+1},
A_{𝑡+1})\)</span>。</p>
<ul>
<li>蒙特卡洛方法没有偏倚，是对当前状态实际价值的无偏估计，但有着较高的变异性，且对初始值不敏感。</li>
<li>时序差分方法方差更低,
但有一定程度的偏差，对初始值较敏感，通常比蒙特卡洛方法更高效。</li>
</ul>
<h3 id="td𝝀">1.3 TD(𝝀)</h3>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/步数对TD的影响.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn_3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/反向TD_补充1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/反向TD_补充2.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/TDn-4.png" alt="图片"></p>
<h2 id="无模型策略优化控制">2.无模型策略优化控制</h2>
<h3 id="蒙特卡洛策略迭代">2.1 蒙特卡洛策略迭代</h3>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/MTCS策略迭代.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="时序差分策略迭代sarsa">2.2 时序差分策略迭代（SARSA）</h3>
<p><span class="math display">\[G_{t:t+n} = R_{t+1} + γR_{t+2} + \dot +
γ^{n−1}R_{t+n} + γ^nQ_{t+n−1}(S_{t+n}, A_{t+n})\]</span></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Sarsa价值评估.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/sarsa-n.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/sarsa-lambda.png" alt="图片"></p>
<h3 id="q值迭代-q-learning">2.3 Q值迭代 (Q-learning)</h3>
<p>Sarsa --&gt; on-policy</p>
<p>Q-learning --&gt; off-policy</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/Q-learning策略迭代.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>强化学习入门——从原理到实践，叶强</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and
Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度强化学习</title>
    <url>/2024/05/15/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>强化学习从深度学习角度出发的挑战：</p>
<ul>
<li>强化学习的奖励信号是有延迟的，而深度学习的输入输出是直接联系的</li>
<li>强化学习的序贯决策序列有很高的相关性，而深度学习的假设数据是独立同分布</li>
<li>强化学习的数据分布是会随着学习发生变化的，而深度学习的假设是底层分布固定的</li>
</ul>
<span id="more"></span>
<h2 id="1-DQN算法">1. DQN算法</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-4.png" alt="图1"></p>
<h3 id="1-1-DQN">1.1 DQN</h3>
<p>Deep Q-learning: DQN, <strong>Approximate $Q^*(s,a)$ by DQN,$Q(s,a;w)$</strong></p>
<p><strong>经历回放</strong>(experience replay): 在每个时间步t 中，DQN先将智能体获得的经验$(S_t, A_t, R_t, S_{t+1})$存入回放缓存中，然后从该缓存中均匀采样小批样本用于 Q-Learning 更新。主要作用是<strong>解决数据的相关性和非静态分布问题</strong>。</p>
<p>DQN2015的改进：<strong>增加目标网络</strong>。目标网络通过使用旧参数生成 Q-Learning 目标，使目标值的产生不受最新参数的影响，从而大大减少发散和震荡的情况。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-1.png" alt="图2"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-2.png" alt="图3"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-3.png" alt="图4"></p>
<h3 id="1-2-Double-DQN-DDQN">1.2 Double-DQN, DDQN</h3>
<p>Double DQN 是对 DQN 在减少过拟合方面的改进。这是由于DQN对动作值函数的max操作会引入一个正向的偏差，导致下一时刻的目标值存在过估计。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-5.png" alt="图5"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-6.png" alt="图6"></p>
<h3 id="1-3-Prioritized-Experience-Replay-优先经验回放">1.3 Prioritized Experience Replay, 优先经验回放</h3>
<p>采用优先级采样达到收敛所需的更新次数相比均匀采样要小很多，这也是进行优先经验池回放的原因。</p>
<p><strong>1.3.1 样本优先级</strong>:</p>
<p>样本优先级应满足两个条件：</p>
<ul>
<li>优先级在数值上应该和误差绝对值成单调递增关系，这是为了满足误差绝对值较大（即优先级较大）的样本获得更大的被抽样的机会；</li>
<li>优先级数值应大于0，这是为了保证每一个样本都有机会被抽样，即抽样概率大于0。</li>
</ul>
<p>优先级可以分为<strong>基于比例的样本优先级</strong>，<strong>基于排序的样本优先级</strong>。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-7.png" alt="图7"></p>
<p><strong>1.3.2 随机优先级采样</strong>:</p>
<p>采样方法：</p>
<ul>
<li>贪婪优先级采样，完全按照优先级去采样</li>
<li>一致随机采样，均匀采样</li>
<li>随机优先级采样，随机采样</li>
</ul>
<p>基本原则：</p>
<ul>
<li>样本被采样的概率应该和样本优先级成正相关关系</li>
<li>每一个样本都应该有机会被采样，即被采样的概率大于0</li>
</ul>
<p>Sum-Tree随机优先级采样，属于基于比例的样本优先级：</p>
<p>重要性采样: 用一个分布来计算当前分布的期望。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-8.png" alt="图8"></p>
<h3 id="1-4-Dueling-DQN">1.4 Dueling-DQN</h3>
<p>算法原理：将动作值的计算分解成状态值和优势函数，$Q(s,a)=V(s)+A(s,a)$。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-9.png" alt="图9"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DQN-10.png" alt="图10"></p>
<h2 id="2-策略梯度方法DDPG">2. 策略梯度方法DDPG</h2>
<h3 id="2-1-DPG-Deterministic-Policy-Gradient-确定性策略梯度">2.1 DPG, (Deterministic Policy Gradient) 确定性策略梯度</h3>
<p>确定性策略：每一步的动作都是确定的，即$a=\mu_\theta(s)$。确定性策略梯度算法正是使用了确定性策略的策略梯度算法。</p>
<h3 id="2-2-DDPG-Deep-Deterministic-Policy-Gradient">2.2 DDPG, (Deep Deterministic Policy Gradient)</h3>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/DDPG-1.png" alt="图11"></p>
<p>几个trick：</p>
<ul>
<li>$\mu^{\prime}(s_t)=\mu(s_t|\theta_{t}^{\mu} + N)$ 添加了一个随迭代次数衰减的随机噪声，增加了动作空间的探索</li>
<li>目标网络缓慢更新保证了训练的稳定性</li>
<li>batch normalization 使得可以在不同的环境中获取的特征统一</li>
</ul>
<p>问题：值函数过估计；自举造成的偏差传播</p>
<h3 id="2-3-PPO">2.3 PPO</h3>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/PPO-1.png" alt="图12"></p>
<h3 id="2-4-SAC-Soft-Actoe-Critic">2.4 SAC, (Soft Actoe Critic)</h3>
<p>熵：随机变量取各值时信息量的期望。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/SAC.png" alt="图13"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://github.com/QiangLong2017/Deep-Reiforcement-Learning">https://github.com/QiangLong2017/Deep-Reiforcement-Learning</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱数据管理</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<pre><code>知识图谱的目标是构建一个能够刻画现实世界的知识库，为自动问答、信息检索等应用提供支撑。因此，对知识的持久化存储并提供对目标知识的高效检索/更新是合格的知识图谱（系统）必须具备的基本功能，也是知识图谱数据管理的主要研究内容。

课程复习使用。</code></pre>
<h2 id="一.符号化知识图谱数据管理">一.符号化知识图谱数据管理</h2>
<h3 id="知识图谱数据模型">1.1 知识图谱数据模型</h3>
<p>数据模型主要包含<strong>逻辑组织结构、操作、约束</strong>三部分，它决定了数据管理所采取的方法和策略，对于存储管理、查询处理、查询语言设计均至关重要。常见的数据模型有层次数据模型，网状数据模型，系数据模型。</p>
<ul>
<li>逻辑组织结构（数据结构）：描述数据的类型、内容、性质以及数据间的联系等。</li>
<li>数据操作：描述在相应的数据结构上的操作类型和操作方式。</li>
<li>数据约束：描述数据结构内数据间的语法、词义联系、他们之间的制约和依存关系，以及数据动态变化的规则，以保证数据的正确、有效和相容。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理1.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理2.png" alt="图片"></p>
<h3 id="知识图谱数据的存储">1.2 知识图谱数据的存储</h3>
<ul>
<li>基于表结构的存储：关系数据库。</li>
<li>基于图结构的存储：常见的图数据库存储系统，如Neo4j, OrientDB,
HyperGraphDB, InfiniteGraph, InfoGrid.</li>
</ul>
<h3 id="知识图谱数据的检索查询">1.3 知识图谱数据的检索（查询）</h3>
<p>知识图谱查询语言可分为
<strong>声明式语言</strong>（描述“是什么”而不是“怎么做”，它关注的是结果而不是实现结果的过程），<strong>过程式语言</strong>（描述“怎么做”，它定义了一系列的步骤或指令来实现目标，如图遍历、导航式游走）。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理4.png" alt="图片"></p>
<p>这些查询语言都有点类似于sql，可借助sql进行进一步理解。</p>
<h2 id="二.参数化知识图谱大模型数据管理">二.参数化知识图谱（大模型）数据管理</h2>
<h3 id="大模型知识存储">2.1 大模型知识存储</h3>
<p>一个流行的观点是：PLM的<strong>FFN</strong>（Feed Forward Nerual
Network）模块存储了大量知识。</p>
<h3 id="大模型的知识编辑检索与更新">2.2
大模型的知识编辑（检索与更新）</h3>
<ul>
<li>知识编辑的目标：定向更新模型中具有的知识，该过程应当尽可能地保证有效、能泛化、避免对无关知识产生不良影响</li>
<li>逻辑研究对象：三元组</li>
<li>评估方法：提示生成，看结果是否符合预期</li>
<li>评估指标：有效性，泛化性，专一性，流畅度</li>
<li>知识编辑的实现方法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理5.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识图谱数据管理6.png" alt="图片"></p>
<h2 id="三.参考资料">三.参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱构建</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一-半结构化文本中的知识抽取">一.半结构化文本中的知识抽取</h2>
<p>目标：从百科普通条目半结构化网页中抽取实体属性名以及实体属性值。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA1.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA2.png" alt="图片"></p>
<h2 id="二-非结构化文本中的知识抽取">二.非结构化文本中的知识抽取</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA3.png" alt="图片"></p>
<h2 id="三-知识图谱众包构建">三.知识图谱众包构建</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA4.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA5.png" alt="图片"></p>
<h2 id="四-知识图谱质量控制">四.知识图谱质量控制</h2>
<p>知识图谱的质量控制指的是如何通过技术手段来确保知识图谱中知识的质量。</p>
<p><strong>评估思路</strong>：</p>
<ul>
<li>内检：利用知识图谱的内部知识进行综合推理，得到新的缺失知识，也可以发现相互矛盾的错误知识等。</li>
<li>外检：从外部知识源获得信息，然后结合知识图谱内部的知识进行比对，从而补全、修正或者更新知识图谱中的知识。</li>
</ul>
<p><strong>评估指标</strong>：准确性，一致性，完整性，时效性。</p>
<p><strong>评估方法</strong>：人工抽样检测法，一致性检测法，基于外部知识的对比评估法。</p>
<h2 id="五-参考资料">五.参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识建模与知识融合</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E5%BB%BA%E6%A8%A1%E4%B8%8E%E7%9F%A5%E8%AF%86%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<pre><code>课程复习使用，这章没有进行细致整理。

一. 知识建模

1.1 知识体系概述

1.2 典型知识体系

1.3 知识体系手工建模方法

1.4 知识体系自动建模方法

二. 知识融合

2.1 知识融合概述

2.2 知识体系融合方法

2.3 知识实例融合方法

三. 大模型中的知识融合

3.1 大模型对齐技术概述

3.2大模型对齐方法</code></pre>
<h2 id="一.-知识建模-知识融合">一. 知识建模 &amp; 知识融合</h2>
<p><a href="https://lwl1751.github.io/pdf/知识建模_知识融合_拼接.pdf">查看对应ppt</a></p>
<h2 id="二.大模型的知识融合">二.大模型的知识融合</h2>
<p>SFT，RLHF，DPO。</p>
<h2 id="三.参考资料">三.参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识推理</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/</url>
    <content><![CDATA[<pre><code>知识工程的生命周期：知识表示，知识建模，知识融合，知识管理，知识推理，知识应用。
</code></pre>
<h2 id="一-知识推理概述">一.知识推理概述</h2>
<p>概念区分：<strong>推理 reasoning</strong>，指的是通过已知知识推断出未知知识的过程。而<strong>推断 inference</strong> 是推理的一个步骤，指的是神经网络中的前向计算。</p>
<p><strong>按新知识推出途径的分类</strong>：</p>
<ul>
<li>归纳推理（特殊-&gt;一般），归纳推理所推出的结论没有包含在前提内容中。这种由个别事物或现象推出一般性知识的过程，是增加新知识的过程。</li>
<li>演绎推理（一般-&gt;特殊），演绎推理只不过是将已有事实揭示出来，因此它不能增加新知识。</li>
<li>缺省推理，也称默认推理，在知识不完全的情况下作出的推理，通常的形式：如果没有足够的证据证明结论不成立，则认为结论是正确的。</li>
<li>溯因推理，也称反绎推理、反向推理。，是推理到最佳解释的过程。它是开始于事实的集合，并推导出其最佳解释的推理过程。</li>
</ul>
<p><strong>按前提与结论的联系性质的分类</strong>：</p>
<ul>
<li>必然性推理，前提与结论有必然性联系，即前提蕴含结论。传统逻辑中通过直言命题变形的直接推理（换质法、换位法推理等）、通过命题间对应关系所进行的直接推理、三段论推理、各种假言推理、选言推理以及完全归纳推理等等，都属于必然性推理。</li>
<li>或然性推理，前提与结论无蕴含关系。简单枚举归纳推理、类比推理、回溯推理等等都属于或然性推理。例如，P：房间里有物品 → C：房子会着火，前提P只是结论C的“疑似”必要条件。</li>
</ul>
<p><strong>按推理过程单调性的分类</strong>：</p>
<ul>
<li>单调推理，是指在推理过程中随着推理的向前推进以及新知识的加入，推出的结论呈单调增加的趋势并越来越接近最终目标，且在推理过程中不会出现反复的情况，即不会因新知识的加入而否定前面推出的结论，从而使推理又退回到前面的某一步。</li>
<li>非单调性推理，是指在推理过程中，由于新知识的加入，不仅没有加强已经推出的结论，反而要否定它，使其需要退回到之前步骤。</li>
</ul>
<p><strong>知识确定性的分类</strong>：</p>
<ul>
<li>确定性推理大多指确定性逻辑推理，它具有完备的推理过程和充分的表达能力，可以严格地按照专家预先定义好的规则准确地推导出最终结论。但是确定性推理很难应对真实世界中，尤其是存在于网络大规模知识图谱中的不确定甚至不正确的事实和知识。</li>
<li>不确定性推理：并不是严格地按照规则进行推理，而是根据以往的经验和分析，结合专家先验知识构建概率模型，并利用统计计数、最大化后验概率等统计学习的手段对推理假设进行验证或推测。</li>
</ul>
<p><strong>按实现技术</strong>：</p>
<ul>
<li>逻辑推理，过程包含了严格的约束和推理过程（研究较多）。</li>
<li>非逻辑推理，自然语言推理，推理过程相对模糊。</li>
<li>符号推理，符号推理的特点就是在知识图谱中的实体和关系符号上直接进行推理。确定性和不确定性逻辑推理都属于符号推理。</li>
<li>数值推理，使用数值计算，尤其是向量矩阵计算的方法，捕捉知识图谱上隐式的关联，模拟推理的进行。</li>
</ul>
<p><strong>应用</strong>：知识图谱补全，知识问答，搜索与推荐，行业应用。</p>
<h2 id="二-演绎推理：推理具体事实，一般-特殊">二.演绎推理：推理具体事实，一般-&gt;特殊</h2>
<ul>
<li>经典逻辑推理：命题逻辑推理。</li>
<li>基于产生式规则的推理</li>
<li>基于概率逻辑学习的推理：马尔可夫逻辑网</li>
<li>自然语言演绎推理。</li>
</ul>
<h2 id="三-归纳推理：学习推理规则">三.归纳推理：学习推理规则</h2>
<ul>
<li>归纳推理概述</li>
<li>归纳推理逻辑程序设计</li>
<li>路径排序算法（PRA）</li>
<li>关联规则挖掘算法（AMIE）</li>
</ul>
<h2 id="四-基于深度学习的知识推理方法">四.基于深度学习的知识推理方法</h2>
<ul>
<li>基于表示学习</li>
<li>基于强化学习</li>
</ul>
<p><a href="https://lwl1751.github.io/pdf/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86_%E6%8B%BC%E6%8E%A5.pdf">查看 演绎推理，归纳推理，基于深度学习的知识推理方法 对应PPT</a></p>
<h2 id="五-大模型的推理方法">五.大模型的推理方法</h2>
<p>相比于传统方法的优势：由于LLM本身已经具备了出色的各类能力（e.g. 工具调用、知识生<br>
成等），因此LLM推理相比于传统推理有更多的形式和种类；CoT工作指出可以简单通过ICL的方式激发LLM的推理能力，因此与传统方式的大量训练相比，LLM的推理成本低、效率高、泛化性强。</p>
<p>相比于传统方法的劣势：部分LLM为黑盒模型，只能调整输入输出，推理过程的不透明度更高；LLM参数量大，幻觉现象严重，导致推理的输出更不可控且不稳定。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%861.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%862.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%863.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%864.png" alt="图片"></p>
<h2 id="六-参考资料">六.参考资料</h2>
<p>中国科学院大学老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识获取</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96/</url>
    <content><![CDATA[<pre><code>信息抽取：从自然语言文本中抽取指定类型的实体、关系、事件等事实信息，并形成结构化数据输出的文本处理技术。
</code></pre>
<h2 id="1-命名实体识别">1.命名实体识别</h2>
<h3 id="1-1-基于词典的方法">1.1 基于词典的方法</h3>
<p>典型方法包括正向匹配方法，反向匹配方法。原理：按照一定的策略将待分析的汉字串与一个充分大的词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。</p>
<h3 id="1-2-基于统计的方法">1.2 基于统计的方法</h3>
<ul>
<li>生成式方法，首先建立学习样本的生成模型，再利用模型对预测结果进行间接推理，如HMM。</li>
<li>判别式方法，基于由字构词的命名实体识别理念，将NER问题转化为判别式分类问题(序列标注问题)，如Maxent，SVM，CRF，CNN，RNN，LSTM+CRF。CRF做解码善于捕捉近距离的标签依赖，LSTM可以捕捉长距离的标签依赖。</li>
</ul>
<h3 id="1-3-基于大模型的方法">1.3 基于大模型的方法</h3>
<p>难点1: 任务形式差距。命名实体识别通常建模为序列标注任务，而大模型往往用于完成文本<br>
生成任务。难点2: 大模型存在较为严重的幻觉问题。</p>
<h2 id="2-关系知识抽取">2.关系知识抽取</h2>
<p>关系抽取：旨在自动识别由一对概念和联系这对概念的关系构成的相关三元组。如CEO(比尔盖茨，微软)。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%961.png" alt="图片"></p>
<h2 id="3-事件知识抽取">3.事件知识抽取</h2>
<p>事件是发生在某个特定的时间点或时间段、某个特定的地域范围内，由一个或者多个角色参与的一个或者多个动作组成的事情或者状态的改变。</p>
<p>事件关系：共指，时序，因果，子事件。</p>
<h2 id="4-参考资料">4.参考资料</h2>
<p>中国科学院大学赵军老师 知识工程课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>知识计算</title>
    <url>/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h2 id="一.-命题逻辑">一. 命题逻辑</h2>
<h3 id="基本概念">1.1 基本概念</h3>
<p><span style="color: purple;">知识库</span>：KB,
利用形式化语言定义的句子的集合。</p>
<p><span style="color: purple;">逻辑</span>：用于表示信息的形式化语言。</p>
<p><span style="color: purple;">语法</span>：句子的形式化结构。</p>
<p><span style="color: purple;">语义</span>：语句在每个可能模型中的真值。</p>
<p><span style="color: purple;">模型</span>： m, m是语句 <span class="math inline">\(\alpha\)</span> 的一个模型，表示语句在模型m
中为真，也称m 满足 <span class="math inline">\(\alpha\)</span>。</p>
<p><span style="color: purple;">逻辑</span>：用于表示信息的形式语言。</p>
<p><span style="color: purple;">逻辑蕴涵</span>：某语句在逻辑上跟随另一个语句，如
<span class="math inline">\(\alpha \models \beta\)</span>，当且仅当在
<span class="math inline">\(\alpha\)</span> 为真的模型中，<span class="math inline">\(\beta\)</span> 也为真，即 <span class="math inline">\(M(\alpha) \subseteq
M(\beta)\)</span>。同样的，对于知识库 <span class="math inline">\(KB
\models \alpha\)</span>，则有 <span class="math inline">\(M(KB)
\subseteq M(\alpha)\)</span>。如果算法i 可以根据KB推导出 <span class="math inline">\(\alpha\)</span>，记为 <span class="math inline">\(KB\vdash _i \alpha\)</span>。</p>
<p><span style="color: purple;">可靠性</span>：只能生成被蕴含的语句。即永远不会返回一个错误的结果。</p>
<p><span style="color: purple;">完备性</span>：生成所有被蕴含的语句。即返回的结果总能覆盖所有正确的解。</p>
<p><span style="color: purple;">命题逻辑</span>：应用一套形式化规则对以符号表示的描述性陈述进行推理的系统。</p>
<p><span style="color: purple;">原子命题</span>：一个或真或假的描述性陈述语句，并且无法再分解为更简单的命题
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑3.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑1.png" alt="图片"></p>
<p>理解：<span class="math inline">\(S_1\)</span> 不成立时，相当于 <span class="math inline">\(S_1\)</span>
为空集，空集可以被其他所有集合包含，因此 <span class="math inline">\(S_1\)</span> 不成立时 <span class="math inline">\(S_1 \Rightarrow S_2\)</span> 为真。</p>
<p><span style="color: purple;">逻辑等价</span>：命题p 和命题q
在所有情况下都具有相同的真假结果。 <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑2.png" alt="图片"></p>
<p><span style="color: purple;">有效性</span>：一个句子在所有模型中都为真，则句子是有效的。</p>
<p><span style="color: purple;">可满足性</span>：一个句子在某些模型中为真。</p>
<h3 id="归结原理">1.2 归结原理</h3>
<p><strong>合取范式</strong>，也被称为CNF范式，以文字析取式的合取式表达的语句。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/命题逻辑4.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>假言推理</p>
<p>在命题逻辑中，归结规则是可靠的、完备的。</p>
<p><strong>反证法</strong>：证明：<span class="math inline">\(KB\vdash  \alpha\)</span>，只需证明<span class="math inline">\(KB\wedge \neg \alpha\)</span>不可满足。</p>
<p><strong>子句集S 的归结闭包<span class="math inline">\(RC(S)\)</span></strong>：对S
中的子句或其派生子句反复使用归结规则而生成的所有子句的集合。</p>
<p><strong>限定子句</strong>：是受限形式的一种，它是指恰好只有一个正文字的析取式。</p>
<p><strong>Horn子句</strong>：包含至多一个正文字的析取式。用Horn子句判定蕴涵需要的时间与知识库的大小呈线性关系。如前向链接与反向链接。</p>
<p><strong>前向链接</strong>：如果蕴含式的所有前提已知，那么就把它的结论添加到已知事实集，真到查询q
被添加或者无法进一步推理。</p>
<p><strong>反向链接</strong>：从查询q 开始推理。如果查询q
为真，那么无须任何操作。否则寻找知识库中那些能以q
为结论的蕴含式。如果其中某个蕴含式的所有前提都能证明为真，那么q
为真。</p>
<h2 id="二.-谓词逻辑">二. 谓词逻辑</h2>
<p><strong>命题逻辑的局限性</strong>：
在命题逻辑中，每个陈述句是最基本的单位（即原子命题），无法对原子命题进行分解。因此，在命题逻辑中，不能表达<strong>局部与整体、一般与个别</strong>的关系，即使不同原子命题蕴含个体、群体和关系等内在丰富语义。</p>
<p><strong>谓词逻辑</strong>：将原子命题进一步细化，分解出<strong>个体、谓词和量词</strong>，来表达个体与总体的内在联系和数量关系。</p>
<p><strong>谓词</strong>：刻画个体属性或者描述个体之间关系存在性的元素,值域为{True,False}。</p>
<p><strong>量词</strong>：全称量词 <span class="math inline">\(\forall\)</span>，存在量词 <span class="math inline">\(\exists\)</span>。</p>
<p><strong>量词对偶</strong>：全称量词与存在量词可以相互转换，互相表示。</p>
<h2 id="三.-不确定性推理">三. 不确定性推理</h2>
<p><strong>效用</strong>：对不同决策结果的偏好。决策理论 = 概率理论 +
效用理论。</p>
<p><strong>不确定性推理</strong>：从不确定的初始证据出发，通过运用不确定性知识，推导出具有一定程度不确定性但合理的结论的思维过程。</p>
<p><strong>贝叶斯网</strong>：一个有向无环图模型，用简单的条件分布描述复杂的联合分布。其中，每个结点有一个条件概率分布<span class="math inline">\(P(X_i |
Parents(X_i))\)</span>，量化其父结点对该结点的影响。所有条件概率构成条件概率表。</p>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识计算5.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>贝叶斯网独立条件</strong>：一个结点的概率只与其父结点有关，与其它祖先结点无关。判断贝叶斯网是否独立可以利用贝叶斯球快速判断(通过、反弹、截止)。</p>
<p><strong>马尔科夫毯</strong>：一个结点的父结点、子结点、子结点的父结点。</p>
<p><strong>贝叶斯球算法</strong>：</p>
<pre><code>假设在贝叶斯网络中有一个按一定规则运动的球。已知中间结点 (或结点集合)Z，如果球不能由结点X 出发到达结点Y (或者由Y 到X )，则称X 和Y 关于Z 独立。

通过： 父结点-&gt;当前结点 方向的球，访问当前结点的任意子结点。子结点-&gt;当前结点 方向的球，访问当前结点的任意父结点。即父-&gt;子，子-&gt;父 。

反弹：父结点-&gt;当前结点 方向的球，访问当前结点的任意父结点。子结点-&gt;当前结点 方向的球，访问当前结点的任意子结点。即父-&gt;父，子-&gt;子。

截止：当前结点阻止贝叶斯球继续运动。

贝叶斯球规则：未知结点：父-&gt;子，子-&gt;父/子 。已知结点：父-&gt;父，子-&gt;截止。</code></pre>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识计算9.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p><strong>枚举推理（精确推理</strong>）：所有的变量集合X
,所有的证据变量集合E ,查询变量Q ,隐藏变量集合H = X - E- Q。则<span class="math inline">\(P(Q|E=e)=\alpha P(Q,E=e)=\alpha\sum_{H=h}
(Q,E=e,H=h)\)</span></p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识计算6.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识计算7.png" alt="图片"> <img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/知识计算8.png" alt="图片"></p>
<p><strong>变量消元</strong>：降低贝叶斯网推理复杂度。</p>
<p><strong>近似推理</strong>：抽取样本形成采样分布，通过该分布近似后验概率。</p>
<blockquote>
<p><strong>采样方法</strong>：</p>
</blockquote>
<pre><code>先验采样：按拓扑顺序进行采样，从$P(X_i | Parents(X_i))$采样$X_i$。

优点：简单直接，可以直接从已知的先验分布中抽样，计算方便。

缺点：不适用于高维空间，不适合于复杂模型。

拒绝采样：拒绝不符合证据E 的样本，即若$X_i$与观察值不一样，放弃这个样本。

优点：容易实现。

缺点：采样效率低，因为拒绝太多的样本，随着证据变量的增多，与证据相一致的样本数量指数级下降。

似然加权采样（重要性采样）：只生成与证据一致的事件，即固定证据变量。但这会导致采样分布与实际分布不一致，因此正需要对样本进行加权，权重为该样本与证据的相似性。

优点：简单易实现，克服了拒绝采样的采样效率低的问题。

缺点：不适用于高维空间，即当证据变量的个数增加时，性能可能大幅下降。 因为大多数的样本权值都非常低/高。

吉布斯采样：固定证据变量，每次模拟一个隐藏变量的采样，从$X_i ～ P(X_i | 马尔可夫毯(X_i))$，使频率收敛到真实概率。

优点：在高维空间中通常比其他采样方法更有效，因为它逐维采样。适合于复杂模型。

缺点：收敛速度慢。依赖于条件独立性假设：需要模型条件分布容易计算。</code></pre>
<figure>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/吉布斯采样1.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h2 id="四.-参考资料">四. 参考资料</h2>
<p>中国科学院大学高级人工智能，李国荣老师，课程课件</p>
<p><a href="https://www.bilibili.com/video/BV1ZY4y1J78y/?spm_id_from=333.788&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d" class="uri">https://www.bilibili.com/video/BV1ZY4y1J78y/?spm_id_from=333.788&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>连续状态系统基于模型的强化学习</title>
    <url>/2024/05/15/%E8%BF%9E%E7%BB%AD%E7%8A%B6%E6%80%81%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<blockquote>
<p>对于大规模的MDP问题，不希望使用查表（Table Lookup）的方式，通过<strong>函数近似</strong>来估计实际的价值函数的方式，既节约了资源，又能达到泛化的效果。</p>
<ul>
<li>$\hat{v}(s,w) = v_\pi (s)$</li>
<li>$\hat{q}(s,a,w) = q_\pi (s,a)$</li>
<li>$\hat{\pi}(a,s,w) = \pi (a|s)$</li>
</ul>
<p><strong>函数近似器</strong></p>
<ul>
<li>特征的线性组合</li>
<li>神经网络</li>
<li>决策树</li>
<li>最近邻方法</li>
<li>傅立叶/小波变换</li>
</ul>
</blockquote>
<span id="more"></span>
<h2 id="1-价值函数近似-Value-Fuction-Approximation-VFA">1. 价值函数近似, Value Fuction Approximation, VFA</h2>
<p>近似函数逼近的类型：</p>
<ul>
<li>input: s, output: $\hat{v}(s,w)$</li>
<li>input: s, output: $\hat{q}(s,a,w)$</li>
<li>input: s, output: $\hat{q}(s,a_1,w),\dots,\hat{q}(s,a_m,w)$</li>
</ul>
<h3 id="1-1-线性函数近似">1.1 线性函数近似</h3>
<p>近似价值函数: $\hat{v}(s,w)=x(s)^Tw$</p>
<p>目标函数: 均方误差。由于实际的价值函数不可知，用样本近似期望损失。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC2.png" alt="图1"></p>
<h3 id="1-2-神经网络值函数近似">1.2 神经网络值函数近似</h3>
<p>参看 深度强化学习 部分。</p>
<h3 id="1-3-基于模型的近似值迭代算法">1.3 基于模型的近似值迭代算法</h3>
<h3 id="1-4-模型无关的近似值迭代算法">1.4 模型无关的近似值迭代算法</h3>
<h2 id="2-近似策略迭代">2. 近似策略迭代</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC1.png" alt="图2"></p>
<p><strong>参考资料</strong>：</p>
<p>中国科学院大学林姝老师 强化学习课程课件</p>
<p>深度强化学习：基础、研究与应用 (董豪 等)</p>
<p>Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)</p>
<p><a href="https://www.bilibili.com/video/BV11V411f7bi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d">https://www.bilibili.com/video/BV11V411f7bi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d</a></p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>群体智能</title>
    <url>/2024/05/31/%E7%BE%A4%E4%BD%93%E6%99%BA%E8%83%BD/</url>
    <content><![CDATA[<blockquote>
<p>知识点：</p>
<ul>
<li>演化计算：蚁群优化算法、粒子群优化算法</li>
<li>博弈论</li>
<li>课程复习使用</li>
</ul>
</blockquote>
<h2 id="一-群体智能">一. 群体智能</h2>
<p><strong>群体智能</strong>：Social/Colletive Intelligence, “无智能”或者仅具有相对简单智能的个体通过合作表现出更高智能行为的特性。其中的“无智能”/简单智能并不是绝对意义上的智能，而是相对于群体表现出来的相对智能。（众人拾材火焰高）</p>
<p><strong>集群智能</strong>：Swarm Intelligence, 众多无智能的个体，通过相互之间的简单合作，所表现出来的智能行为。<br>
特点：</p>
<ul>
<li>分布式，无中心控制</li>
<li>随机性，非确定性</li>
<li>自适应，个体根据环境进行策略调整</li>
<li>正反馈，个体好的尝试会对个体产生正反馈</li>
<li>自发涌现，会在群体层面涌现出一种智能</li>
</ul>
<p><strong>博弈</strong>： Game Theory, 具备一定智能的理性个体，按照某种机制行动，群体层面表现出的智能。</p>
<p><strong>众包</strong>：Crowdsourcing, 设计合适的机制，激励个体参与，从而实现单个个体不具备的社会智能。</p>
<h2 id="二-演化计算">二. 演化计算</h2>
<h3 id="2-1-蚁群优化算法">2.1 蚁群优化算法</h3>
<p>Ant Colony Optimization, AOC. <strong>在图上寻找最优路径问题</strong>。</p>
<p><strong>形式化</strong>：蚂蚁(智能体) 依据一定的概率选择位置进行移动，途中会留下信息素，信息素会随时间挥发，且信息素浓度与该位置被选择的概率成正相关。</p>
<p><span style="color:purple;">用蚁群优化算法求解TSP问题</span>：</p>
<ul>
<li><strong>TSP问题描述</strong>：给定n个城市及每对城市之间的距离，求解访问每个城市一次、并回到起点的最短回路。</li>
<li><strong>符号表示</strong>：n个城市的有向图$G = (V,E)$，其中 $V={1,2,\dots,n}$,$E={(i,j)|i,j\in V}$,$d_{ij}$为节点之间的距离</li>
<li><strong>目标函数</strong>：$min f(w)=\sum_{l=1}^{n} d_{i_{l}i_{l+1}}$,其中，$s=(i_{1},\dots,i_{n})$</li>
<li><strong>根据信息素来选择下一个城市的概率计算为</strong>：</li>
</ul>
<p>$$<br>
p_{i j}^{k}(t)=\left{\begin{array}{ll}<br>
\frac{\left(\tau_{i j}(t)\right)^{\alpha}\left(\eta_{i j}(t)\right)^{\beta}}{\sum_{k \in \text { allowed }}\left(\tau_{i k}(t)\right)^{\alpha}\left(\eta_{i k}(t)\right)^{\beta}} &amp; j \in \text { allowed } \<br>
0, &amp; \text { otherwise }<br>
\end{array}\right.<br>
$$</p>
<p>其中，i为当前城市,j为下一城市，$\tau_{i j}(t)$为边$(i,j)$上的信息素浓度, $\eta_{i j}(t)=1/d_{i j}$是根据距离定义的启发式函数，$\alpha$，$\beta$反映了信息素与启发信息的相对重要性。</p>
<ul>
<li><strong>信息素更新</strong></li>
</ul>
<p>$$\begin{array}{l}\Delta \tau_{i j}^{k}=f(x)=\left{\begin{array}{cc}\frac{Q}{L_{k}}, &amp; (i, j) \in w_{k} \0, &amp; \text { otherwise }\end{array}\right. \\tau_{i j}(t+1)=\rho \cdot \tau_{i j}(t+1)+\Delta \tau_{i j} \\Delta \tau_{i j}=\sum_{k=1}^{m} \Delta \tau_{i j}^{k}\end{array}$$</p>
<p>其中：$Q$ 为常数，$w_k$ 表示第 $k$ 只蚂蚁在本轮迭代中走过的路径，$L_k$ 为路径长度，$\rho$ 为小于 1 的常数，反映信息素挥发速度。即路径越长，信息素越小。</p>
<ul>
<li><strong>算法流程</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 初始化 随机放置蚂蚁</span><br><span class="line"><span class="number">2.</span> 迭代过程</span><br><span class="line">    t = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> t &lt;= ItCount do (执行迭代)</span><br><span class="line">        <span class="keyword">for</span> k = <span class="number">1</span> to m do (对 m 只蚂蚁循环)</span><br><span class="line">            <span class="keyword">for</span> j = <span class="number">1</span> to n - <span class="number">1</span> do (对 n 个城市循环)</span><br><span class="line">                根据概率选择下一个城市；</span><br><span class="line">                将 j 置入禁忌表，蚂蚁转移到 j；</span><br><span class="line">            end <span class="keyword">for</span></span><br><span class="line">            计算每只蚂蚁的路径长度 \( L_k \);</span><br><span class="line">        end <span class="keyword">for</span></span><br><span class="line">        更新所有蚂蚁路径上的信息量；</span><br><span class="line">        t = t + <span class="number">1</span>;</span><br><span class="line">    end <span class="keyword">while</span></span><br><span class="line"><span class="number">3.</span> 输出结果</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>缺点</strong>：收敛速度慢（与m,n取值相关），容易陷入局部最优解(探索与开发的平衡)，不适于求解连续空间的优化问题。</li>
</ul>
<h3 id="1-2-粒子群优化算法">1.2 粒子群优化算法</h3>
<p>Particle Swarm Optimization, PSO. 求解<strong>连续解空间</strong>的优化问题，主要启发来源于对⻦群群体运动行为的研究。</p>
<ul>
<li><strong>形式化</strong>：每一只鸟(称为粒子，代表一个可行解解) 都有自己的状态信息：位置与速度，同时可以获得领域内其它鸟的信息，根据这些信息不断的改变自己的状态，去更好的适应环境，最终能找到最近最优解。</li>
<li><strong>算法流程</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%951.png" alt="图1"></p>
<p>其中，$x_{n}^{(i)}$为粒子i 在第 n 轮的位置，$v_{n}^{(i)}$为粒子i 在第 n 轮的速度，$p_{best}^{(i)}$为粒子i 的历史最好位置,$g_{best}^{(i)}$全局最好的历史位置。</p>
<p>速度更新公式包含三项，第一项为<strong>惯性项</strong>（保持原速度不变的倾向），第二项为<strong>记忆项</strong>（回到历史最好位置的倾向），第三项为<strong>社会项</strong>（走向粒子群全局最好位置的倾向）。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%952.png" alt="图2"></p>
<ul>
<li>
<p><strong>算法终止条件</strong>：迭代次数，最佳位置连续为更新的次数，适应度函数的值达到预期要求。</p>
</li>
<li>
<p><strong>优化</strong>：对惯性项加上一个权重</p>
</li>
<li>
<p><strong>优点</strong>：收敛速度快，所需微粒群规模较小</p>
</li>
<li>
<p><strong>缺点</strong>：不保证收敛到全局最优解</p>
<p>蚁群算法和粒子群算法的相同点：都基于自然界的生物行为；都使用多个个体（蚂蚁或粒子）组成的群体来寻找全局最优解；都可用于解决各种优化问题；都是不确定算法。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%953.png" alt="图3"></p>
<h2 id="三-博弈">三. 博弈</h2>
<ul>
<li>
<p><strong>局中人</strong>：在博弈中有权决定自己行动方案的博弈参加者。重要假设：局中人是自私的理性人。</p>
</li>
<li>
<p><strong>策略</strong>：博弈中可供局中人选择的行动方案。</p>
</li>
<li>
<p><strong>策略集合</strong>：局中人可以选择的策略的集合。</p>
</li>
<li>
<p><strong>局势</strong>：所有局中人选择的策略形成的策略组。</p>
</li>
<li>
<p><strong>博弈类型</strong></p>
<p>静态博弈（同时选择策略） vs 动态博弈（按顺序选择策略）</p>
<p>竞争博弈（炒股） vs 合作博弈（结盟）</p>
<p>完全信息博弈（每个局中人对所有局中人的策略及效用充分了解） vs 不完全信息博弈</p>
</li>
<li>
<p><strong>效用函数</strong>，payoff，通常用$U$来表示，是局势、时间（动态博弈中）的函数。每个局中人都有自己的效用函数。希望效用函数越大越好。</p>
</li>
<li>
<p><strong>最佳应对</strong>：对局中人1，若 $U_1(s,t) \ge U_1(s’,t)$ ，其中 s’ 是局中人除 s 外的其它策略，t 为局中人2的策略，$U_{1}(s,t)$为局中人1 从这组决策中获得的收益，则称策略 𝑠 是局中人1对局中人2的策略 t 的最佳应对。</p>
</li>
<li>
<p><strong>最优策略</strong>：如果一个局中人的某个策略对其它局中人的任何策略都是最佳应对，那么这个策略就是该局中人的占优策略。</p>
</li>
<li>
<p><strong>纳什均衡</strong>：如果一个局势下，每个局中人的策略都是相对其他局中人当前策略的最佳应对，则称该局势是一个纳什均衡。也就是博弈进入了僵局。不存在纯策略的纳什均衡。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%887.png" alt="图4"></p>
<ul>
<li><strong>混合策略</strong>：每个局中人以某个概率分布在其策略集合中选择策略。</li>
<li><strong>混合策略纳什均衡</strong>：给定其他局中人的策略选择概率分布的情况下， 当前局中人选择任意一个(纯)策略获得的期望效用相等。</li>
<li><strong>纳什定理</strong>：任何有限博弈都至少存在一个纳什均衡。但寻找博弈的纳什均衡是困难的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%886.png" alt="图5"></p>
<ul>
<li><strong>帕累托最优</strong>：对于一组策略选择(局势)，若不存在其他策略选择 使<strong>所有参与者</strong>得到至少和目前一样高的回报，且至少一个参与者会得到严格较高的回报，则这组策略选择为帕累托最优。</li>
<li><strong>社会最优</strong>：使参与者的回报之和最大的策略选择(局势)。社会最优的结果一定也是帕累托最优的结果，但帕累托最优不一定是社会最优。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%885.png" alt="图6"></p>
<ul>
<li>
<p><strong>机制设计</strong>：设计一个博弈，使其达到预期结果，如实现社会最优。</p>
</li>
<li>
<p><strong>maxmin策略</strong>，以我为主，最小化损失，抑制风险<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%881.png" alt="图7"></p>
</li>
<li>
<p><strong>minmax策略</strong>，抑制对手<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%882.png" alt="图8"></p>
</li>
<li>
<p><strong>匹配市场</strong></p>
<p>匹配定理：对于左右两部节点数相同的二部图，如果其不存在完全匹配（刚好一一对应），那么该二部图一定包含一个受限集。</p>
<p>匹配的效用：成功匹配的估价之和，称为匹配的效用。</p>
<p>最优匹配：效用最大的匹配。最优匹配对于个体而言不一定是最优的，甚至是最差的。</p>
<p>市场结清：每个卖方和买方都成交了。市场结清价格总是存在，且使得买卖双方总效用最优。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%883.png" alt="图9"></p>
</li>
<li>
<p><strong>中介市场</strong></p>
<p>买方和卖方通过中介交易。竞争不充分的地方，中介垄断价格。竞争充分的地方，中介的收益趋近于0。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%8D%9A%E5%BC%884.png" alt="图10"></p>
<ul>
<li>
<p><strong>议价权</strong></p>
<p>问题描述：给定一个网络，每个节点代表一个人，达成议价约定的两个人可以分配价值为1的东西。</p>
<p>不稳定边：对于结局中未参与配对的边，如果边的两个端点获得的收益之和小于1，则称这条边为不稳定边。</p>
<p>稳定结局：不存在不稳定边。</p>
<p>有备选项的议价：A、B两人议价，确定分配比例。A的备选项收益为x ，B的备选项为y 。要求 $x+y\le 1$，否则A和B达不成交易。则定义剩余价值为$s=1-x-y$。</p>
<p>纳什议价解：A的收益 $x+\frac{s}{2}=\frac{1+x-y}{2}$, B的收益 $y+\frac{s}{2}=\frac{1-x+y}{2}$。</p>
<p>均衡结局：结局中的任意一个参与配对的边都满足纳什议价解的条件。</p>
</li>
</ul>
<h2 id="四-参考资料">四. 参考资料</h2>
<p>中国科学院大学 计院高级人工智能课程课件</p>
<p><a href="https://www.zhihu.com/question/633226340/answer/3466364119">https://www.zhihu.com/question/633226340/answer/3466364119</a></p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>数值化知识表示</title>
    <url>/2024/06/01/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<pre><code>语言模型发展四范式：形式语言模型，统计语言模型，神经语言模型，预训练语言模型。

课程复习使用。
</code></pre>
<h2 id="一-语言的分布表示">一. 语言的分布表示</h2>
<ul>
<li>
<p><strong>Harris分布假说</strong>：上下文相似的词，其语义也相似。认为词的语义可以根据上下文统计获得，词之间的相似性可以通过向量距离衡量。</p>
</li>
<li>
<p><strong>word2vect</strong>: 词嵌入，上下文预测目标词</p>
</li>
<li>
<p><strong>CBOW</strong>：目标词预测上下文。</p>
</li>
</ul>
<h2 id="二-知识的分布表示">二. 知识的分布表示</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA1.png" alt="图1"></p>
<ul>
<li>
<p>打分函数：</p>
<p>位移距离模型。位移距离模型 (translational distance models)：基于位移假设，即头尾实体的表示存在位移关系，采用基于“头尾实体表示的位移”与“关系表示”的距离作为打分函数来衡量三元组成立的可能性。</p>
<p>语义匹配模型。无上述假设，直接利用头实体、关系和尾实体的数值表示进行计算，采用基于相似度的打分函数来衡量三元组成立的可能性。</p>
</li>
<li>
<p>模型训练：</p>
<p>封闭世界假设，但凡未在知识图谱中出现的事实都是错误的。</p>
<p>开放世界假设，知识图谱只包括正确的事实，那些不在其中出现的事实要么是错误的，要么是缺失的。</p>
</li>
</ul>
<h2 id="三-预训练语言模型">三. 预训练语言模型</h2>
<ul>
<li>Elmo: Embeddings from Language Models，首次使用大规模语料训练一个两层双向的RNN。</li>
<li>Bert: Bidirectional Encoder Representations from Transformers, transformer结构的encoder。</li>
<li>GPT: Generative Pre-Training,transformer结构的decoder。</li>
<li>In-context learning, CoT, few-shot learning, SFT, RLHF。由于对llm 部分比较了解，此处省略。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA2.png" alt="图2"></p>
<h2 id="四-讨论：预训练语言模型能否作为世界模型">四. 讨论：预训练语言模型能否作为世界模型</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA3.png" alt="图3"></p>
<p>相关论文：</p>
<p>Language Models Represent Space and Time, ICLR2024, MIT.</p>
<p>Reasoning with Language Model is Planning with World Model, EMNLP 2023, US San Diego.</p>
<p>Language Models Meet World Models, AAAI 2024, UC San Diego</p>
<h2 id="五-讨论：预训练语言模型能否作为知识库">五. 讨论：预训练语言模型能否作为知识库</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E6%95%B0%E5%80%BC%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA4.png" alt="图4"></p>
<h2 id="六-参考文献">六. 参考文献</h2>
<p>中国科学院大学赵军老师 知识工程 课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>符号化知识表示</title>
    <url>/2024/06/01/%E7%AC%A6%E5%8F%B7%E5%8C%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<blockquote>
<p>四种知识建模方式：严格结构化符号表示，松散结构化/自由形式 符号表示，数值化表示，数值与符号融合表示。</p>
<p>表示方法的衡量：表达能力、推理能力、计算能力、可读性。</p>
<p>目前的语义网革命并不是在科学上有革命性的突破，而大部分是工程上的挑战，其中标准化、规模化、系统开发与集成、用户交互等都是语义网技术面临的挑战。</p>
<p>课程复习使用。</p>
</blockquote>
<h2 id="一-经典知识表示理论">一. 经典知识表示理论</h2>
<h3 id="1-1-产生式规则">1.1 产生式规则</h3>
<ul>
<li>
<p><strong>产生式规则</strong>：用于表示事物之间的因果关系。</p>
</li>
<li>
<p><strong>确定性规则</strong>：P-&gt;Q。</p>
</li>
<li>
<p><strong>不确定性规则</strong>：P-&gt;Q(置信度)。当事实与前提条件不能精确匹配时，按照置信度的要求模糊匹配，并按特定算法将不确定性传递到结论。</p>
</li>
<li>
<p><strong>产生式系统</strong>：由数据库、规则库和推理机三部分组成。</p>
</li>
</ul>
<blockquote>
<p>数据库：用来存放问题的初始状态、已知事实、推理的中间结果和最终结论等。</p>
<p>规则库：用来存放与求解问题有关的所有规则。</p>
<p>推理机：用来控制整个系统的运行，决定问题求解的线路，包括匹配、冲突消解、路径解释等。</p>
<p>正向推理：类似于命题逻辑（查看<a href="https://lwl1751.github.io/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/">知识计算</a>）中的前向链接，从事实出发，通过规则获取结论。</p>
<p>反向推理：类似于命题逻辑中的反向链接，从目标出发，反向使用规则，求得已知事实</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA1.png" alt="图1"></p>
<h3 id="1-2-语义网络">1.2 语义网络</h3>
<ul>
<li>
<p><strong>语义网络</strong>：一种将相关概念联系起来的 有向图表示的 知识系统。</p>
</li>
<li>
<p><strong>语义基元</strong>：语义网络中最基本的语义单元，用三元组形式表示，&lt;节点1，关系，节点2&gt;。</p>
</li>
<li>
<p><strong>语义网络系统</strong>：由知识库和推理机组成。</p>
</li>
<li>
<p><strong>优点</strong>：使用直观的图结构来描述知识，表达自然，而且方便于计算机的存储和检索，有较为成熟的应用。</p>
</li>
<li>
<p><strong>缺点</strong>：由于缺少形式化的语义定义，不同的语义网络之间难以互相操作，表示不完善。推理过程复杂。</p>
</li>
</ul>
<h3 id="1-3-框架">1.3 框架</h3>
<p>框架是一种描述所论对象属性的数据结构。</p>
<ul>
<li><strong>框架名</strong>：用来指代某一类或某一个对象。</li>
<li><strong>槽</strong>：用来表示对象的某个方面的属性，<strong>语义网络中的三元组也可看作槽结构</strong>。</li>
<li><strong>侧面</strong>：从不同侧面描述某个属性。</li>
<li><strong>值</strong>：槽/侧面的取值。</li>
<li><strong>类型</strong>：类框架，实例框架。</li>
<li><strong>层次结构</strong>：子类-subclass of-&gt;父类，示例-instance of-&gt;类。</li>
<li><strong>推理</strong>：继承推理（下层框架继承上层框架的信息），匹配推理（安装条件进行推理）。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA2.png" alt="图2"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA3.png" alt="图3"></p>
<h3 id="1-4-脚本">1.4 脚本</h3>
<p>脚本与框架类似，由一组槽组成，用来表示特定领域内一些事件的发生序列。但脚本表示的知识有明确的时间或因果顺序，因此它描述的是一个<strong>过程</strong>而非静态知识。</p>
<p>脚本的结构化表示包括：进入条件，角色，道具，场景，结果。</p>
<h3 id="1-5-一阶谓词逻辑">1.5 一阶谓词逻辑</h3>
<p>由于语义网络、框架、脚本缺少形式化定义，不同网络之间难以相互操作，且推理过程复杂。因此引入了一阶谓词逻辑。参看<a href="https://lwl1751.github.io/2024/05/15/%E7%9F%A5%E8%AF%86%E8%AE%A1%E7%AE%97/">知识计算</a>。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA4.png" alt="图4"></p>
<h3 id="1-6-描述逻辑">1.6 描述逻辑</h3>
<p>通过<strong>概念类别</strong>来描述物理世界，没有一阶谓词逻辑中变量和谓词的概念，但具有形式化定义。描述逻辑有概念描述、属性、个体三个基础部分组成。</p>
<ul>
<li><strong>概念描述</strong>：表示一类事物。</li>
<li><strong>概念构造器</strong>：用两个概念描述构造出一个新的概念，有交集构造器、并集构造器、否定构造器。</li>
<li><strong>属性</strong>：作用于概念，必须搭配全称量词 $\forall$，或存在量词 $\exists$使用。</li>
<li><strong>个体</strong>：概念示例。</li>
<li><strong>知识库</strong>：包括术语（TBox，描述概念定义、公理），断言（ABox，描述个体知识）两部分。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA5.png" alt="图5"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA6.png" alt="图6"></p>
<h2 id="二-语义网的知识描述体系">二.语义网的知识描述体系</h2>
<ul>
<li><strong>本质</strong>：以Web数据的内容（即语义）为核心，用机器能够理解和处理的方式链接起来的海量分布式数据库。</li>
<li><strong>特征</strong>：Web上的事物拥有唯一的URI（通用资源标识符），事物之间存在显示链接，事物链接又具有不同的语义类型。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA7.png" alt="图7"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA8.png" alt="图8"></p>
<ul>
<li><strong>XML</strong>：由起始标签、元素内容和结尾标签构成，并且元素具有嵌套结构，同时没有约束嵌套的深度。</li>
<li><strong>RDF</strong>：Resource Description Framework。由于XML只定义了文档结构和数据类型，没有定义数据的语义，机器仍然无法理解文档的内容。为了让应用程序理解数据的语义，定义了RDF。RDF利用Web标识符（URI）来标识事物，并通过指定的属性和相应的值描述资源的性质或资源之间的关系。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA9.png" alt="图9"></p>
<p>注意：RDF并不是一种语言，只是一种书写规范。</p>
<ul>
<li>
<p><strong>RDFs</strong><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA10.png" alt="图10"></p>
</li>
<li>
<p><strong>OWL</strong>: Web Ontology Language，相比于RDFS，添加了更多用于描述类和属性的建模原语，支持更加丰富的语义表达并支持推理。</p>
</li>
<li>
<p><strong>RIF</strong>: Rule Interchange Format，一种不同的规则语言和推理引擎之间的交换格式。</p>
</li>
</ul>
<h2 id="三-参考资料">三. 参考资料</h2>
<p>中国科学院大学陈玉博老师 知识工程 课程课件</p>
]]></content>
      <tags>
        <tag>知识工程</tag>
      </tags>
  </entry>
  <entry>
    <title>高级人工智能复习绪论-机器学习部分</title>
    <url>/2024/06/02/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%8D%E4%B9%A0%E7%BB%AA%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%83%A8%E5%88%86/</url>
    <content><![CDATA[<p>课程复习使用。之前已经学过模式识别与机器学习课程，对这部分内容掌握较好，此处只整理相关考试知识点PPT。<a href="https://lwl1751.github.io/pdf/高级人工智能复习绪论-机器学习部分.pdf">点击查看PPT</a></p>
<p>参考资料：中国科学院大学高级人工智能课程</p>
]]></content>
      <tags>
        <tag>高级人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM训练</title>
    <url>/2024/06/05/LLM%20%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h2 id="一-DeepSpeed">一.DeepSpeed</h2>
<p>DeepSpeed 是由Mircrosoft 提供的<strong>分布式训练工具</strong>，DeepSpeed Zero（零冗余优化器）是大规模模型训练优化的技术，目的是<strong>减少模型的内存占用</strong>。<strong>Zero将模型参数分成三个部分</strong>：</p>
<ul>
<li>Optimizer States, 优化器在进行梯度更新的时候需要用到的数据</li>
<li>Gradient, 在反向转播过程中产生的数据，其决定参数的更新方向</li>
<li>Model Parameter, 模型参数，在模型训练过程中通过数据“学习”的信息</li>
</ul>
<p><strong>Zero 的级别如下</strong>：</p>
<ul>
<li>Zero-0, 不使用所有类型的分片，仅使用DeepSpeed作为DDP</li>
<li>Zero-1, 分割Optimizer States， 减少4倍内存，通信容量和数据并行性相同</li>
<li>Zero-2, 分割Optimizer States和Gradients，减少8倍内存，通信容量和数据并行性相同</li>
<li>Zero-3, 分割Optimizer States、gradients、Parametes，内存减少与数据并行度呈线性关系。例如，在64个GPU（Nd=64）之间进行拆分将产生64倍的内存缩减。通信量有50%的适度增长</li>
<li>Zero-Infinity, Zero-Infinity是Zero-3的扩展，它允许通过使用 NVMe 固态硬盘扩展 GPU 和 CPU 内存来训练大型模型</li>
</ul>
<h2 id="二-参数设置">二. 参数设置</h2>
<ul>
<li>lr_scheduler_type：学习率变化策略。如[linear, cosine, cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau]
<ul>
<li>linear, 学习率从一个较高的初始值开始，然后随着时间线性地减少到一个较低的值。</li>
<li>cosine, 学习率按照余弦曲线的形状进行周期性调整，这种周期性的起伏有助于模型在不同的训练阶段探索参数空间。</li>
<li>cosine_with_restarts, 这是余弦调整的一种变体，每当学习率达到一个周期的最低点时，会突然重置到最高点，然后再次减少。</li>
<li>polynomial, 学习率按照一个多项式函数减少，通常是一个幂次递减的形式。</li>
<li>constant_with_warmup, 开始时使用较低的学习率“预热”模型，然后切换到一个固定的较高学习率。</li>
<li>inverse_sqrt, 学习率随训练步数的增加按逆平方根递减。</li>
<li>reduce_lr_on_plateau, 当模型的验证性能不再提升时，自动减少学习率。</li>
</ul>
</li>
<li>warmup_steps：warmup步数。学习率经过多少步，增长到指定的数值。</li>
<li>warmup_ratio：用于指定线性warmup 占总训练步骤的比例，如果设置了warmup_steps，将会忽略warmup_ratio。</li>
<li>weight_decay：权重衰减，防止模型过拟合。</li>
<li>optim：优化器。<a href="https://mp.weixin.qq.com/s/nW6PpFsIbc0SwgI3QPKHFg">优化策略梯度下降算法：SGD、MBGD、Momentum、Adam、AdamW</a></li>
<li>lora_rank：lora矩阵的秩。一般设置为8、16、32、64等。</li>
<li>lora_alpha: lora中的缩放参数。一般设为16、32即可。</li>
<li>target_modules: lora训练模块，q_proj、k_proj、v_proj、o_proj、up_proj、down_proj。分别对应自注意力机制中的查询、键、值和输出投影层，以及LoRA特有的上投影和下投影层。</li>
<li>fp16：使用使用fp16混合精度。V100建议开启。</li>
<li>bf16：使用使用bf16混合精度。A100建议开启。</li>
<li>preprocessing_num_workers：数据预处理时的工作进程数量。</li>
<li>block_size：块大小，即输入序列的最大长度。</li>
<li>group_by_length：是否根据输入序列的长度进行分组。</li>
<li>gradient_checkpointing：梯度检查点。</li>
<li>ddp_timeout：ddp超时时间设置，若某些节点在规定时间内没有响应，则训练会报错并中止。</li>
<li>ddp_find_unused_parameters：是否在DDP中寻找未使用的参数。如果模型中存在未被使用的参数，该选项可以帮助检测到，以避免不必要的计算和资源浪费。</li>
<li>evaluation_strategy：训练期间采用的评估策略，[‘no’,‘steps’,‘eppoch’]，[不评估，每个eval_step后评估，每个epoch后评估]</li>
</ul>
<h2 id="三-参考资料">三.参考资料</h2>
<p><a href="https://mp.weixin.qq.com/s/qvhzagFFgNdtb3sSymhAdA">https://mp.weixin.qq.com/s/qvhzagFFgNdtb3sSymhAdA</a></p>
<p><a href="http://t.csdnimg.cn/Z2S9h">http://t.csdnimg.cn/Z2S9h</a></p>
]]></content>
      <tags>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>图的基础知识</title>
    <url>/2024/06/24/%E5%9B%BE%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<pre><code>课程知识复习使用。
</code></pre>
<h2 id="1-基本概念">1.基本概念</h2>
<ul>
<li><strong>图</strong>：一系列的点和对应的连接边。</li>
<li><strong>度数</strong>：顶点v的连接边数目。有向图中, 节点度数分为入度和出度。一个节点度数是其入度和出度的和。</li>
<li><strong>连通图</strong>：对于一个无向图，任意两个顶点之间都存在一条路径，则称之为连通图，否则为非连通图。</li>
<li><strong>最大连通区域</strong>：一个非连图由多个部分组成，其中规模最大的被称为最大连通区域。</li>
<li><strong>强连通图</strong>：对于一个有向图，对于任意一对节点A和B，存在从A到B的有向路径，同时也存在从B到A的一条有向路径，则称为强连通图。</li>
<li><strong>弱连通图</strong>：对于一个有向图，忽略边的方向，这个图在无向图的概念中是连通的，则这个有向图被称为弱连通图。</li>
<li><strong>完全图（团）</strong>：任意两个节点间都连接有边。</li>
<li><strong>二部图</strong>：节点分为两个不相交的集合𝑈和𝑉，每条边都分别连接集合𝑈和𝑉中的一个点。</li>
<li><strong>多重图</strong>：含有平行边或者自环边的图，即图中某两个顶点之间的边数不止一条，又允许顶点通过一条边与本身关联，则该图被称为多重图。</li>
<li><strong>自环图</strong>：无平行边而只存在自环边的图又被称为自环图。</li>
<li><strong>图的表示</strong>：邻接矩阵，邻接表，压缩稀疏表达$（CSR）$。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%9B%BE%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%863.png" alt="图片"></li>
</ul>
<h2 id="2-图的度量">2.图的度量</h2>
<p>对一张图进行度量的四种方式：度数分布，路径长度，聚集系数，连通分量。</p>
<ul>
<li>度数分布：图中度数的概率分布。对于有向图，需要考虑入度分布和出度分布。真实图的度数分布通常近似满足幂律分布，即度数大的节点非常少，度数小的节点占了绝大多数。</li>
<li>路径长度：路径中所包含的边的数目。节点间的距离为它们的最短路径长度。</li>
</ul>
<p>引申概念：<strong>小世界图</strong>：任意两个节点之间均存在较短路径，且节点间的最短路径⻓度与总节点数目不是线性关系，而是对数关系。</p>
<ul>
<li>聚集系数：衡量节点$v_i$ 的$k_i$ 个邻居间的连接紧密程度。计算如下：</li>
</ul>
<p>$$C_{local}(v_{i}) = \frac{2E_i}{k_i(k_i-1)}$$</p>
<p>其中$E_i$ 是$v_i$ 的邻居之间两两连边的数目。度数较大的节点，其聚集系数一般是较小的，而度数较小的节点，其聚集系数一般是较大的。</p>
<ul>
<li>连通分量：无向图的极大连通子图。</li>
</ul>
<h2 id="3-静态图生成模型">3.静态图生成模型</h2>
<p>给定待生成图的节点数 𝑁 和边数 𝑀，静态图生成模型一次性生成整张图。</p>
<h3 id="3-1Erdos-Renyi-模型-E-R-模型">3.1Erdos-Renyi 模型,E-R 模型</h3>
<ul>
<li>定义：𝐺(𝑁, 𝑝)，𝑁为图中节点总数，$p$为任意两个节点之间连边的概率。</li>
<li>生成算法：首先生成$N$ 个节点，对每个节点的除它以外的$N-1$ 个节点，都以概率$p$ 连边。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%9B%BE%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%862.png" alt="图1"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E5%9B%BE%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%861.png" alt="图2"></li>
</ul>
<h3 id="3-2Watts–Strogatz-小世界模型">3.2Watts–Strogatz 小世界模型</h3>
<ul>
<li>$k$-正则图：每个节点都有$k$个邻居的图。</li>
<li>生成算法：首先生成$k$ 正则图，接着对每一条边以概率$p$ 将这条边的其中一个端点移到平均随机选取的节点上。</li>
<li>当$p=0$时，生成图为$k$ 正则图；当$p=1$时，生成图为$E-R$ 图，此时边是随机生成的。</li>
</ul>
<h3 id="3-3Kronecker-模型">3.3Kronecker 模型</h3>
<p>递归的图生成模型，其基本思想是自相似性(self-similarity)，即通过在图的部分结构中模仿其整体结构的特征递归地生成图。递归地将图的整体结构代入到子结构中，在某种程度上模仿了真实社交网络图中社区的生⻓规律。</p>
<h2 id="4-动态图生成模型">4.动态图生成模型</h2>
<p>给定待生成图的节点数$n$或边数$m$，模型建模节点和边逐步加入的过程。</p>
<h3 id="4-1Barabasi–Albert-BA-模型">4.1Barabási–Albert (BA)模型</h3>
<ul>
<li>基本思想：假设图是以节点为中心不断增⻓的，通过不断增加节点来生成图。并且存在优先连接的现象，即一个节点当前 具有的度数越大，新增加的节点就越有可能与它连边。</li>
<li>生成算法：首先生成有$m_0$ 个节点的随机图，接下来每一步生成一个新的节点，由它延伸出$m$ 条边，与之前已有的节点相连。每次连接都按照已有节点当前的度数来分配连边的概率。</li>
</ul>
<h3 id="4-2以边为中心的优先连接模型">4.2以边为中心的优先连接模型</h3>
<p>通过加入新边来实现图规模的增⻓。注意的是，每次边插入可能引入0 个、1 个或 2 个新节点。</p>
<h2 id="5-参考资料">5.参考资料</h2>
<p>中国科学院大学邹磊老师，图数据管理与应用课程课件</p>
]]></content>
      <tags>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title>社区发现算法</title>
    <url>/2024/06/25/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<pre><code>课程知识复习使用。
</code></pre>
<p>图中的社区是指一张图中的一些子图。这些子图内部顶点直接紧密相连，而子图内部节点与外部节点之间连接是稀疏的。社区发现算法是需要找到图中所有的社区，可以分为基于层级聚类的方法和基于边介数的方法。</p>
<h2 id="1-基于层级聚类的方法">1. 基于层级聚类的方法</h2>
<ul>
<li>点关联强度：衡量两点间的关联强度。
<ul>
<li>两点间独立路径数：能够连接两点的所有互不相交(没有公共节点)的简单路径总数，即为割点数目。</li>
<li>全路径融合强度：认为节点间的关联强度与所有路径都相关，路径长度越小，强度越大。</li>
</ul>
</li>
<li>算法流程
<ul>
<li>选定并计算关联强度</li>
<li>初始化每个顶点为单个社区</li>
<li>定义两个社区的关联强度为其间<strong>所有点对的关键强度平均值</strong></li>
<li>每次合并两个关联强度最大的点对，合并关系形成层级关系</li>
<li>聚类质量达到一定程度之后不再继续</li>
</ul>
</li>
<li>衡量标准：模块度，是评估社区结果质量高低的度量方法。模块度Q 计算如下：</li>
</ul>
<p>$$Q \propto  \sum_{s\in S} [ 社区s内部的实际边数 - 社区s内部的期望边数 ] $$</p>
<p>模块度取值范围为[-1,1]，实践中，模块度达到0.3到0.7之间就说明划分质量很好。</p>
<p>不足：对度数低的点的社区分配不友好。例如一个顶点如果只有一条边，则这个顶点理应分配在其唯一邻居所在的社区，然后基于层级聚类的方法中，该点同邻居的关联强度低，很容易被排斥在邻居所属社区之外。</p>
<h2 id="2-基于边介数的方法">2. 基于边介数的方法</h2>
<ul>
<li>边介数：对于图上的一条边 $e$ 而言，$e$ 的介数是指图中所有点对的最短路径中，要经$e$ 的路径比例总和。</li>
<li>算法流程：
<ul>
<li>计算所有边的介数</li>
<li>移除介数最高边</li>
<li>重新计算所有边的介数</li>
<li>如果剩余所有边的介数都低于一定阈值则终止，否则回到步骤二</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%951.png" alt="图1"></p>
<h2 id="3-其它社区发现算法：BK-k-clique-k-core">3. 其它社区发现算法：BK,k-clique,k-core</h2>
<ul>
<li>
<p>团（完全图）：任意两个节点间都连接有边。</p>
</li>
<li>
<p>最大团：一个图中顶点数最多的团</p>
</li>
<li>
<p>极大团：加入任何其它顶点都无法在图中构成更大的完全子图的团</p>
</li>
<li>
<p>BK算法求解极大团：<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%952.png" alt="图2"></p>
</li>
<li>
<p>k-clique算法，允许社区重叠，将发现的社区定义为团联。给定一个大于1的正整数k，由一系列k-团(节点数目为k的团)互相连接形成的子图称为k-团联。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%953.png" alt="图3"></p>
</li>
<li>
<p>k-core算法：子图是连通的，且每个节点的度数均不小于k。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%954.png" alt="图4"></p>
</li>
</ul>
<h2 id="4-社区搜索">4. 社区搜索</h2>
<p>社区发现是为了找到图G中所有的社区，然而社区搜索只需找到用户所关心的社区，如找到包含用户输入的查询点/查询点集合的社区。即在图G中找到一个连通子图H，且$f(H)$在所有可能子图的得分函数中值最大。得分函数$f(H)$定义为H中最小的节点度数。</p>
<ul>
<li>
<p>贪心算法<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%955.png" alt="图5"></p>
</li>
<li>
<p>K-Truss算法：K-Truss要求子图H中任意一条边都被包含在至少(k-2)个不同的三角形中。</p>
<ul>
<li>支持度,$sup(e,G)$：包含e的三⻆形数目</li>
<li>子图 Trussness：子图H中所有边的最小支持度</li>
<li>边 Trussness：边e所在所有子图中最大的子图 Trussness</li>
<li>k 分类：图G中所有边 Trussness 为k 的边集合</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%956.png" alt="图6"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%957.png" alt="图7"></p>
<h3 id="5-图划分">5. 图划分</h3>
<p>图的划分是将图切分为多个不相交的子集，每个子集称为一个分割，并希望各个子图的大小要相对均衡。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%958.png" alt="图8"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%959.png" alt="图9"></p>
<p>KL 算法最大的问题是可扩展性太差，对于大图考虑所有可能的节点置换的时间复杂性太高。一种改进的算法则是METIS 算法: 通过对基于点边的融合来不断压缩原始图，当原图小到一定程度之后，再进行KL 等图分割算法，最后基于分割结果进行原图恢复。</p>
<h2 id="6-参考资料">6.参考资料</h2>
<p>中国科学院大学邹磊老师，图数据管理与应用课程课件</p>
]]></content>
      <tags>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title>Unifying Large Language Models and Knowledge Graphs: A Roadmap</title>
    <url>/2024/07/06/Unifying-Large-Language-Models-and-Knowledge-Graphs-A-Roadmap/</url>
    <content><![CDATA[<blockquote>
<p>类型：文献综述</p>
<p>第一作者：Shirui Pan</p>
<p>作者单位：Griffith University</p>
<p>发表时间：2024/07</p>
<p>发表期刊：IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</p>
<p>键内容：对 KG-enhanced LLMs, LLM-augmented KGs, Synergized LLMs + KGs 三种框架进行介绍</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs18.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs4.png" alt="图片"></p>
<h2 id="1-background">1. background</h2>
<p>KGs的四种分类：百科全书式知识图谱、常识知识图谱、特定领域知识图谱和多模态知识图谱。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs2.png" alt="图片"></p>
<p>LLM 与 KGs 的优缺点对比：</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs1.png" alt="图片"></p>
<p>其中，KGs 的缺点 Unseen Facts：知识图谱无法有效地对看不见的实体进行建模并表示新的事实知识。此外，知识图谱中丰富的文本信息常被忽略。</p>
<p>LLM 与 KGs 的应用示例：</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs3.png" alt="图片"></p>
<h2 id="2-KG-enhanced-LLMs">2. KG-enhanced LLMs</h2>
<h3 id="2-1-KG-Enhanced-LLM-Pre-Training">2.1 KG-Enhanced LLM Pre-Training</h3>
<blockquote>
<p><strong>2.1.1  Integrating KGs into training objective</strong></p>
</blockquote>
<p>两种方式：</p>
<ul>
<li>在预训练中暴露更多的实体，通过改变词语的掩码概率实现。如GLM中，假定在知识图谱中一定跳数内可以到达的实体是较为重要的实体，并且在预训练期间会给予它们更高的掩码概率。而在SKEP中，通过词语的情感分配不同的掩码概率。</li>
<li>更改训练训练的目标函数。如对ERNIE，目标为tokens到entity的对齐链接，如下图所示。对WKLM，首先用其他相同类型的实体替换文本中的实体，然后将它们输入 LLM，训练目标为区分实体是否已被替换。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs5.png" alt="图片"></p>
<blockquote>
<p><strong>2.1.2 Integrating KGs into LLM inputs</strong></p>
</blockquote>
<p>Colake中提出的集成方法如下图所示，需要注意的是：</p>
<ul>
<li>只有句子中的实体才能访问知识图谱中的三元组信息</li>
<li>输入句子的 tokens 形成了完全图</li>
<li>更多的关注热门实体，而忽视了低频和长尾实体。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs6.png" alt="图片"></p>
<blockquote>
<p><strong>2.1.3 KGs Instruction-tuning</strong></p>
</blockquote>
<p>KG指令微调的目的不是向LLM注入事实知识，而是让LLM更好的理解KG的结构。具体分析见 KG指令微调部分。</p>
<h3 id="2-2-KG-Enhanced-LLM-Inference">2.2 KG-Enhanced LLM Inference</h3>
<p>虽然通过训练确实可以为LLM注入大量的知识，但是这些知识缺乏时效性，且模型训练需要消耗大量资源，因此可以借助KGs 在LLM推理时注入知识。两种方式：</p>
<ul>
<li>RAG，Retrieval-Augmented Knowledge Fusion<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs7.png" alt="图片"></li>
<li>KGs Prompting，设计提示，将结构化的知识图谱转换为文本序列，然后将其作为上下文输入到LLM中。但通常设计提示需要耗费大量人力。</li>
</ul>
<h3 id="2-3-KG-Enhanced-LLM-Interpretability">2.3 KG-Enhanced LLM Interpretability</h3>
<p>LLM 可解释性是指对其内部工作和决策过程的理解和解释。研究人员试图利用知识图谱来提高LLM的可解释性，大致可以分为两类：1）用于语言模型探测的知识图谱，2）用于语言模型分析的知识图谱。</p>
<blockquote>
<p>KGs for LLM Probing，旨在理解 LLM 中存储的知识</p>
</blockquote>
<p>下图为LAMA 使用 KGs 对 LLM 进行知识探测的示例图，首先通过预定义的提示模板将 KG 中的事实转换为完形填空语句，然后使用 LLM 来预测缺失的实体。预测结果用于评估 LLM 中存储的知识。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs8.png" alt="图片"></p>
<p>注意：可以通过优化提示来更准确的评估 LLM 中包含的知识，且研究发现 LLM 对低频/尾部事实知识的掌握较差。</p>
<blockquote>
<p>KGs for LLM Analysis，旨在分析 LLM 中的推理过程，如 LLM 如何生成结果，LLM 的功能和结构如何工作等。</p>
</blockquote>
<p>一种方式是将 LLM 在每个推理步骤中生成的结果都以知识图谱为基础。这样，LLM的推理过程就可以通过从KG中提取图结构来解释，如下所示。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs9.png" alt="图片"></p>
<h2 id="3-LLM-AUGMENTED-KGS">3. LLM-AUGMENTED KGS</h2>
<h3 id="3-1-LLM-Augmented-KG-Embedding">3.1 LLM-Augmented KG Embedding</h3>
<p>知识图嵌入（KGE）旨在将每个实体和关系映射到低维向量空间，这些嵌入包含知识图谱的语义和结构信息。通常有两种方式：1）采用 LLM 通过对实体和关系的文本描述进行编码来丰富KG的表示，2）使用 LLM 将图结构和文本信息同时合并到嵌入空间中。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs10.png" alt="图片"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs11.png" alt="图片"></p>
<h3 id="3-2-LLM-Augmented-KG-Completion">3.2 LLM-Augmented KG Completion</h3>
<p>知识图补全（KGC）旨在推断给定知识图谱中缺失的事实。与KGE类似，传统的KGC方法主要关注KG的结构，而没有考虑广泛的文本信息。然而，LLM 的集成使 KGC 方法能够编码文本或生成事实，以获得更好的 KGC 性能。这些方法根据其使用方式分为两个不同的类别：1）LLM 作为编码器（PaE），2）LLM 作为生成器（PaG）。顾名思义，将 LLM 分别作为 encoder, decoder。</p>
<blockquote>
<p>LLM as Encoders (PaE)</p>
</blockquote>
<p>优点：</p>
<ul>
<li>易于微调，在LLM基础上加上一个预测层，训练时可以冻结LLMs，只需优化预测头</li>
<li>输出可整合，预测输出易于与现有的KGC功能整合，用于不同的KGC任务</li>
</ul>
<p>缺点</p>
<ul>
<li>计算开销大，推理阶段需要为每个候选实体计算分数，计算昂贵</li>
<li>不能泛化到未见过的实体</li>
<li>需要LLMs的表示输出，但无法获取闭源LLM 的表示输出</li>
</ul>
<blockquote>
<p>LLM as Generators (PaG)</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs12.png" alt="图片"></p>
<p>优点：无需微调，能泛化到未见过的实体</p>
<p>缺点：生成的实体可能不在KGs 中，设计有效的提示需要耗费大量人力</p>
<h3 id="3-3-LLM-Augmented-KG-Construction">3.3 LLM-Augmented KG Construction</h3>
<p>知识图谱构建通常包含以下几个步骤：实体抽取，共指消解，关系抽取。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs13.png" alt="图片"></p>
<h3 id="3-4-LLM-Augmented-KG-to-Text-Generation">3.4 LLM-Augmented KG-to-Text Generation</h3>
<p>旨在生成准确一致地描述输入知识图谱信息的高质量文本，但收集大量图文并行数据具有挑战性且成本高昂，导致训练不足和生成质量差。有两种方法：1）利用 LLM 的知识，如下图所示，线性遍历知识图谱作为输入，用 LLM 获取文本输出，但存在问题是 LLM 的无监督预训练目标不一定与知识图谱到文本生成的任务很好地契合，2）构建大规模弱监督知识图谱文本语料库。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs14.png" alt="图片"></p>
<h3 id="3-5-LLM-Augmented-KG-Question-Answering">3.5 LLM-Augmented KG Question Answering</h3>
<p>知识图谱问答（KGQA）旨在根据知识图谱中存储的结构化事实找到自然语言问题的答案。可以将LLM作为实体/关系的抽取器，或是答案推理器，即根据检索到的事实进行推理。</p>
<h2 id="4-SYNERGIZED-LLMS-KGS">4. SYNERGIZED LLMS +KGS</h2>
<p>主要可以分为协同知识表示，协同推理两部分。</p>
<h3 id="4-1-Synergized-Knowledge-Representation">4.1 Synergized Knowledge Representation</h3>
<p>文本语料库和知识图谱都蕴藏着海量的知识。然而，文本语料库中的知识通常是隐式的、非结构化的，而知识图谱中的知识是显性的、结构化的。协同知识表示旨在设计一个协同模型，可以有效地表示来自 LLM 和 KG 的知识。协同模型可以更好地理解两个来源的知识，使其对许多下游任务有价值。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs15.png" alt="图片"></p>
<h3 id="4-2-Synergized-Reasoning">4.2 Synergized Reasoning</h3>
<p>可以分为 LLM-KG 融合推理（使用两个独立的 LLM 和 KG 编码器来处理文本和 KG 输入），LLM 代理推理（使用 LLM 作为与 KG 交互进行推理的 代理）两部分。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs16.png" alt="图片"></p>
<h2 id="5-FUTURE-DIRECTIONS-AND-MILESTONES">5. FUTURE DIRECTIONS AND MILESTONES</h2>
<ul>
<li>KGs for Hallucination Detection in LLMs</li>
<li>KGs for Editing Knowledge in LLMs，尽管 LLM 能够存储海量的现实世界知识，但他们无法随着现实世界情况的变化而快速更新其内部知识。有研究人员提出了一些研究工作来编辑 LLM 的知识，而无需重新训练整个模型。然而，此类解决方案仍然存在性能不佳或计算开销过大的问题。现有研究还表明，编辑单个事实会对其他相关知识产生连锁反应。因此，有必要开发一种更高效、更有效的方法来编辑 LLM 知识。最近，研究人员尝试利用知识图谱来有效地编辑 LLM 的知识。</li>
<li>KGs for Black-Box LLMs Knowledge Injection</li>
<li>Multi-Modal LLMs for KGs</li>
<li>LLMs for Understanding KG Structure，基于纯文本数据训练的 LLM 并非旨在理解知识图等结构化数据。因此， LLM 可能无法完全掌握或理解知识图谱结构传达的信息。一种直接的方法是将结构化数据线性化为 LLM 可以理解的句子。然而，知识图谱的规模使得不可能将整个知识图谱线性化为输入。此外，线性化过程可能会丢失知识图谱中的一些底层信息。因此，有必要开发能够直接理解KG结构并对其进行推理的LLM。</li>
<li>Synergized LLMs and KGs for Birectional Reasoning</li>
</ul>
<h2 id="6-CONCLUSION">6. CONCLUSION</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/unifying_llm_kgs17.png" alt="图片"></p>
<h2 id="7-参考论文">7. 参考论文</h2>
<p>Pan S, Luo L, Wang Y, et al. Unifying large language models and knowledge graphs: A roadmap[J]. IEEE Transactions on Knowledge and Data Engineering, 2024.</p>
]]></content>
      <tags>
        <tag>知识图谱</tag>
        <tag>大模型</tag>
        <tag>文献笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding</title>
    <url>/2024/07/07/Knowledge-Prompting-in-Pre-trained-Language-Model-for-Natural-Language-Understanding/</url>
    <content><![CDATA[<pre><code>第一作者：Jianing Wang
作者单位：华东师范大学
发表时间：2022/10
发表期刊：EMNLP2022
关键内容：提出了一种基于知识提示的 PLM 架构：KP-PLM。首先根据每个句子上下文的知识库构建一个知识子图，再设计多个连续提示规则，将知识子图转化为自然语言提示，对模型进行微调。并提出了两个知识感知的自监督任务：prompt relevance inspection and masked prompt modeling。前者旨在让PLM 学习多个知识提示的语义相关性，后者预测 prompt 中的屏蔽实体。
</code></pre>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM1.png" alt="图1"><br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM3.png" alt="图2"></p>
<h2 id="1-引言">1. 引言</h2>
<p>增强PLM知识的方法有以下几种：</p>
<ul>
<li>knowledge-masking-based methods</li>
<li>knowledge-fusion-based methods</li>
<li>graph-learning-based methods</li>
</ul>
<p>但是这些方法存在以下不足：</p>
<ul>
<li>一些方法通过堆叠复杂的模块来修改现有PLM的内部结构，增加了模型的计算成本</li>
<li>一些方法从知识库中引入冗余和不相关的知识（知识噪声），可能会降低模型的性能</li>
</ul>
<p>因此，作者提出了一种基于知识提示的 PLM 架构：KP-PLM，可以有效的解决以上两个不足，如图1、图2所示。</p>
<h2 id="2-KP-PLM-模型框架">2.KP-PLM 模型框架</h2>
<h3 id="2-1-Knowledge-Prompting">2.1 Knowledge Prompting</h3>
<p>知识提示旨在为每个句子构建知识子图，然后将事实知识转化为自然语言提示。下图分两个步骤说明了该过程。</p>
<ul>
<li>上下文知识子图构建。从每个句子中抽取中所有的实体，选择主题实体，根据该主题实体构建一个2-hop子图，再对该子图进行pruning。pruning规则：若尾实体未出现在该句子的抽取实体集合中，删除该路径。</li>
<li>Continuous prompting mapping.根据子图中的一阶和二阶结构信息设计了三种类型的提示映射规则。其中，[K]、[/K]为知识触发标记，[Pi]为伪标记。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM2.png" alt="图3"></p>
<h3 id="2-2-Model-Architecture">2.2 Model Architecture</h3>
<p>主要是对 Input Embedding Layer 进行介绍。嵌入分为 token embeddings, position embeddings and segmentation embeddings 三种类型。</p>
<ul>
<li>token embeddings，the trigger and pseudo tokens are randomly initialized while others are initialized by looking up the PLM embedding table.</li>
<li>position embeddings，为了减轻 prompt 顺序对模型性能的影响，所有 prompt 的起始位置编码相同</li>
<li>segmentation embeddings，同样的，对所有 prompt 使用相同的分段 id，并为其起始标记设置相同的位置 id。</li>
</ul>
<h3 id="2-3-Self-supervised-Pre-training-Tasks">2.3 Self-supervised Pre-training Tasks</h3>
<ul>
<li>Prompt Relevance Inspection (PRI)，由于提示可以将事实知识注入 PLM，因此它们应该在语义上与上下文序列相关。因此，作者设计了一种提示相关性检查任务，增强了模型学习提示与句子相关性的能力。对于训练语料库中的每个句子 S，生成一组相关提示集 PS。并构造一个“正”提示集 Pos。 此外，对于每个句子 S，可以从 PS 中随机选择一个提示，并从该提示中随机选择一个实体，替换为 KB 中的任意实体。最后将更新的提示标记为否定提示，并添加到“负”提示集 Neg。利用这两个提示集进行训练。</li>
<li>Masked Prompt Modeling (MPM). 与传统的实体预测相比，有两点不同：一是搜索空间不同（不是在整个 PLM 词汇库上进行搜索，而是在上下文知识子图中进行搜索），二是训练数据集的构建（给定一个训练语料库，对于每个句子 S，生成一组相关提示集 PS，在任意选择的提示 P ∈ PS 中使用 [MASK] 标记随机屏蔽实体（主题实体除外），构成训练集用于模型训练）</li>
</ul>
<h2 id="3-实验">3. 实验</h2>
<ul>
<li>基座模型：RoBERTa-base</li>
<li>数据集：使用的知识库是WikiData5M，其中包括3,085,345个实体和822个关系类型。</li>
<li>对比PLM选择：ERNIE-THU, KnowBERT, KEPLER, CoLAKE, K-Adapter, DKPLM。</li>
<li>实验设置：Knowledge-aware Tasks(使用实体类型预测、关系提取和知识探测三个任务来评估模型性能)，Performance on General NLU Tasks，Knowledge Prompting Study，Ablation Study</li>
</ul>
<h3 id="3-1-实验结果">3.1 实验结果</h3>
<ul>
<li>
<p>知识感知–实体类型预测，根据给定一个句子和相应的实体提及，预测实体的类型。模型变体 KP-PLMKNOW ，它通过直接将知识提示与每个示例连接起来，在微调阶段使用知识提示。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM4.png" alt="图4"></p>
</li>
<li>
<p>知识感知–关系抽取，根据相应的文本对两个给定实体之间的关系进行分类。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM8.png" alt="图5"></p>
</li>
<li>
<p>知识感知–知识探测，旨在评估 PLM 是否拥有零样本环境下的内在事实知识。</p>
</li>
<li>
<p>Performance on General NLU Tasks，KP-PLM 与 RoBERTa-base 在多个自然理解任务中的对比，<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM5.png" alt="图6"></p>
</li>
<li>
<p>不同知识增强PLM 架构的对比<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM6.png" alt="图7"></p>
</li>
<li>
<p>消融实验<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/KP-PLM7.png" alt="图8"></p>
</li>
</ul>
<h2 id="4-结论">4.结论</h2>
<p>In this paper, we presented a seminal knowledge prompting paradigm, based on which a novel knowledge-prompting-based PLM framework KPPLM was proposed. Experimental results validate the effectiveness of knowledge prompting in boosting the performance of PLMs.</p>
<h2 id="5-参考文献">5.参考文献</h2>
<p>Wang J, Huang W, Shi Q, et al. Knowledge prompting in pre-trained language model for natural language understanding[J]. arXiv preprint arXiv:2210.08536, 2022.</p>
]]></content>
      <tags>
        <tag>知识图谱</tag>
        <tag>大模型</tag>
        <tag>文献笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Ontology-enhanced Prompt-tuning for Few-shot Learning</title>
    <url>/2024/07/08/Ontology-enhanced-Prompt-tuning-for-Few-shot-Learning/</url>
    <content><![CDATA[<blockquote>
<p>第一作者：Hongbin Ye, Ningyu Zhang</p>
<p>作者单位：浙江大学</p>
<p>发表时间：2022/4</p>
<p>发表期刊：Proceedings of the ACM Web Conference 2022</p>
<p>关键内容：探索如何更好的使用预训练语言模型进行 few-shot learning 知识注入，并提出本体增强提示调优（OntoPrompt）。如何优化：将KG的本体知识转化为文本用于模型训练，并修改注意力机制以减小知识噪声，最后，对本体嵌入向量也进行参数训练。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt1.png" alt="图1"></p>
<h2 id="1-INTRODUCTION">1. INTRODUCTION</h2>
<p>few-shot learning 中存在的三个问题：</p>
<ul>
<li>知识缺失。由于外部知识库的不完整性，可能无法检索与任务相关的事实，从而无法为下游任务提供有用的信息。</li>
<li>知识噪声。先前的研究已经证明，并非所有的知识都对下游任务有益，不加区别地注入知识可能会导致负面的知识注入，从而不利于下游任务的性能。</li>
<li>知识异质性。下游任务的语言语料库与注入的知识有很大不同，导致两种独立的向量表示，即注入知识不能很好的泛化到下游任务中。</li>
</ul>
<p>针对这三个问题，作者提出了对应的解决方案：</p>
<ul>
<li>利用预定义的模板将基于外部知识图谱的本体知识转换为文本作为提示。</li>
<li>通过跨度敏感的知识注入，来选择信息知识，从而减轻噪声注入。</li>
<li>通过集体训练算法来共同优化 下游任务的语言语料库与注入知识 的表示。</li>
</ul>
<p>最后在关系提取、事件提取和知识图补全这三个任务进行测试，取得了不错的效果。</p>
<h2 id="2-METHODOLOGY">2. METHODOLOGY</h2>
<h3 id="2-1-Ontology-Transformation">2.1 Ontology Transformation</h3>
<p>针对下游任务的差异，对不同的任务利用不同的本体来源进行本体转换。首先从外部知识图中提取每个实例的本体，然后将这些本体转换为原始文本作为辅助提示，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt2.png" alt="图2"></p>
<h3 id="2-2-Span-sensitive-Knowledge-Injection">2.2 Span-sensitive Knowledge Injection</h3>
<p>给定输入文本 $X_{in} = [x_1, x_2, …, x_L]$ 和 $L$ 个标记，作者使用可视化的矩阵来限制对输入文本的知识注入，即。在语言模型架构中，在softmax层之前添加了具​​有自注意力权重的注意力掩码矩阵。因此，作者将注意力掩模矩阵M修改如下：</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt4.png" alt="图3"></p>
<p>其中，$x$为输入文本，$x^o$为本体文本。当 $M_{ij} = -\infty$ 时，表示$token_i$ 被阻止关注$token_j$，当$M_{ij} = 0$ 时，表示$token_i$ 可以关注$token_j$。$p_k$ 表示输入文本中提到的跨度（例如，关系提取和知识图补全中的实体、触发器或事件提取中的参数）的位置。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt3.png" alt="图4"></p>
<h3 id="2-3-Collective-Training">2.3 Collective Training</h3>
<p>首先，利用真实的词嵌入初始化本体标记，并在固定的语言模型下优化这些本体标记。其次，优化模型的所有参数，包括语言模型和本体标记。</p>
<h2 id="3-EXPERIMENTS">3. EXPERIMENTS</h2>
<ul>
<li>
<p>数据集<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt5.png" alt="图5"></p>
</li>
<li>
<p>关系抽取<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt6.png" alt="图6"></p>
</li>
<li>
<p>知识图谱构建<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPromp7.png" alt="图7"></p>
</li>
<li>
<p>事件提取 + 消融实验<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt8.png" alt="图8"></p>
</li>
<li>
<p>Case Study<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/OntoPrompt9.png" alt="图9"></p>
</li>
</ul>
<h2 id="4-参考文献">4. 参考文献</h2>
<p>Ye H, Zhang N, Deng S, et al. Ontology-enhanced Prompt-tuning for Few-shot Learning[C]//Proceedings of the ACM Web Conference 2022. 2022: 778-787.</p>
]]></content>
      <tags>
        <tag>知识图谱</tag>
        <tag>大模型</tag>
        <tag>文献笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models</title>
    <url>/2024/07/20/ChatKBQA-A-Generate-then-Retrieve-Framework-for-Knowledge-Base-Question-Answering-with-Fine-tuned-Large-Language-Models/</url>
    <content><![CDATA[<blockquote>
<p>第一作者：Haoran Luo, Haihong E</p>
<p>作者单位：北京邮电大学</p>
<p>发表时间：2024/5</p>
<p>发表期刊：ACL 2024</p>
<p>关键内容：将知识图谱与大模型结合。亮点：通常是先利用知识图谱进行检索，检索结果作为prompt的一部分输入到模型中。而ChatKBQA是先利用LLM生成逻辑形式，再通过无监督方法对实体和关系进行检索替换，从而提升答案的准确率。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_3.png" alt="图1"></p>
<h2 id="1-引言">1.引言</h2>
<p>KBQA(Knowledge Base Question Answering) 主要有两个核心问题：知识检索、语义解析。</p>
<ul>
<li>知识检索(IR)：根据知识库中的问题定位最相关的实体、关系或三元组</li>
<li>语义解析(SP)：将问题从非结构化自然语言转换为结构化逻辑形式，再将其转换为可执行的图查询，以获得精确的答案和可解释的路径</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_1.png" alt="图2"><br>
retrieve-then-generate KBQA 框架：先对问题文本进行实体和关系的检索，再进行语义解析。它的不足在于：</p>
<ul>
<li>
<p>检索效率低下</p>
<p>Traditional methods first identify the span of candidate entities and then do entity retrieval and relation retrieval. Since the structure of natural language questions differs from KB facts, most approaches require training dedicated models for extraction and linking inefficiently.</p>
</li>
<li>
<p>不正确的检索结果会导致错误的语义解析</p>
<p>Previous methods have utilized retrieved triples also as input of reference to the seq2seq model along with the original question. However, since the retrieved triples are not always accurate, they adversely impact semantic parsing outcomes. Additionally, if there are numerous retrieved triples, the seq2seq model requires a much longer context length.</p>
</li>
<li>
<p>多处理步骤使KBQA成为一项极其复杂的任务</p>
<p>Previous work decomposed the KBQA task into multiple sub-tasks, forming a complex pipeline, which made reproduction and migration challenging.</p>
</li>
</ul>
<h2 id="2-方法">2. 方法</h2>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_2.png" alt="图3"></p>
<p>无监督检索，它的原理是：首先计算实体与其它实体的相似度，保留前 k 个及概率大于阈值 t 的相似实体，对实体进行替换，再保留前 k 个及概率大于阈值 t 的相似实体。对关系的检索替换操作类似。</p>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_4.png" alt="图4"></p>
<h2 id="3-实验">3.实验</h2>
<p>从以下几个角度开展实验：</p>
<ul>
<li>
<p>Does ChatKBQA outperform other KBQA methods?</p>
<p>在WebQSP与CWQ两个数据集上进行测试，将ChatKBQA模型与其它模型的效果进行对比。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_5.png" alt="图5"></p>
</li>
<li>
<p>Does the main components of ChatKBQA work?</p>
<p>消融实验，如下图(a)(b)所示<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_6.png" alt="图6"></p>
</li>
<li>
<p>Why use Generate-then-Retrieve method instead of Retrieve-then-Generate method?<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_7.png" alt="图7"><br>
Topk指标：前k个预测结果中有一个为正确即为正确。结果分析：检索得到的信息会有错误的干扰信息和增加指令的Max Token，这导致LLMs对原始问题的灾难性遗忘，增加了训练的难度。</p>
</li>
<li>
<p>Why use fine-tuned open-source LLMs instead of calling ChatGPT or training traditional T5 models?</p>
<p>结果如上图©所示。</p>
</li>
<li>
<p>Does Generate-then-Retrieve method improve retrieval efficiency?</p>
<p>为了体现&quot;Generate-then-Retrieve&quot;方法对检索效率的提升，将逻辑形式生成后的实体检索( ER )和关系检索( RR )与传统的自然语言问句检索( NL-R )进行比较。我们将检索的效率定义为待检索文本与检索答案集合之间的平均相似度范围[0,1]，通过不同的检索模型对其进行评分。结果如上图(d)所示。</p>
</li>
<li>
<p>Does ChatKBQA has plug-and-play characteristics?<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/ChatKBQA_8.png" alt="图8"><br>
说明ChatKBQA框架可以适用于多种 基座模型+微调方法+检索方法 的组合搭配。</p>
</li>
</ul>
<h2 id="4-参考文献">4. 参考文献</h2>
<p>Luo H, Tang Z, Peng S, et al. Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models[J]. arXiv preprint arXiv:2310.08975, 2023.</p>
]]></content>
      <tags>
        <tag>知识图谱</tag>
        <tag>大模型</tag>
        <tag>文献笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</title>
    <url>/2024/07/26/Agent-Pro-Learning-to-Evolve-via-Policy-Level-Reflection-and-Optimization/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>Augmenting large language models with chemistry tools</title>
    <url>/2024/07/26/Augmenting-large-language-models-with-chemistry-tools/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>REASONING ON GRAPHS: FAITHFUL AND INTERPRETABLE LARGE LANGUAGE MODEL REASONING</title>
    <url>/2024/07/26/REASONING-ON-GRAPHS-FAITHFUL-AND-INTERPRETABLE-LARGE-LANGUAGE-MODEL-REASONING/</url>
    <content><![CDATA[<blockquote>
<p>第一作者：Linhao Luo</p>
<p>作者单位：Monash University</p>
<p>发表时间：2024/2</p>
<p>发表期刊：ICLR 2024</p>
<p>关键内容：先利用LLM 生成与问题回答 有关的关系路径，再到KGs中进行检索（实体来自问题句子，关系来自LLM生成的关系），再利用检索到的推理路径进行模型推理。亮点在于：知识图谱中的实体会动态变化，而实体间的关系是较为稳定的，通过对KG中关系路径的检索得到可靠的知识作为模型的上下文输入。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/RoG1.png" alt="图1"></p>
<h2 id="1-引言">1.引言</h2>
<p>先前使用 KGs 和 LLMs 进行推理的方法有以下两种：</p>
<ul>
<li>语义解析，通过 LLMs 将问题转化为逻辑查询，在KGs中进行查询，与<a href="https://lwl1751.github.io/2024/07/20/ChatKBQA-A-Generate-then-Retrieve-Framework-for-Knowledge-Base-Question-Answering-with-Fine-tuned-Large-Language-Models/">ChatKBQA</a>类似。这种方法的缺点在于生成的逻辑查询不一定是可执行的。（因此，ChatKBQA会对逻辑查询中的实体/关系进行无监督检索替换，以确保逻辑查询的可执行性）</li>
<li>RAG。将KGs作为LLM推理的事实知识库，没有充分利用KGs结构信息。For instance, as shown in Figure 1, a relation path, which is a sequence of relations, “child of -&gt; has son” can be used to obtain answers to the question “Who is the brother of Justin Bieber?”.</li>
</ul>
<p>因此，不同于ChatKBQA中利用LLM生成逻辑查询，RoG是先通过LLM生成关系路径，再对KGs进行检索，最后将检索得到的推理路径作为上下文输入到LLM中进行推理。</p>
<h2 id="2-实验">2.实验</h2>
<ul>
<li>
<p>RoG与其它模型比较，其中，type 为LLM的模型并没有进行微调，同时为zero shot inference。RoG的基座模型为 LLaMA2-Chat-7B，微调数据集为WebQSP、CWQ、Freebase。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/RoG2.png" alt="图2"></p>
</li>
<li>
<p>消融实验。w/ random plan，从KGs中随机检索推理路径，并将其输入到推理模块中；w/ vote reasoning，采用多数投票，从检索到的推理路径中选择前5个答案。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/RoG4.png" alt="图3"></p>
<ul>
<li>
<p>探究超参数——关系路径数量的影响。关键路径数量越多，召回率越高，但也引入更多噪声，精确度下降。<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/RoG3.png" alt="图4"></p>
</li>
<li>
<p>PLUG-AND-PLAY ROG PLANNING MODULE 的验证<br>
<img src="https://raw.githubusercontent.com/lwl1751/Image_Hosting/main/img/RoG5.png" alt="图5"></p>
</li>
</ul>
<h2 id="3-参考文献">3. 参考文献</h2>
<p>Luo L, Li Y F, Haffari G, et al. Reasoning on graphs: Faithful and interpretable large language model reasoning[J]. arXiv preprint arXiv:2310.01061, 2023.</p>
]]></content>
      <tags>
        <tag>知识图谱</tag>
        <tag>大模型</tag>
        <tag>文献笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research</title>
    <url>/2024/07/26/HeCiX-Integrating-Knowledge-Graphs-and-Large-Language-Models-for-Biomedical-Research/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models</title>
    <url>/2024/07/26/UniMEL-A-Unified-Framework-for-Multimodal-Entity-Linkingwith-Large-Language-Models/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</title>
    <url>/2024/07/26/Self-Distillation-Bridges-Distribution-Gap-in-Language-Model-Fine-Tuning/</url>
    <content><![CDATA[
]]></content>
  </entry>
</search>
