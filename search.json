[{"title":"多模态知识图谱应用","path":"/2024/05/15/多模态知识图谱应用/","content":"相比于单模态知识图谱，多模态知识图谱能够综合多种类型的数据，从而可以让智能体更深入的感知和理解真实的数据场景，因而多模态知识图谱在各个领域都有广泛的应用。如图像检索、模型推理与生成、模型预训练等。以电子商务为例，通过多模态产品图谱，可以对产品进行更细致的表示，再通过预训练，可以增强大型模型对电子商务领域的多模态知识理解，从而推动电子商务平台的发展。 AliMe MKG是阿里提出的一种面向直播的知识图谱，与传统的知识图谱不同，它的目标是向顾客种草某一产品，而非解决顾客的问题。因此，它需要构建逻辑思维链，引导用户需求。例如，在该左图1的知识图谱示例中，”熬夜”导致”皮肤暗沉”问题，这就需要”皮肤白皙”，而含有”甘草酸二钾”成分的”面膜”产品适合相应的用户。 在电商直播领域，这种知识图谱有两种应用：智能辅播和虚拟主播。智能辅播是在真人直播间构造了一个智能助理机器人，来协助主播去做商品介绍。比如说用户问的是口红，直播间内有多个口红，智能辅播就会将相关的信息展示出来给用户进行浏览，当用户点击确认，选择一个感兴趣的口红之后，辅播就会从知识图谱中抽取相应信息，以商品卡片信息的方式让用户和图片进行交互。除此之外，智能辅播还可以回答用户丰富的产品相关问题。比如用户问尺码的时候，辅播可以去推出文本介绍和对应的尺码图，用文本及图片来回答用户的咨询。 虚拟主播则是一个智能的直播间虚拟人，通过自动生成图文剧本介绍商品，生成具有吸引力和认知的知识型短视频，从而可以影响客户的购买决策。因此，多模态的知识图谱可以促进电商的发展。 另一个是医疗诊断的案例，这是一个基于多模态知识图谱的医疗健康问答系统示例。它首先利用多种方法获取用户提交数据的关键信息，并确定用户查询的主题意图，建立用户的知识需求模型。在知识匹配阶段，我们计算用户需求与医疗健康知识的相关度，并消除可能的歧义，最终向用户提供匹配度高的医疗健康知识。 参考资料: Chen Z, Zhang Y, Fang Y, et al. Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey[J]. arXiv preprint arXiv:2402.05391, 2024. Xu G, Chen H, Li F L, et al. Alime mkg: A multi-modal knowledge graph for live-streaming e-commerce[C]&#x2F;&#x2F;Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. 2021: 4808-4812. https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g 韩普,叶东宇,陈文祺,等.面向多模态医疗健康数据的知识组织模式研究[J].现代情报,2023,43(10):27-34+151.","tags":["知识工程","多模态","知识图谱"]},{"title":"强化学习基础知识","path":"/2024/05/15/强化学习基础知识/","content":"知识点：马尔科夫决策过程，动态规划。 马尔科夫决策过程 马尔科夫过程: 一个具备马尔科夫性质的离散随机过程。 马尔科夫性: 下一时刻的状态只与当前状态有关。即$P[S_{t+1}|S_{1},…,S_{t}]=P[S_{t+1}|S_{t}]$ 马尔科夫奖励函数: 把马尔科夫过程从 &lt;S, P&gt; 拓展到 &lt;S, P, R, γ&gt;。其中P 为状态转移矩阵，R 和 γ 分别表示奖励函数和奖励折扣因子。折扣因子越大，代表了智能体对长期性能指标考虑的程度越高（远视）；折扣因子越小，代表了智能体对长期性能指标考虑的程度越低（近视）。 回报: 回报是一个轨迹的累积奖励，$G_{t} = R_{t+1} + \\gamma R_{t+2} = \\sum_{k=0}^{\\infty } \\gamma ^{k}R_{t+k+1}$ 价值函数：状态s的期望回报，$V(s) = E[G_{t}|S_{t}=s]$。 马尔科夫决策过程: 马尔可夫奖励过程的立即奖励只取决于状态(奖励值在节点上)，而马尔可夫决策过程的立即奖励与状态和动作都有关。即把马尔科夫过程从 &lt;S, P, R, γ&gt; 拓展到 &lt;S, A, P, R, γ&gt;。A是有限动作的集合。 动作价值函数: 依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。$Q(s,a) = E[G_{t}|S_{t}=s, A_{t}=a]$。易知$V(s)=E_{a}[Q(s,a)]$。 策略: 状态到行为的映射。 对于任何马尔科夫决策过程： 总是存在一个最优策略$\\pi^*$，比任何其他策略更好或至少相等。 所有的最优策略有相同且最优的价值。 所有的最优策略具有相同且最优的动作价值。 贝尔曼方程：用于计算给定策略 π 时价值函数在策略指引下所采轨迹上的期望。 最优价值函数: 即使是在相同的状态和动作集合上，不同的策略也将会带来不同的价值函数。定义最优价值函数为 $$v_*(s) = \\max_{π} v_π(s), ∀s ∈ S$$ 最优动作价值函数： $$q_*(s,a) = \\max_{\\pi} q_π(s,a), ∀s ∈ S, a ∈ A$$ 则 $$v*(s) = \\max_{a\\sim \\mathbf{A}} q_*(s, a)$$ $$q_(s, a) = E[R_t + γv_(S_{t+1}) | S_t = s, A_t = a]$$ 逆矩阵法求解贝尔曼方程： $$\\mathbf{v} = \\mathbf{r} + γP\\mathbf{v} $$ 其中 v 和 r 矢量，P是状态转移概率矩阵。求解如下： $$\\mathbf{v} = (I − γP )^{-1}\\mathbf{r}$$ 复杂度为$O(n^{3})$，考虑其他方法进行求解，如动态规划、蒙特卡洛估计、时序差分法等。 动态规划 用动态规划算法在 能够获取MDP完整的环境信息（包括状态动作空间、转移矩阵、奖励等）的基础上 求解最优策略。 预测：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;和策略𝜋，输出基于当前策略𝜋的价值函数v。 控制：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;，输出最优价值函数𝑣∗以及最优策略𝜋∗ 迭代策略评估： 预测问题，评估一个给定的策略$\\pi$。 策略迭代： 策略评估，在当前策略𝜋上迭代地计算𝑣值 策略更新，根据𝑣值贪婪地更新策略 如此反复多次，最终得到最优策略𝜋∗和最优状态价值函数𝑣∗ 价值迭代： 参考资料： 中国科学院大学 林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)","tags":["强化学习"]},{"title":"基于策略梯度的强化学习","path":"/2024/05/15/基于策略梯度的强化学习/","content":"强化学习可分为两大类： value-based method(DP,MC,TD) 通过价值函数求解最优策略，求解出来的策略是确定性的，虽然可以通过$\\epsilon$-贪心策略来获取一定的随机性。要求动作空间离散。 policy-based method 适用场景：随机策略；动作空间连续。 优点：具有更好的收敛性质。 缺点：通常会收敛到局部最优而非全局最优；评估一个策略通常不够高效并且具有较大的方差。 1.基本原理 由于策略实际上是一个概率分布，可以将策略参数化 $\\pi(a|s,\\theta)$ ，其中$\\theta$ 是策略的参数。通过这种方式，可以将可见的已知状态泛化到未知的状态上。 1.1 策略目标函数 在片段式的环境中，使用每个经历片段(episode)的平均总回报。在连续性的环境中，使用每一步的平均奖励。 希望能够找到最大化$J(\\theta)$的$\\theta$，属于最优化问题，求解方法如下： 不使用梯度的方法(Hill climbing, Simplex, 模拟退火, 遗传算法) 使用梯度的方法更高效(梯度下降, 共轭梯度, 拟牛顿法) 1.2 策略函数 softmax策略，离散型动作空间 高斯策略策略，连续型动作空间 线性函数策略，连续型动作空间，表示给定状态下确定性的动作，$a=\\pi(s,\\theta)$ 1.3 单步马尔可夫决策过程 从一个分布d(s)中采样得到一个状态s，从s开始，按照策略𝜋采取一个行为a，得到即时奖励$r=R_{s,a}$。由于是单步过程，目标函数为 2.策略梯度定理 由于状态转移函数的存在，虽然训练用的轨迹都是由同一个策略生成的，但其两两差异仍十分显著，并且显然轨迹越长差异越大，决策中每一个微小的差异累积起来都会导致最后结果的极大差异。也就是数据有着较大的方差，这会导致使用均值计算期望的效果变差，并使算法难以收敛。 改进方法：使用时序因果关系；加入基线。 3.蒙特卡洛策略梯度(REINFORCE) 4. Actor-Critic 算法(A3C) actor --&gt; policy network，决定采取哪个动作 $\\pi(a|s;\\theta)$ input: state s output: probability distribution over the actions 训练目标： 增加状态值函数 state-value critic --&gt; value network，只负责评估动作的好坏 $q(s,a;w)$ input: state s and action a output: approximate action-value(scalar) 训练目标： 使价值评估的更精准，接近于实际环境的return 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.) https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d","tags":["强化学习"]},{"title":"无模型强化学习","path":"/2024/05/15/无模型强化学习/","content":"知识点 1.无模型价值学习评估 蒙特卡洛方法 时序差分学习 TD(𝝀) 2.无模型策略优化控制 蒙特卡洛策略迭代 时序差分策略迭代（SARSA） Q值迭代 (Q-learning) 1. 无模型价值学习评估 1.1 蒙特卡洛方法 蒙特卡洛方法是一种基于样本的方法，不需要知道环境的所有信息。只需基于过去的经验就可以学习。具体来说，给定一个策略 π，通过对 π 产生的回报取平均值来评估状态价值函数。这样就有两种估算方式: 首次蒙特卡罗(First-Visit Monte Carlo)和每次蒙特卡罗(Every-Visit Monte Carlo)。首次蒙特卡罗只考虑每一个回合中第一次到状态 s 的访问，而每次蒙特卡罗就是考虑每次到状态 s 的访问。 注意的是，和动态规划不同的是，蒙特卡罗不使用自举(Bootstrapping)，也就是说，它不用其他状态的估算来估算当前的状态值。 离线学习：智能体从预先收集好的数据中进行学习。 在线学习：智能体通过与环境实时交互来获取知识和经验。 1.2 时序差分学习 时序差分学习方法同蒙特卡洛方法一样是不基于模型的，不需要马尔可夫决策过程的知识。但是时序差分学习方法可以直接从经历的不完整经历片段中学习，它通过**自举(bootstrap)**猜测经历片段的结果并不断更新猜测。即时序差分学习方法可以在每一次经历的过程中进行学习，而蒙特卡洛方法只能等到每次经历完全结束时才能进行学习。 $$𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝐺_t − 𝑉(𝑆_{𝑡}))$$ 对TD(0)，即one-step TD: $$𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝑅_{𝑡+1} + 𝛾𝑉(𝑆_{t+1}) − 𝑉(𝑆_{𝑡}))$$ 这个算法又被叫做SARSA，因为用到了 $(S_t, A_{𝑡}, R_{𝑡+1}, S_{𝑡+1}, A_{𝑡+1})$。 蒙特卡洛方法没有偏倚，是对当前状态实际价值的无偏估计，但有着较高的变异性，且对初始值不敏感。 时序差分方法方差更低, 但有一定程度的偏差，对初始值较敏感，通常比蒙特卡洛方法更高效。 1.3 TD(𝝀) 2.无模型策略优化控制 2.1 蒙特卡洛策略迭代 2.2 时序差分策略迭代（SARSA） $$G_{t:t+n} = R_{t+1} + γR_{t+2} + \\dot + γ^{n−1}R_{t+n} + γ^nQ_{t+n−1}(S_{t+n}, A_{t+n})$$ 2.3 Q值迭代 (Q-learning) Sarsa --&gt; on-policy Q-learning --&gt; off-policy 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) 强化学习入门——从原理到实践，叶强 Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)","tags":["强化学习"]},{"title":"how to build up a blog","path":"/2024/05/12/how-to-build-up-a-blog/","content":"登录root用户 1su root 切换路径 1cd /Users/lwl/Blog 博客文章发布 12345hexo new &quot;title&quot; # 新建文章# 编辑对应的markdown文件hexo g # 渲染md文件为博客页面hexo s # 执行后打开http://localhost:4000/预览hexo d # 预览并编辑无误后再部署，也可以直接部署 有时部署会失败，此时尝试下面命令清除缓存后再执行部署命令。 1hexo clean 更换端口 1hexo s -p 8888 # 示例 释放端口 12lsof -i :端口号 # 查找占用指定端口的进程ID（PID）kill -9 进程ID"}]