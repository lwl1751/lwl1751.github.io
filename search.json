[{"title":"大模型知识分析、萃取与增强","path":"/2024/05/25/大模型知识分析、萃取与增强/","content":"大模型中蕴含着大量的知识，但是知识的类型、数量和质量并不可控。 知识分析实验表明，大模型自发学到了一些世界知识、常识知识，这些知识隐式地存储于模型参数中。 课程考试复习使用。 1.大模型的知识分析 1.1 知识探测 知识探测：探测预训练语言模型掌握的知识。 实现方式：将三元组或问答对形式的世界知识转化为自然语言填空的形式，从而判断语言模型掌握知识的准确性。 预训练模型知识探测的良好性能主要来源于： 提示语偏差，预测结果会受到提示词的影响，如 was born in [Mask] , 模型会猜测下一个词应该为地名。 类别指导，类似于few shot learning，模型已经见过类似的问题。 答案泄漏，基于上下文的推理。 1.2 知识定位 知识定位：分析预训练语言模型中的知识存储机制，可分为层粒度与神经元粒度。 大量事实知识存储在FNN模块中。 1.3 知识学习机理分析 分析影响预训练语言模型学习效果的因素。 长尾知识：信息出现次数非常少，甚至只出现了一次。LLM对长尾知识的掌握并不充分，回答问题的准确度就会降低，可以通过扩大模型规模(scaling low)、检索增强来解决该问题。 共现频率：LLM更加倾向于预测共现频率更高的答案。如，对于问题“加拿大的首都是？”，在预训练语料中，（加拿大，多伦多）共同出现的频率要大于（加拿大，渥太华）出现的频率，于是模型倾向于输出共现频率更高的“多伦多”，而不是正确答案“渥太华”。 逆转诅咒，模型很难逆转思考，如 A is B 不能推出 B is A. 2.大模型的知识萃取 知识萃取是指利用特定方式诱导大模型，从中萃取出有用的显式符号化知识。 3.大模型的知识增强 幻觉可以分为事实性现象（生成的内容不忠于既定的事实知识）和忠实性幻觉（生成的内容前后冲突）。 幻觉消除：清洗训练数据，解码方式改进，指令数据优化，外部知识增强。 知识增强：RAG，Fine-tuning。 关于幻觉的知识，可以查看文献综述：《Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity》 3.1 RAG 检索知识源 文档：检索粒度粗，知识覆盖度高，存在冗余信息。 知识图谱：提供丰富的结构化信息，受限于图谱的覆盖度。 检索方式 稀疏检索：简单词汇匹配，缺少上下文理解能力（BM25） 稠密检索：将问题和文档编码为稠密向量，计算点积作为相似度（DPR） 生成检索：直接使用大模型为问题生成相关文档，而不需要检索库 检索时间 推理前检索一次：效率高，但相关度低 推理过程中自适应地检索：平衡知识和效率，但是难以判断模型何时需要知识 推理过程中每隔N个token检索一次：效率低，信息量大 利用检索得到的知识进行推理 输入增强：使用简单，受限于上下文长度 中间增强：需要重新训练模型，支持处理更多文档 输出增强：对输出进行后修改，需要两次推理模型：第一次，模型直接输出答案。第二次，根据问题和答案，进行检索，对输出答案进行修改。 知识拉锯战：由于错误信息，观点不同，以及知识进化的本质，知识冲突问题广泛存在于检索增强语言模型中。 知识冲突形式： 模型内部参数化知识和外部非参数化知识之间存在冲突。其中，使用外部知识回答的模型作为专家模型，依靠内部知识回答的模型作为业余模型。 非参数化知识中真实、虚假以及无关证据之间存在冲突。其中，通过指令微调，使用真实证据回答的模型作为专家模型，使用虚假证据回答的模型作为业余模型。 Dunning-Kruger现象：人对于某些欠缺的能力反而会过度自信，对模型也同样适用。 4.大模型的工具增强 工具增强：让模型学会使用外部工具，以补充模型相关知识。 相关论文： Timo Schick, Toolformer: Language Models Can Teach Themselves to Use Tools, NeurIPS 2023。 Yujia Qin, ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs, ICLR2024。 5.参考资料 中国科学院大学赵军老师 知识工程课程课件","tags":["知识工程","大模型"]},{"title":"多模态知识图谱应用","path":"/2024/05/15/多模态知识图谱应用/","content":"相比于单模态知识图谱，多模态知识图谱能够综合多种类型的数据，从而可以让智能体更深入的感知和理解真实的数据场景，因而多模态知识图谱在各个领域都有广泛的应用。如图像检索、模型推理与生成、模型预训练等。以电子商务为例，通过多模态产品图谱，可以对产品进行更细致的表示，再通过预训练，可以增强大型模型对电子商务领域的多模态知识理解，从而推动电子商务平台的发展。 AliMe MKG是阿里提出的一种面向直播的知识图谱，与传统的知识图谱不同，它的目标是向顾客种草某一产品，而非解决顾客的问题。因此，它需要构建逻辑思维链，引导用户需求。例如，在该左图1的知识图谱示例中，”熬夜”导致”皮肤暗沉”问题，这就需要”皮肤白皙”，而含有”甘草酸二钾”成分的”面膜”产品适合相应的用户。 在电商直播领域，这种知识图谱有两种应用：智能辅播和虚拟主播。智能辅播是在真人直播间构造了一个智能助理机器人，来协助主播去做商品介绍。比如说用户问的是口红，直播间内有多个口红，智能辅播就会将相关的信息展示出来给用户进行浏览，当用户点击确认，选择一个感兴趣的口红之后，辅播就会从知识图谱中抽取相应信息，以商品卡片信息的方式让用户和图片进行交互。除此之外，智能辅播还可以回答用户丰富的产品相关问题。比如用户问尺码的时候，辅播可以去推出文本介绍和对应的尺码图，用文本及图片来回答用户的咨询。 虚拟主播则是一个智能的直播间虚拟人，通过自动生成图文剧本介绍商品，生成具有吸引力和认知的知识型短视频，从而可以影响客户的购买决策。因此，多模态的知识图谱可以促进电商的发展。 另一个是医疗诊断的案例，这是一个基于多模态知识图谱的医疗健康问答系统示例。它首先利用多种方法获取用户提交数据的关键信息，并确定用户查询的主题意图，建立用户的知识需求模型。在知识匹配阶段，我们计算用户需求与医疗健康知识的相关度，并消除可能的歧义，最终向用户提供匹配度高的医疗健康知识。 参考资料: Chen Z, Zhang Y, Fang Y, et al. Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey[J]. arXiv preprint arXiv:2402.05391, 2024. Xu G, Chen H, Li F L, et al. Alime mkg: A multi-modal knowledge graph for live-streaming e-commerce[C]&#x2F;&#x2F;Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. 2021: 4808-4812. https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g 韩普,叶东宇,陈文祺,等.面向多模态医疗健康数据的知识组织模式研究[J].现代情报,2023,43(10):27-34+151.","tags":["知识工程","多模态","知识图谱"]},{"title":"知识获取","path":"/2024/05/15/知识获取/","content":"信息抽取：从自然语言文本中抽取指定类型的实体、关系、事件等事实信息，并形成结构化数据输出的文本处理技术。 1.命名实体识别 1.1 基于词典的方法 典型方法包括正向匹配方法，反向匹配方法。原理：按照一定的策略将待分析的汉字串与一个充分大的词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。 1.2 基于统计的方法 生成式方法，首先建立学习样本的生成模型，再利用模型对预测结果进行间接推理，如HMM。 判别式方法，基于由字构词的命名实体识别理念，将NER问题转化为判别式分类问题(序列标注问题)，如Maxent，SVM，CRF，CNN，RNN，LSTM+CRF。 1.3 基于阅读理解的方法 1.4 基于模板生成的方法 1.5 基于大模型的方法 难点1: 任务形式差距。命名实体识别通常建模为序列标注任务，而大模型往往用于完成文本 生成任务。难点2: 大模型存在较为严重的幻觉问题。 2.关系知识抽取 3.事件知识抽取 4.脚本知识抽取 5.多粒度知识联合抽取 6.参考资料 中国科学院大学赵军老师 知识工程课程课件","tags":["知识工程"]},{"title":"搜索","path":"/2024/05/15/搜索/","content":"知识点： 经典搜索算法：基于路径的搜索的问题。如盲目搜索，启发式搜索。 局部搜索算法：最优化问题，没有初始状态，也没有终止状；并不需要到达这些解的路径。如爬山法，元启发算法。 元启发式算法是启发式算法的改进，它是随机算法与局部搜索算法相结合的产物。如禁忌搜索算法(Tabu Search)，模拟退火算法(Simulated annealing)，遗传算法(Geneticalgorithm)。 对抗搜索算法：也被称为博弈搜索，在一个竞争的环境中，智能体(agents)之间通过竞争实现相反的利益，一方最大化利益，另外一方最小化这个利益。如mini-max算法，Alpha-Beta算法、蒙特卡洛树。 1.搜索算法基础 树搜索： 结点：n.state, n.parent, n.action(父节点生成该节点时所采取的行动), n.path-cost(从初始状态到达该结点的路径消耗g(n)) 搜索策略：节点扩展到顺序 策略评价标注：完备性，时间复杂度，空间复杂度，最优性 复杂度表示：分子因子b，搜索树的中节点的最大分支树；深度d，目标结点所在的最浅深度；m，状态空间中，任何路径的最大长度 图搜索： 边缘(fringe)：待扩展的叶子结点，将状态空间分成已探索区域和未被探索区域。 2.盲目搜索 只能使用访问过的结点的信息。如宽度优先搜索，深度优先搜索，一致代价搜索（扩展路径消耗g(n)最小的结点），深度受限搜索（对深度优先搜索设置最大深度的界限l），迭代加深的深度优先搜索（不断增大深度限制，并且每次重新开始深度受限搜索）。 3.启发式搜索 优先扩展最优的结点。评价函数：f = g + h。其中，g为一致代价，h为启发式函数，指的是结点n到目标结点的最小代价路径的代价估计值。 贪婪算法：扩展离目标最近的结点，以期望很快找到解。f(n)=h(n) A* 算法：避免扩展代价已经很高的路径。f(n) = g(n) + h(n) A* 算法: 可采纳启发式：永远不会高估代价，即$h(n)\\le h^*(n)$，且要求$h(n)\\ge 0$。 定理：如果h(n)是可采纳的，A*的树搜索版本是最优的。 一致的启发式：对于每个结点n和通过任一行动a生成的后继结点n’，从结点n到达目标的估计代价不大于从n到n’的单步代价与从n到目标的估计代价之和，即$h(n)\\le c(n,a,n’) + h(n’)$。如果h(n)是一致的，那么沿着任何路径的f(n)是非递减的。 定理：如果h(n)是一致的，那么A*的图搜索版本是最优的。 松弛问题：对原问题移除一些限制，一个松弛问题的最优解的代价是原问题的一个可采纳、一致的启发式。 松弛问题的最优解的代价不大于原问题最优解的代价。 4.局部搜索 局部搜索：找到满足条件的状态，不关心路径，从单个当前节点(而不是多条路径)出发，通常只移动到它的邻近状态。 4.1 爬山法 属于贪婪法，不断向值增加的方向移动，容易到达局部极大值。为了克服局部极大值，可以采用随机重启爬山法，完备的概率接近1。 4.2 禁忌搜索算法 从一个初始可行解出发，选择一系列的特定搜索方向（移动）作为试探，选择实现让特定的目标函数值变化最多的移动。 为了避免陷入局部最优解，TS搜索中采用了一种灵活的“记忆”技术，对已经进行的优化过程进行记录和选择，指导下一步的搜索方向，这就是Tabu表的建立。 标记已经解得的局部最优解或求解过程，并在进一步的迭代中避开这些局部最优解或求解过程。局部搜索的缺点在于，太过于对某一局部区域以及其邻域的搜索，导致一叶障目。为了找到全局最优解，禁忌搜索就是对于找到的一部分局部最优解，有意识地避开它，从而或得更多的搜索区域。 4.3 模拟退火算法 算法概述： 若目标函数f在第i+1步移动后比第i步更优，即$f(Y(i+1))\\le f(Y(i))$，则总是接受该移动。 若$f(Y(i+1))&gt;f(Y(i))$，（即移动后的解比当前解要差），则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）。 Metroplis准则：温度越高，算法接受新解的概率就越高。 4.4 遗传算法 基本思想：从初始种群出发，采用优胜劣汰、适者生存的自然法则选择个体，并通过杂交、变异来产生新一代种群，如此逐代进化，直到满足目标为止。 种群：组候选解的集合，遗传算法正是通过种群的迭代进化，实现了最优解或者近似最优解。 个体：一个个体对应一个解，也就是构成种群的基本单元。 适应度函数:用来对种群中各个个体的环境适应性进行度量的函数，函数值是遗传算法实现优胜劣汰的主要依据。 遗传操作：作用于种群而产生新的种群的操作。如选择、交叉、变异。 遗传编码： 二进制编码 格雷编码，要求两个连续整数的编码之间只能有一个码位不同，其余码位都是完全相同的。 符号编码，个体染色体编码串中的基因值取自一个无数值含义、而只有代码含义的符号集 适应度函数： 原始适应度函数，直接将待求解问题的目标函数f(x)定义为遗传算法的适应度函数。它能够直接反映出待求解问题的最初求解目标但是有可能出现适应度值为负的情况。 标准适应度函数。在遗传算法中，一般要求适应度函数非负，并其适应度值越大越好。这就往往需要对原始适应函数进行某种变换，将其转换为标准的度量方式，以满足进化操作的要求，这样所得到的适应度函数被称为标准适应度函数$f_{Normal}(x)$ 选择：各个个体被选中的概率与其适应度大小成正比 比例选择 锦标赛选择 轮盘赌选择 交叉： 单点交叉，先在两个父代个体的编码串中随机设定一个交叉点，然后对这两个父代个体交叉点前面或后面部分的基因进行交换，并生成子代中的两个新的个体。 两点交叉，先在两个父代个体的编码串中随机设定两个交叉点，然后再按这两个交叉点进行部分基因交换，生成子代中的两个新的个体。 多点交叉，从两个父代个体中选择多个交叉点，然后交换这些交叉点之间的基因片段，从而产生新的个体。 均匀交叉，父串中的每一位都是以相同的概率随机进行交叉的。 实值交叉，在实数编码情况下所采用的交叉操作，可分为部分离散交叉、整体交叉。 部分离散交叉: 先在两个父代个体的编码向量中随机选择一部分分量， 然后对这部分分量进行交换，生成子代中的两个新的个体。 整体交叉: 对两个父代个体的编码向量中的所有分量，都以1/2的概率进行交换，从而生成子代中的两个新的个体。 洗牌交叉，打乱之后再选择交叉点，再进行复原 变异： 二进制变异，随机地产生一个变异位，0-&gt;1，1-&gt;0 实值变异，用另外一个在规定范围内的随机实数去替换原变异位置上的基因值，产生一个新的个体。 5.对抗搜索 5.1 Alpha-Beta算法 对mini-max算法的改进。剪枝本身不影响算法输出结果，但节点先后次序会影响剪枝效率。 5.2 蒙特卡洛树 参看蒙特卡洛树搜索部分。 6.参考资料 中国科学院大学李国荣老师 高级人工智能课程课件 高级算法设计与分析 启发式算法 林海老师","tags":["高级人工智能","搜索"]},{"title":"深度强化学习","path":"/2024/05/15/深度强化学习/","content":"强化学习从深度学习角度出发的挑战： 强化学习的奖励信号是有延迟的，而深度学习的输入输出是直接联系的 强化学习的序贯决策序列有很高的相关性，而深度学习的假设数据是独立同分布 强化学习的数据分布是会随着学习发生变化的，而深度学习的假设是底层分布固定的 1. DQN算法 1.1 DQN Deep Q-learning: DQN, Approximate $Q^*(s,a)$ by DQN,$Q(s,a;w)$ 经历回放(experience replay): 在每个时间步t 中，DQN先将智能体获得的经验$(S_t, A_t, R_t, S_{t+1})$存入回放缓存中，然后从该缓存中均匀采样小批样本用于 Q-Learning 更新。主要作用是解决数据的相关性和非静态分布问题。 DQN2015的改进：增加目标网络。目标网络通过使用旧参数生成 Q-Learning 目标，使目标值的产生不受最新参数的影响，从而大大减少发散和震荡的情况。 1.2 Double-DQN, DDQN Double DQN 是对 DQN 在减少过拟合方面的改进。这是由于DQN对动作值函数的max操作会引入一个正向的偏差，导致下一时刻的目标值存在过估计。 1.3 Prioritized Experience Replay, 优先经验回放 采用优先级采样达到收敛所需的更新次数相比均匀采样要小很多，这也是进行优先经验池回放的原因。 1.3.1 样本优先级: 样本优先级应满足两个条件： 优先级在数值上应该和误差绝对值成单调递增关系，这是为了满足误差绝对值较大（即优先级较大）的样本获得更大的被抽样的机会； 优先级数值应大于0，这是为了保证每一个样本都有机会被抽样，即抽样概率大于0。 优先级可以分为基于比例的样本优先级，基于排序的样本优先级。如下图所示： 1.3.2 随机优先级采样: 采样方法： 贪婪优先级采样，完全按照优先级去采样 一致随机采样，均匀采样 随机优先级采样，随机采样 基本原则： 样本被采样的概率应该和样本优先级成正相关关系 每一个样本都应该有机会被采样，即被采样的概率大于0 Sum-Tree随机优先级采样，属于基于比例的样本优先级： 重要性采样 1.4 Dueling-DQN 算法原理：将动作值的计算分解成状态值和优势函数，$Q(s,a)=V(s)+A(s,a)$。 2. 策略梯度方法DDPG 2.1 DPG, (Deterministic Policy Gradient) 确定性策略梯度 确定性策略：每一步的动作都是确定的，即$a=\\mu_\\theta(s)$。确定性策略梯度算法正是使用了确定性策略的策略梯度算法。 2.2 DDPG, (Deep Deterministic Policy Gradient) 几个trick： $\\mu^{\\prime}(s_t)=\\mu(s_t|\\theta_{t}^{\\mu} + N)$ 添加了一个随迭代次数衰减的随机噪声，增加了动作空间的探索 目标网络缓慢更新保证了训练的稳定性 batch normalization 使得可以在不同的环境中获取的特征统一 问题：值函数过估计；自举造成的偏差传播 2.3 PPO 2.4 SAC, (Soft Actoe Critic) 熵：随机变量取各值时信息量的期望。 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.) https://github.com/QiangLong2017/Deep-Reiforcement-Learning","tags":["强化学习"]},{"title":"连续状态系统基于模型的强化学习","path":"/2024/05/15/连续状态系统基于模型的强化学习/","content":"对于大规模的MDP问题，不希望使用查表（Table Lookup）的方式，通过函数近似来估计实际的价值函数的方式，既节约了资源，又能达到泛化的效果。 $\\hat{v}(s,w) = v_\\pi (s)$ $\\hat{q}(s,a,w) = q_\\pi (s,a)$ $\\hat{\\pi}(a,s,w) = \\pi (a|s)$ 函数近似器 特征的线性组合 神经网络 决策树 最近邻方法 傅立叶/小波变换 1. 价值函数近似, Value Fuction Approximation, VFA 近似函数逼近的类型： input: s, output: $\\hat{v}(s,w)$ input: s, output: $\\hat{q}(s,a,w)$ input: s, output: $\\hat{q}(s,a_1,w),\\dots,\\hat{q}(s,a_m,w)$ 1.1 线性函数近似 近似价值函数: $\\hat{v}(s,w)=x(s)^Tw$ 目标函数: 均方误差。由于实际的价值函数不可知，用样本近似期望损失。 1.2 神经网络值函数近似 参看 深度强化学习 部分。 1.3 基于模型的近似值迭代算法 1.4 模型无关的近似值迭代算法 2. 近似策略迭代 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.) https://www.bilibili.com/video/BV11V411f7bi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d","tags":["强化学习"]},{"title":"强化学习基础知识","path":"/2024/05/15/强化学习基础知识/","content":"知识点：马尔科夫决策过程，动态规划。 马尔科夫决策过程 马尔科夫过程: 一个具备马尔科夫性质的离散随机过程。 马尔科夫性: 下一时刻的状态只与当前状态有关。即$P[S_{t+1}|S_{1},…,S_{t}]=P[S_{t+1}|S_{t}]$ 马尔科夫奖励函数: 把马尔科夫过程从 &lt;S, P&gt; 拓展到 &lt;S, P, R, γ&gt;。其中P 为状态转移矩阵，R 和 γ 分别表示奖励函数和奖励折扣因子。折扣因子越大，代表了智能体对长期性能指标考虑的程度越高（远视）；折扣因子越小，代表了智能体对长期性能指标考虑的程度越低（近视）。 回报: 回报是一个轨迹的累积奖励，$G_{t} = R_{t+1} + \\gamma R_{t+2} = \\sum_{k=0}^{\\infty } \\gamma ^{k}R_{t+k+1}$ 价值函数：状态s的期望回报，$V(s) = E[G_{t}|S_{t}=s]$。 马尔科夫决策过程: 马尔可夫奖励过程的立即奖励只取决于状态(奖励值在节点上)，而马尔可夫决策过程的立即奖励与状态和动作都有关。即把马尔科夫过程从 &lt;S, P, R, γ&gt; 拓展到 &lt;S, A, P, R, γ&gt;。A是有限动作的集合。 动作价值函数: 依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。$Q(s,a) = E[G_{t}|S_{t}=s, A_{t}=a]$。易知$V(s)=E_{a}[Q(s,a)]$。 策略: 状态到行为的映射。 对于任何马尔科夫决策过程： 总是存在一个最优策略$\\pi^*$，比任何其他策略更好或至少相等。 所有的最优策略有相同且最优的价值。 所有的最优策略具有相同且最优的动作价值。 贝尔曼方程：用于计算给定策略 π 时价值函数在策略指引下所采轨迹上的期望。 最优价值函数: 即使是在相同的状态和动作集合上，不同的策略也将会带来不同的价值函数。定义最优价值函数为 $$v_*(s) = \\max_{π} v_π(s), ∀s ∈ S$$ 最优动作价值函数： $$q_*(s,a) = \\max_{\\pi} q_π(s,a), ∀s ∈ S, a ∈ A$$ 则 $$v*(s) = \\max_{a\\sim \\mathbf{A}} q_*(s, a)$$ $$q_(s, a) = E[R_t + γv_(S_{t+1}) | S_t = s, A_t = a]$$ 逆矩阵法求解贝尔曼方程： $$\\mathbf{v} = \\mathbf{r} + γP\\mathbf{v} $$ 其中 v 和 r 矢量，P是状态转移概率矩阵。求解如下： $$\\mathbf{v} = (I − γP )^{-1}\\mathbf{r}$$ 复杂度为$O(n^{3})$，考虑其他方法进行求解，如动态规划、蒙特卡洛估计、时序差分法等。 动态规划 用动态规划算法在 能够获取MDP完整的环境信息（包括状态动作空间、转移矩阵、奖励等）的基础上 求解最优策略。 预测：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;和策略𝜋，输出基于当前策略𝜋的价值函数v。 控制：给定一个MDP &lt;𝒮, 𝒜, 𝒫, ℛ, 𝛾&gt;，输出最优价值函数𝑣∗以及最优策略𝜋∗ 迭代策略评估： 预测问题，评估一个给定的策略$\\pi$。 策略迭代： 策略评估，在当前策略𝜋上迭代地计算𝑣值 策略更新，根据𝑣值贪婪地更新策略 如此反复多次，最终得到最优策略𝜋∗和最优状态价值函数𝑣∗ 价值迭代： 参考资料： 中国科学院大学 林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)","tags":["强化学习"]},{"title":"基于策略梯度的强化学习","path":"/2024/05/15/基于策略梯度的强化学习/","content":"强化学习可分为两大类： value-based method(DP,MC,TD) 通过价值函数求解最优策略，求解出来的策略是确定性的，虽然可以通过$\\epsilon$-贪心策略来获取一定的随机性。要求动作空间离散。 policy-based method 适用场景：随机策略；动作空间连续。 优点：具有更好的收敛性质。 缺点：通常会收敛到局部最优而非全局最优；评估一个策略通常不够高效并且具有较大的方差。 1.基本原理 由于策略实际上是一个概率分布，可以将策略参数化 $\\pi(a|s,\\theta)$ ，其中$\\theta$ 是策略的参数。通过这种方式，可以将可见的已知状态泛化到未知的状态上。 1.1 策略目标函数 在片段式的环境中，使用每个经历片段(episode)的平均总回报。在连续性的环境中，使用每一步的平均奖励。 希望能够找到最大化$J(\\theta)$的$\\theta$，属于最优化问题，求解方法如下： 不使用梯度的方法(Hill climbing, Simplex, 模拟退火, 遗传算法) 使用梯度的方法更高效(梯度下降, 共轭梯度, 拟牛顿法) 1.2 策略函数 softmax策略，离散型动作空间 高斯策略策略，连续型动作空间 线性函数策略，连续型动作空间，表示给定状态下确定性的动作，$a=\\pi(s,\\theta)$ 1.3 单步马尔可夫决策过程 从一个分布d(s)中采样得到一个状态s，从s开始，按照策略𝜋采取一个行为a，得到即时奖励$r=R_{s,a}$。由于是单步过程，目标函数为 2.策略梯度定理 由于状态转移函数的存在，虽然训练用的轨迹都是由同一个策略生成的，但其两两差异仍十分显著，并且显然轨迹越长差异越大，决策中每一个微小的差异累积起来都会导致最后结果的极大差异。也就是数据有着较大的方差，这会导致使用均值计算期望的效果变差，并使算法难以收敛。 改进方法：使用时序因果关系；加入基线。 3.蒙特卡洛策略梯度(REINFORCE) 4. Actor-Critic 算法(A2C) actor --&gt; policy network，决定采取哪个动作 $\\pi(a|s;\\theta)$ input: state s output: probability distribution over the actions 训练目标： 增加状态值函数 state-value critic --&gt; value network，只负责评估动作的好坏 $q(s,a;w)$ input: state s and action a output: approximate action-value(scalar) 训练目标： 使价值评估的更精准，接近于实际环境的return 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) https://www.bilibili.com/video/BV16Y411f7Hp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d https://mp.weixin.qq.com/s/y1Rj3fIaXkNjEyakCqRSIg Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.) https://www.bilibili.com/video/BV1Sq4y1q7sw/?spm_id_from=333.337.search-card.all.click&amp;vd_source=514a3ed3ac370caf4facad7d6f4e1a2d","tags":["强化学习"]},{"title":"无模型强化学习","path":"/2024/05/15/无模型强化学习/","content":"知识点 1.无模型价值学习评估 蒙特卡洛方法 时序差分学习 TD(𝝀) 2.无模型策略优化控制 蒙特卡洛策略迭代 时序差分策略迭代（SARSA） Q值迭代 (Q-learning) 1. 无模型价值学习评估 1.1 蒙特卡洛方法 蒙特卡洛方法是一种基于样本的方法，不需要知道环境的所有信息。只需基于过去的经验就可以学习。具体来说，给定一个策略 π，通过对 π 产生的回报取平均值来评估状态价值函数。这样就有两种估算方式: 首次蒙特卡罗(First-Visit Monte Carlo)和每次蒙特卡罗(Every-Visit Monte Carlo)。首次蒙特卡罗只考虑每一个回合中第一次到状态 s 的访问，而每次蒙特卡罗就是考虑每次到状态 s 的访问。 注意的是，和动态规划不同的是，蒙特卡罗不使用自举(Bootstrapping)，也就是说，它不用其他状态的估算来估算当前的状态值。 离线学习：智能体从预先收集好的数据中进行学习。 在线学习：智能体通过与环境实时交互来获取知识和经验。 1.2 时序差分学习 时序差分学习方法同蒙特卡洛方法一样是不基于模型的，不需要马尔可夫决策过程的知识。但是时序差分学习方法可以直接从经历的不完整经历片段中学习，它通过**自举(bootstrap)**猜测经历片段的结果并不断更新猜测。即时序差分学习方法可以在每一次经历的过程中进行学习，而蒙特卡洛方法只能等到每次经历完全结束时才能进行学习。 $$𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝐺_t − 𝑉(𝑆_{𝑡}))$$ 对TD(0)，即one-step TD: $$𝑉(𝑆_{𝑡}) ← 𝑉(𝑆_{𝑡}) + 𝛼(𝑅_{𝑡+1} + 𝛾𝑉(𝑆_{t+1}) − 𝑉(𝑆_{𝑡}))$$ 这个算法又被叫做SARSA，因为用到了 $(S_t, A_{𝑡}, R_{𝑡+1}, S_{𝑡+1}, A_{𝑡+1})$。 蒙特卡洛方法没有偏倚，是对当前状态实际价值的无偏估计，但有着较高的变异性，且对初始值不敏感。 时序差分方法方差更低, 但有一定程度的偏差，对初始值较敏感，通常比蒙特卡洛方法更高效。 1.3 TD(𝝀) 2.无模型策略优化控制 2.1 蒙特卡洛策略迭代 2.2 时序差分策略迭代（SARSA） $$G_{t:t+n} = R_{t+1} + γR_{t+2} + \\dot + γ^{n−1}R_{t+n} + γ^nQ_{t+n−1}(S_{t+n}, A_{t+n})$$ 2.3 Q值迭代 (Q-learning) Sarsa --&gt; on-policy Q-learning --&gt; off-policy 参考资料： 中国科学院大学林姝老师 强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) 强化学习入门——从原理到实践，叶强 Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)","tags":["强化学习"]},{"title":"how to build up a blog","path":"/2024/05/12/how-to-build-up-a-blog/","content":"登录root用户 1su root 切换路径 1cd /Users/lwl/Blog 博客文章发布 12345hexo new &quot;title&quot; # 新建文章# 编辑对应的markdown文件hexo g # 渲染md文件为博客页面hexo s # 执行后打开http://localhost:4000/预览hexo d # 预览并编辑无误后再部署，也可以直接部署 有时部署会失败，此时尝试下面命令清除缓存后再执行部署命令。 1hexo clean 更换端口 1hexo s -p 8888 # 示例 释放端口 12lsof -i :端口号 # 查找占用指定端口的进程ID（PID）kill -9 进程ID"}]