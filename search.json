[{"title":"多模态知识图谱应用","path":"/2024/05/15/多模态知识图谱应用/","content":"相比于单模态知识图谱，多模态知识图谱能够综合多种类型的数据，从而可以让智能体更深入的感知和理解真实的数据场景，因而多模态知识图谱在各个领域都有广泛的应用。如图像检索、模型推理与生成、模型预训练等。以电子商务为例，通过多模态产品图谱，可以对产品进行更细致的表示，再通过预训练，可以增强大型模型对电子商务领域的多模态知识理解，从而推动电子商务平台的发展。 AliMe MKG是阿里提出的一种面向直播的知识图谱，与传统的知识图谱不同，它的目标是向顾客种草某一产品，而非解决顾客的问题。因此，它需要构建逻辑思维链，引导用户需求。例如，在该左图1的知识图谱示例中，”熬夜”导致”皮肤暗沉”问题，这就需要”皮肤白皙”，而含有”甘草酸二钾”成分的”面膜”产品适合相应的用户。 在电商直播领域，这种知识图谱有两种应用：智能辅播和虚拟主播。智能辅播是在真人直播间构造了一个智能助理机器人，来协助主播去做商品介绍。比如说用户问的是口红，直播间内有多个口红，智能辅播就会将相关的信息展示出来给用户进行浏览，当用户点击确认，选择一个感兴趣的口红之后，辅播就会从知识图谱中抽取相应信息，以商品卡片信息的方式让用户和图片进行交互。除此之外，智能辅播还可以回答用户丰富的产品相关问题。比如用户问尺码的时候，辅播可以去推出文本介绍和对应的尺码图，用文本及图片来回答用户的咨询。 虚拟主播则是一个智能的直播间虚拟人，通过自动生成图文剧本介绍商品，生成具有吸引力和认知的知识型短视频，从而可以影响客户的购买决策。因此，多模态的知识图谱可以促进电商的发展。 另一个是医疗诊断的案例，这是一个基于多模态知识图谱的医疗健康问答系统示例。它首先利用多种方法获取用户提交数据的关键信息，并确定用户查询的主题意图，建立用户的知识需求模型。在知识匹配阶段，我们计算用户需求与医疗健康知识的相关度，并消除可能的歧义，最终向用户提供匹配度高的医疗健康知识。 参考资料: Chen Z, Zhang Y, Fang Y, et al. Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey[J]. arXiv preprint arXiv:2402.05391, 2024. Xu G, Chen H, Li F L, et al. Alime mkg: A multi-modal knowledge graph for live-streaming e-commerce[C]&#x2F;&#x2F;Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. 2021: 4808-4812. https://mp.weixin.qq.com/s/rW9ezfkAgOHAsICYuiPq6A https://mp.weixin.qq.com/s/bTqr5EEQD5_rModP8NR99g 韩普,叶东宇,陈文祺,等.面向多模态医疗健康数据的知识组织模式研究[J].现代情报,2023,43(10):27-34+151.","tags":["知识工程","多模态","知识图谱"]},{"title":"强化学习基础知识","path":"/2024/05/15/强化学习基础知识/","content":"知识点：马尔科夫决策过程，动态规划。 马尔科夫决策过程马尔科夫过程: 一个具备马尔科夫性质的离散随机过程。 马尔科夫性: 下一时刻的状态只与当前状态有关。即$P[S_{t+1}|S_{1},…,S_{t}]&#x3D;P[S_{t+1}|S_{t}]$ 马尔科夫奖励函数: 把马尔科夫过程从 &lt;S, P&gt; 拓展到 &lt;S, P, R, γ&gt;。其中P 为状态转移矩阵，R 和 γ 分别表示奖励函数和奖励折扣因子。折扣因子越大，代表了智能体对长期性能指标考虑的程度越高（远视）；折扣因子越小，代表了智能体对长期性能指标考虑的程度越低（近视）。 回报: 回报是一个轨迹的累积奖励，$G_{t} &#x3D; R_{t+1} + \\gamma R_{t+2} &#x3D; \\sum_{k&#x3D;0}^{\\infty } \\gamma ^{k}R_{t+k+1}$ 价值函数：状态s的期望回报，$V(s) &#x3D; E[G_{t}|S_{t}&#x3D;s]$。 马尔科夫决策过程: 马尔可夫奖励过程的立即奖励只取决于状态(奖励值在节点上)，而马尔可夫决策过程的立即奖励与状态和动作都有关。即把马尔科夫过程从 &lt;S, P, R, γ&gt; 拓展到 &lt;S, A, P, R, γ&gt;。A是有限动作的集合。 动作价值函数: 依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。$Q(s,a) &#x3D; E[G_{t}|S_{t}&#x3D;s, A_{t}&#x3D;a]$。易知$V(s)&#x3D;E_{a}[Q(s,a)]$。 策略: 状态到行为的映射。 对于任何马尔科夫决策过程： 总是存在一个最优策略𝜋∗，比任何其他策略更好或至少相等。 所有的最优策略有相同且最优的价值。 所有的最优策略具有相同且最优的动作价值。 贝尔曼方程：用于计算给定策略 π 时价值函数在策略指引下所采轨迹上的期望。 最优价值函数: 即使是在相同的状态和动作集合上，不同的策略也将会带来不同的价值函数。定义最优价值函数为$$v_*(s) &#x3D; \\max_{π} v_π(s), ∀s ∈ S$$最优动作价值函数：$$q_*(s,a) &#x3D; \\max_{\\pi} q_π(s,a), ∀s ∈ S, a ∈ A$$则$$v*(s) &#x3D; \\max_{a~\\mathbf{A}} q_*(s, a)$$$$q_*(s, a) &#x3D; E[R_t + γv_*(S_{t+1}) | S_t &#x3D; s, A_t &#x3D; a]$$ 逆矩阵法求解贝尔曼方程： $$\\mathbf{v} &#x3D; \\mathbf{r} + γP\\mathbf{v} $$其中 v 和 r 矢量，P是状态转移概率矩阵。求解如下：$$\\mathbf{v} &#x3D; (I − γP )^{-1}\\mathbf{r}$$复杂度为$O(n^{3})$，考虑其他方法进行求解，如动态规划、蒙特卡洛估计、时序差分法等。 动态规划参考资料： 中国科学院大学强化学习课程课件 深度强化学习：基础、研究与应用 (董豪 等) Reinforcement Learning An Introduction (Adaptive Computation and Machine Learning series) (Sutton, Richard S., Barto, Andrew G.)","tags":["强化学习"]},{"title":"how to build up a blog","path":"/2024/05/12/how-to-build-up-a-blog/","content":"登录root用户 1su root 切换路径 1cd /Users/lwl/Blog 博客文章发布 12345hexo new &quot;title&quot; # 新建文章# 编辑对应的markdown文件hexo g # 渲染md文件为博客页面hexo s # 执行后打开http://localhost:4000/预览hexo d # 预览并编辑无误后再部署，也可以直接部署 有时部署会失败，此时尝试下面命令清除缓存后再执行部署命令。 1hexo clean 更换端口 1hexo s -p 8888 # 示例 释放端口 12lsof -i :端口号 # 查找占用指定端口的进程ID（PID）kill -9 进程ID"}]